# 고급 백엔드 스터디 15주차

## Concurrency in LSM Trees

LSM 트리에서의 주요 동시성 문제는 테이블 뷰(플러시 및 압축 중에 변경되는 메모리 및 디스크 상주 테이블의 모음) 전환 및 로그 동기화와 관련이 있습니다. Memtable은 또한 일반적으로 동시에 접근되지만(ScyllaDB와 같은 코어 분할 저장소 제외), 동시 인메모리 데이터 구조는 이 책의 범위를 벗어납니다.

플러시 중에는 다음 규칙을 따라야 합니다:

- 새로운 memtable은 읽기와 쓰기가 가능해져야 합니다.
- 이전(플러시 중인) memtable은 읽기를 위해 계속 보여야 합니다.
- 플러시 중인 memtable은 디스크에 기록되어야 합니다.
- 플러시된 memtable을 폐기하고 플러시된 디스크 상주 테이블을 만드는 것은 원자적 연산으로 수행되어야 합니다.
- 플러시된 memtable에 적용된 연산의 로그 항목을 담고 있는 미리 쓰기 로그 세그먼트는 폐기되어야 합니다.

예를 들어, Apache Cassandra는 연산 순서 장벽을 사용하여 이러한 문제를 해결합니다: 쓰기를 위해 수락된 모든 연산은 memtable 플러시 이전에 대기하게 됩니다. 이런 방식으로 플러시 프로세스(소비자 역할을 함)는 어떤 다른 프로세스(생산자 역할을 함)가 자신에게 의존하는지 알게 됩니다.

- LSM 트리에서도 동시성 문제를 고려해야 함. ‘테이블 뷰 전환’ 및 ‘로그 동기화’가 핵심 문제라고 함. 이 둘에 대해 알아보자
- 테이블 뷰 전환
  - LSM 트리는 여러 개의 memtable 및 sstable들로 이루어진다고 했음. 이들의 모음을 테이블 뷰라고 함.
  - 플러시나 컴팩션이 발생하면 이들 사이에 변경이 발생함. memtable이 sstable로 바뀐다거나, 여러 sstable이 하나로 합쳐지는 등…
  - 이러한 변경이 발생하는 시점에 다른 읽기 쓰기 요청이 들어오면 일관되지 않은 데이터를 읽거나 쓸 가능성이 있음.
- 로그 동기화
  - memtable에서 변경사항은 wal에 먼저 쓰고 memtable에 기록됨. 그리고 memtable이 sstable로 플러시되면 이는 영속화된 데이터이므로 이 구간에 해당하는 wal 일부분은 제거해야 함
  - 이 로그 세그먼트의 삭제 시점과 다른 읽기 / 쓰기 / 플러시가 겹치게 되면 마찬가지로 문제가 발생할 수 있음
- 이러한 문제가 발생하지 않기 위해서, 플러시 동작 중에는 아래와 같은 규칙을 지켜야 함. 그래야 데이터 정합성이 깨지지 않음
  - 새로운 memtable은 읽기 쓰기가 가능해져야 함 → 플러시가 시작되면 기존 멤테이블은 플러싱 상태로 진입하여 새로운 쓰기 요청을 받지 않음. 플러시 동안 쓰기 작업이 블로킹되면 안되므로 새로운 멤테이블을 만들어서 쓰기 요청이 중단없이 처리될 수 있어야 함.
  - 이전(플러시 중인) memtable은 읽기를 위해 계속 보여야 합니다 → 플러싱 상태로 전환된 테이블은 그럼 플러시만 하냐? 아님. 읽기도 받을 수 있어야 함. 현재 플러시 중인 테이블에 최신 정보가 있을 수도 있기 때문임. 만약 플러싱 멤테이블을 읽기 대상에서 제외한다면 당연히 정확한 데이터를 읽을 수 없음.
  - 플러시된 memtable을 폐기하고 플러시된 디스크 상주 테이블을 만드는 것은 원자적 연산으로 수행되어야 함 → 플러시를 하게 되면 하나의 데이터는 멤테이블과 플러싱 대상 sstable 둘 다에 존재하게 됨. 그리고 이 sstable은 아직 읽기 대상으로 잡히지 않는 상태임. 왜? 플러싱 중인 memtable이 읽기 요청을 받고 있으므로. 이제 플러시가 다 끝났으면 기존 읽기를 받고 있던 memtable은 폐기하고 새롭게 만들어진 sstable이 읽기 요청을 받아줘야 함. 이 두 과정이 원자적으로 수행되어야 함. 만약 memtable을 먼저 폐기하고 sstable을 공개한다면? 그러면 memtable이 폐기되고 sstable이 공개될 때까지 그 데이터는 없어진 것처럼 보일 것임.
  - 플러시된 memtable에 적용된 연산의 로그 항목을 담고 있는 미리 쓰기 로그 세그먼트는 폐기되어야 함 → 아까 말했듯 WAL은 휘발성 매체에 존재하는 memtable의 유실 가능성에 대비하여 존재하는 것이므로 sstable에 플러시되어 데이터가 영속화되면 더 이상 wal을 유지할 필요 없음. 따라서 버려야 함
- 아파치 카산드라의 연산 순서 장벽
  - 복잡한 여러 개의 규칙을 어떻게 동시에 구현할까? → 연산 순서 장벽
  - 플러시를 시작하기 전에 지금 진행 중인 쓰기 연산이 끝날 때까지 쉬는 지점을 만드는 것.
  - 이 장벽을 통해서 플러시 수행 시 처리해야 하는 데이터 범위를 특정할 수 있음. 즉 어떤 쓰기 연산까지 플러시하고 어떤 연산부터는 다음 memtable에 포함시킬지 알 수 있음.
  - 이걸 생산자-소비자 모델로 표현하는데, 쓰기를 수행하는 스레드를 생산자, 플러시를 수행하는 프로세스를 소비자로 표현

더 일반적으로, 우리는 다음과 같은 동기화 지점들을 가집니다:

- **Memtable 교체(Memtable switch)**
  이후 모든 쓰기는 새로운 memtable로만 향하게 하여 그것을 주(primary) memtable로 만들며, 이전 memtable은 여전히 읽기용으로 사용할 수 있습니다.
- **플러시 확정(Flush finalization)**
  테이블 뷰에서 이전 memtable을 플러시된 디스크 상주 테이블로 교체합니다.
- **미리 쓰기 로그 잘라내기(Write-ahead log truncation)**
  플러시된 memtable과 연관된 레코드를 담고 있는 로그 세그먼트를 폐기합니다.

이러한 연산들은 심각한 정확성 문제를 가집니다. 이전 memtable에 쓰기를 계속하면 데이터 유실이 발생할 수 있습니다; 예를 들어, 이미 플러시된 memtable 섹션에 쓰기가 이루어지는 경우입니다. 마찬가지로, 디스크 상주 대응물이 준비될 때까지 이전 memtable을 읽기용으로 남겨두지 못하면 불완전한 결과를 초래할 것입니다.

- 세 가지 동기화 지점이 있음
  - 멤테이블 교체 → 플러시가 시작되는 첫 번째 동기화 포인트임. 이 시점을 기준으로 기존 멤테이블은 쓰기를 받지 않고 새로운 쓰기는 새로운 멤테이블로 보냄
  - 플러시 확정 → 플러시가 끝나는 두 번째 동기화 포인트. 이때 시스템은 데이터를 테이블 뷰로부터 읽는데, 이 테이블 뷰를 원자적으로 갱신해야 함. 구체적으로는 플러시중인 멤테이블을 제거 + 플러시 결과물인 SSTable을 읽기 대상으로 공개하는 작업
  - WAL 삭제 → 플러시 성공 이후 마지막 동기화 포인트. 앞에서 설명했음
- 이 세 지점은 문제가 발생할 수 있는 포인트이기도 함
  - 가령 1단계 멤테이블 교체에서 플러싱 중인 멤테이블에 쓰기 요청을 넣어버리면 유실될 수 있음
  - 또 2단계 SSTable이 읽기 가능 상태가 아닌데 플러싱 memtable을 읽기 대상에서 제외해버리면 그 데이터는 없는 것처럼 보일 것임 (정확하게는 오래된 데이터로 보일 것)

압축 중에도 테이블 뷰는 변경되지만, 여기서는 프로세스가 약간 더 간단합니다: 이전의 디스크 상주 테이블들은 폐기되고, 대신 압축된 버전이 추가됩니다. 이전 테이블들은 새로운 테이블이 완전히 기록되고 읽기를 위해 그것들을 대체할 준비가 될 때까지 읽기용으로 접근 가능하게 남아 있어야 합니다. 동일한 테이블들이 병렬로 실행되는 여러 압축 작업에 참여하는 상황 또한 피해야 합니다.

- 플러시 말고 컴팩션은 어떨까? 이때도 테이블 뷰는 바뀜. 즉 여러 개의 SSTable이 하나로 바뀜.
- 플러시보다는 그래도 간단함. 플러시에서는 memtable, wal, sstable을 모두 고려해야 했는데, 컴팩션은 그냥 sstable들끼리만 고려하면 되므로 동기화가 간단
- 그래서 지켜야 하는 규칙도 간단. 컴팩션 이후 새롭게 만들어지는 sstable이 읽기 가능해질 때까지 기존 sstable을 지우지 않는 것 (그래야 원자적으로 스왑 가능)
- 여러 컴팩션 작업에 하나의 테이블이 중복해서 참여하게 되어서도 안됨. 앞에서 말했듯 새로운 sstable이 읽기 가능해질 때까지 기존 sstable을 지우면 안된다고 했는데, 이 경우 읽기 가능해지더라도 기존 sstable을 지우지 못해서 기다려야 하거나 / 기다리지 않고 지우면다른 진행 중인 컴팩션이 실패하는 문제가 생길 것
  - 따라서 컴팩션 대상 테이블에 락을 걸어야 함

B-트리에서는 내구성을 보장하기 위해 로그 잘라내기가 페이지 캐시로부터 더티 페이지(dirty pages)를 플러시하는 것과 조율되어야 합니다. LSM 트리에서도 비슷한 요구사항이 있습니다: 쓰기는 memtable에 버퍼링되고, 그 내용은 완전히 플러시될 때까지 내구성이 없으므로, 로그 잘라내기는 memtable 플러시와 조율되어야 합니다. 플러시가 완료되자마자, 로그 관리자는 가장 최근에 플러시된 로그 세그먼트에 대한 정보를 받고, 그 내용은 안전하게 폐기될 수 있습니다.

- wal 제거의 경우 lsm tree 말고 b-tree에서도 했던 거임. 둘은 사실 비슷한 얘기임. 데이터가 디스크에 영속화되기 전에는 wal을 날리면 안된다 라는 원칙을 지키고 있음.
- b-tree를 보자…
  - b-tree에서는 변경이 발생하면 디스크에 바로 쓰지 않고 페이지 캐시에 반영함. 이를 더티 페이지라고 함. 더티 페이지는 메모리에 존재하기에 휘발될 수 있음. 따라서 변경을 더티 페이지에 쓰기 전에 WAL에 또 따로 기록해야 함. 장애가 발생하면 WAL을 활용하여 복구함
- LSM도 비슷했음
  - b-tree에서는 페이지 캐시였지만 lsm tree는 memtable임. 쓰기 요청을 페이지 캐시에 버퍼링했던 것처럼 memtable에 버퍼링함. 마찬가지로 memtable은 메모리에 존재하기에 휘발될 수 있음. 따라서 변경을 memtable에 쓰기 전에 WAL에 따로 기록해야 함.
- lsm 트리도 특정 부분에 해당하는 wal을 삭제하려면, 그 로그에 담긴 데이터가 담긴 memtable이 sstable로 플러시되어야 함 → 구체적으로는… 플러시 시점을 받아서 LSN을 활용하여 어디까지 플러시되었는지를 기반으로 그 이전 시점까지의 세그먼트를 지우는 식

로그 잘라내기와 플러시를 동기화하지 않으면 데이터 손실이 발생합니다. 만약 플러시가 완료되기 전에 로그 세그먼트가 폐기되고 노드가 다운되면, 로그 내용은 재실행(replay)되지 않으며 해당 세그먼트의 데이터는 복원되지 않을 것입니다.

- 로그 제거와 플러시가 동기화되지 않는 경우 생기는 문제
  - 플러시 전에 로그 세그먼트를 버리고 노드가 죽는다면? 복구할 때 리플레이할 WAL이 없으므로 복원 실패할 것

## Log Stacking

많은 최신 파일 시스템은 로그 구조화되어 있습니다: 쓰기를 메모리 세그먼트에 버퍼링하고, 가득 차면 내용을 추가 전용 방식으로 디스크에 플러시합니다. SSD 또한 작은 임의 쓰기를 처리하고, 쓰기 오버헤드를 최소화하며, 웨어 레벨링을 개선하고, 장치 수명을 늘리기 위해 로그 구조화 스토리지를 사용합니다.

- SSD의 FTL 역시 로그 구조화 방법을 사용함. SSD도 덮어쓰기가 비효율적이라는 이야기 했음. 페이지 단위로 쓰고 삭제는 블록 단위로 했어야 하기 때문. 그래서 작은 일부분만 수정하는 랜덤 쓰기에 취약. 따라서 변경사항을 새 공간에 순차적으로 쓰고 물리 주소와 논리 주소를 매핑하는 방식을 사용했음.
- 웨어 레벨링 → SSD의 메모리 셀은 수명이 정해져 있으므로, 특정 셀에만 쓰기가 집중되는 것을 막고 모든 셀을 균등하게 사용하도록 관리하는 기술임. 로그 구조화 방식은 데이터를 디스크 전체에 분산시켜 쓰므로 자연스럽게 웨어 레벨링 효과를 가져와 SSD의 수명을 연장시키는 효과 있음

로그 구조화 스토리지(LSS) 시스템은 SSD가 더 저렴해질 무렵부터 인기를 얻기 시작했습니다. LSM 트리와 SSD는 좋은 조합입니다. 순차적인 워크로드와 추가 전용 쓰기는 SSD의 성능에 부정적인 영향을 미치는 인플레이스 업데이트로 인한 증폭을 줄이는 데 도움이 되기 때문입니다.

- SSD의 대중화가 LSS의 장점을 극대화하는 환경을 제공했고, 이로 인해 LSS 기반의 시스템들이 주목받기 시작했음
- LSM 트리는 쓰기 작업을 메모리에서 처리한 후 디스크에 순차적으로 기록하므로, SSD의 동작 방식과 매우 잘 맞음
- 반면, B-Tree와 같은 제자리 갱신 방식은 기존 데이터를 직접 수정하므로 SSD에서 비효율적인 읽기-수정-쓰기 동작과 쓰기 증폭을 유발하여 성능 저하 및 수명 단축으로 이어짐

만약 여러 로그 구조화 시스템을 서로 겹쳐 쌓으면, 쓰기 증폭, 단편화, 그리고 낮은 성능을 포함하여 LSS를 사용하여 해결하려 했던 여러 문제에 직면할 수 있습니다. 최소한, 우리는 애플리케이션을 개발할 때 SSD 플래시 변환 계층과 파일 시스템을 염두에 두어야 합니다 [YANG14]

- 로그 스태킹이라는 문제가 있음
- LSM 트리 데이터베이스를 로그 구조화 파일 시스템 위에서 실행하고, 이를 다시 내부적으로 LSS로 동작하는 SSD 위에서 구동하는 상황
- 각 계층은 서로를 인지하지 못한 채 독립적으로 최적화를 시도함. 상위 계층에서 순차 쓰기로 보낸 작업이 하위 계층에서는 단편화된 임의 쓰기로 변환될 수 있고, 각 계층의 가비지 컬렉션(compaction) 작업이 서로 간섭하여 전체적인 쓰기 증폭과 성능 저하를 야기함.
- 데이터베이스는 하드웨어 성능에 민감하므로 운영체제는 물론 SSD FTL과 같은 하드웨어 동작을 고려하여 설계해야 함. 그래야 LSS의 성능을 극대화 가능

SSD에서 로그 구조화 매핑 레이어를 사용하는 것은 두 가지 요인에 의해 동기 부여됩니다: 작은 무작위 쓰기는 물리적 페이지에 함께 배치되어야 하며, SSD가 프로그램/삭제 주기로 작동한다는 사실입니다. 쓰기는 이전에 지워진 페이지에만 수행될 수 있습니다. 이는 페이지가 비어 있지 않으면(즉, 지워지지 않았으면) 프로그래밍(즉, 쓰기)될 수 없음을 의미합니다.

- SSD 컨트롤러 내부에 존재하는 '로그 구조화 매핑 레이어'의 필요성 두 가지
  - 첫 번째 이유는 성능 최적화임. 작은 크기의 무작위적인 쓰기 요청들을 메모리 버퍼에 모았다가 하나의 큰 물리 페이지 단위로 묶어서 순차적으로 기록함으로써 쓰기 효율을 높임
  - 두 번째 이유는 SSD의 근본적인 동작 방식 때문임. SSD는 하드디스크처럼 데이터를 덮어쓸 수 없고, '프로그램(쓰기를 의미)'과 '삭제'라는 별도의 과정을 거쳐야 함
- SSD의 낸드 플래시 메모리 셀에는 데이터 덮어쓰기(overwrite)가 불가능함.
  - 데이터를 쓰기 위해서는 해당 페이지가 반드시 이전에 '삭제(erase)'되어 완전히 비워진 상태여야 함.
  - 만약 어떤 페이지의 데이터를 수정하려면, 그 페이지를 직접 수정하는 것이 아니라, 수정된 데이터를 다른 비어있는 페이지에 쓰고 원래 페이지는 '무효(invalid)' 처리함.

단일 페이지는 지울 수 없으며, 블록 내의 페이지 그룹(일반적으로 64~512 페이지를 보유)만이 함께 지워질 수 있습니다. 그림 7-13은 블록으로 그룹화된 페이지의 개략적인 표현을 보여줍니다. 플래시 변환 계층(FTL)은 논리적 페이지 주소를 물리적 위치로 변환하고 페이지 상태(활성, 폐기 또는 비어 있음)를 추적합니다. FTL에 사용 가능한 페이지가 부족해지면 가비지 컬렉션을 수행하고 폐기된 페이지를 지워야 합니다.

- SSD의 또 다른 핵심 제약사항이 있음. 쓰기(프로그램) 단위와 삭제(erase) 단위가 다름.
  - 데이터 쓰기는 비교적 작은 '페이지' 단위로 가능함.
  - 하지만 데이터 삭제는 페이지보다 훨씬 큰 '블록' 단위로만 가능함. 하나의 블록은 수십에서 수백 개의 페이지로 구성됨.
  - 이 때문에 블록 내의 단 하나의 페이지만을 지우고 싶어도, 그 페이지가 속한 블록 전체를 삭제해야만 함.
- 이러한 복잡한 물리적 제약사항을 OS나 개발자가 직접 다루지 않도록 중간에서 관리해주는 소프트웨어 계층이 FTL(Flash Translation Layer)임
  - FTL의 핵심 역할 중 하나는 주소 변환임. 운영체제가 요청하는 논리적 주소(LBA)를 실제 낸드 플래시 칩의 물리적 페이지 주소로 매핑해줌.
  - 또 다른 핵심 역할은 각 물리 페이지의 상태를 관리하는 것임. 아래 세 가지 상태가 있음
    - 활성(live): 현재 유효한 데이터가 저장된 페이지
    - 폐기(discarded): 데이터가 삭제되거나 수정되어 더 이상 유효하지 않은 페이지
    - 비어 있음(empty): 삭제 작업이 완료되어 새로운 데이터를 쓸 수 있는 페이지
- 가비지 컬렉션
  - 쓰기 작업이 계속되다 보면 더 이상 쓸 수 있는 '비어 있는(empty)' 페이지가 고갈됨.
  - 이때 FTL은 '폐기된(discarded)' 페이지가 많이 포함된 블록을 찾아 가비지 컬렉션을 시작함.
  - 가비지 컬렉션은 해당 블록에 남아있는 소수의 '활성(live)' 페이지들을 다른 비어있는 블록의 페이지로 복사(relocation)한 뒤, 원래 블록 전체를 삭제(erase)하여 새로운 쓰기 공간으로 확보하는 과정임. 이 과정은 추가적인 쓰기를 유발하여 SSD 성능에 영향을 줄 수 있음. (쓰기 증폭)

삭제하려는 블록의 모든 페이지가 폐기된다는 보장은 없습니다. 블록을 삭제하기 전에 FTL은 살아있는 페이지를 빈 페이지가 있는 블록 중 하나로 재배치해야 합니다. 그림 7-14는 살아있는 페이지를 한 블록에서 새로운 위치로 옮기는 과정을 보여줍니다. 모든 살아있는 페이지가 재배치되면 블록을 안전하게 삭제할 수 있으며, 해당 블록의 빈 페이지는 쓰기 작업에 사용할 수 있게 됩니다. FTL은 페이지 상태와 상태 전환을 인지하고 있고 필요한 모든 정보를 가지고 있기 때문에, SSD 웨어 레벨링도 담당합니다.

- 가비지 컬렉션을 위해 특정 블록을 삭제하려고 할 때, 그 블록 안에는 더 이상 유효하지 않은 데이터(discarded page)와 여전히 유효한 데이터(live page)가 섞여 있을 수 있음
- FTL은 유효한 데이터의 손실을 막기 위해, 삭제 대상 블록 안에 있는 '살아있는 페이지'의 내용을 다른 블록의 '빈 페이지'로 먼저 복사(재배치)함 → 이 과정을 통해 유효한 데이터는 보존하면서 불필요한 데이터가 차지하던 공간을 정리할 수 있음
- 살아있는 페이지를 모두 다른 곳으로 옮기고 나면, 원래 블록에는 더 이상 유효한 데이터가 없으므로 블록 전체를 안전하게 삭제할 수 있음.
- 삭제된 블록은 전체가 '빈 페이지' 상태가 되어, 새로운 데이터를 쓸 수 있는 공간으로 재활용됨.
- FTL은 모든 페이지 상태를 관리함
  - 각 페이지가 '살아있는(live)', '폐기된(discarded)', '빈(empty)' 상태 중 어디에 속하는지와 그 변화 과정을 모두 알고 있음
  - 이러한 정보 관리 능력을 바탕으로 FTL은 SSD의 또 다른 핵심 기능인 '웨어 레벨링'을 수행함.
  - 웨어 레벨링은 SSD의 모든 메모리 셀(블록)에 쓰기/삭제 작업이 균등하게 분산되도록 하여, 특정 셀만 과도하게 사용하여 수명이 단축되는 것을 막고 SSD 전체의 수명을 늘리는 기술임
    - “웨어 레벨링은 매체 전반에 걸쳐 부하를 고르게 분산시켜, 높은 횟수의 프로그램-삭제 주기로 인해 블록이 조기에 고장나는 핫스팟(hotspot)을 방지합니다. 플래시 메모리 셀은 제한된 횟수의 프로그램-삭제 주기만을 거칠 수 있으므로 이는 필수적이며, 메모리 셀을 고르게 사용하는 것은 장치의 수명을 연장하는 데 도움이 됩니다.”

요약하자면, SSD에서 로그 구조화 저장소를 사용하는 동기는 작은 임의 쓰기를 함께 묶어 입출력 비용을 분할 상환하는 것입니다. 이는 일반적으로 더 적은 수의 작업으로 이어지고, 결과적으로 가비지 컬렉션이 실행되는 횟수를 줄여줍니다.

- SSD와 로그 구조화 저장소를 같이 쓰는 이유 → SSD 특성 상 작은 랜덤 쓰기가 불리한데, LSS를 쓰면 버퍼링하여 순차 쓰기로 대체하므로 굿…

### Filesystem Logging

그 위에 파일시스템이 있는데, 이들 중 다수는 쓰기 증폭을 줄이고 기본 하드웨어를 최적으로 사용하기 위해 쓰기 버퍼링에 로깅 기술을 사용합니다.

- 앞선 문맥에서 SSD의 FTL(Flash Translation Layer)이 로그 구조화 방식으로 동작함을 설명했음. 그 하드웨어 계층 '위에' 운영체제의 파일시스템 계층 또한 비슷한 최적화 방식을 사용함
- 현대 파일시스템(ext4, NTFS 등)은 '저널링(journaling)'이라는 로깅 기법을 사용함. 이는 파일시스템 변경 사항(메타데이터 등)을 먼저 로그에 기록하여 시스템 장애 발생 시에도 데이터 일관성을 빠르게 복구하기 위함 → 이때 여기서도 쓰기를 버퍼링하여 쓰기 증폭을 해결

로그 스태킹은 몇 가지 다른 방식으로 나타납니다. 첫째, 각 계층은 자체적인 북키핑(bookkeeping)을 수행해야 하며, 대부분의 경우 기본 로그는 중복된 노력을 피하는 데 필요한 정보를 노출하지 않습니다.

- 로그 스태킹(LSM 트리, 저널링, FTL이 각각 로그 구조화 시스템을 사용하는 상황)
  - 자체적인 북키핑 → 서로의 상황에 대해서 모름. 즉 어떤 데이터가 live고 가비지인지 알 수 없음. 각각 독립적으로 추적함
  - 정보 노출 X → 그리고 독립적으로 추적하고 공유도 안함. 그래서 데이터베이스에서 삭제하더라도 OS나 FTL은 모름.

그림 7-15는 상위 계층 로그(예: 애플리케이션)와 하위 계층 로그(예: 파일시스템) 간의 매핑을 보여주며, 이는 중복 로깅과 다른 가비지 컬렉션 패턴을 초래합니다 [YANG14]. 정렬되지 않은 세그먼트 쓰기는 상위 계층 로그 세그먼트를 폐기할 때 인접한 세그먼트 부분의 단편화와 재배치를 유발할 수 있으므로 상황을 더욱 악화시킬 수 있습니다.

- 두 가지 문제가 발생
  - 중복 로깅 → 상위 계층(데이터베이스)이 데이터 변경을 로그에 기록하면, 하위 계층(파일시스템)은 그 '로그를 쓰는 행위' 자체를 또다시 자신의 로그(저널)에 기록할 수 있음. 동일한 작업에 대해 로깅이 중복으로 발생하는 비효율성
  - 다른 GC 패턴 → 각 계층은 자신만의 정책과 주기에 따라 가비지 컬렉션(또는 compaction)을 수행함. 데이터베이스가 특정 데이터를 삭제하여 공간을 정리했더라도, 파일시스템 계층에서는 그 공간이 다른 이유로 여전히 '사용 중'으로 간주될 수 있어, 공간 회수가 즉시 이루어지지 않는 등 비동기적인 패턴으로 인해 비효율이 발생함.
- 정렬되지 않은 세그먼트 쓰기 (misaligned segment write)
  - 상위 계층의 논리적인 데이터 단위(세그먼트)와 하위 계층의 물리적인 데이터 단위(블록, 페이지)의 경계가 일치하지 않는 쓰기 작업을 의미함.
  - 단편화, 재배치 → 예를 들어, 상위 계층에서 10KB 크기의 데이터 블록 A를 삭제한다고 가정. 하위 계층의 물리 페이지 크기가 4KB라면, 블록 A는 3개의 물리 페이지에 걸쳐서 저장될 수 있음. 이때 블록 A가 저장된 물리 페이지에는 다른 유효한 데이터(예: 블록 B의 일부)가 함께 있을 수 있음.
  - 따라서 블록 A를 삭제하기 위해 해당 물리 페이지들을 단순히 비울 수 없고, 유효한 데이터(블록 B의 일부)를 다른 곳으로 옮긴 후(재배치) 공간을 정리해야 함. 이 과정에서 불필요한 읽기와 쓰기가 발생하고 저장 공간이 조각나는(단편화) 문제가 발생함.

레이어들이 LSS 관련 스케줄링(예를 들어, 세그먼트의 폐기 또는 재배치)을 소통하지 않기 때문에, 하위 레벨 서브시스템들은 폐기된 데이터나 곧 폐기될 데이터에 대해 중복된 작업을 수행할 수 있습니다. 비슷하게, 단일 표준 세그먼트 크기가 없기 때문에, 정렬되지 않은 상위 레벨 세그먼트가 여러 하위 레벨 세그먼트를 차지하는 경우가 발생할 수 있습니다. 이러한 모든 오버헤드는 줄이거나 완전히 피할 수 있습니다.

- 각 계층이 독립적으로 LSS를 사용하면서 서로의 GC 정보를 공유하지 않는 상황
  - 예를 들어, 데이터베이스가 특정 데이터를 삭제하여 내부적으로 '폐기' 처리했더라도, 이 정보가 하위 계층인 파일 시스템이나 SSD에 전달되지 않음.
  - 결과적으로 하위 계층은 이미 상위 계층에서 버려진 데이터를 유효한 데이터로 착각하여, 자신의 GC 과정에서 이 데이터를 불필요하게 다른 곳으로 복사하는 등 중복되고 비효율적인 작업을 수행하게 됨.
- 정렬되지 않은 세그먼트 쓰기
  - 각 계층의 데이터 관리 단위(세그먼트 크기)가 서로 다르기 때문에 발생하는 문제를 지적함.
  - 예를 들어, 데이터베이스가 8KB 단위로 데이터를 쓰는데 파일 시스템이 4KB 단위로 관리한다면, 데이터베이스의 쓰기 요청 하나가 파일 시스템의 블록 두 개에 걸쳐서 저장될 수 있음.
  - 이는 상위 계층의 논리적인 작업 하나가 하위 계층에서 여러 개의 물리적인 작업으로 나뉘게 만들어 비효율을 야기함.
- 이 두 문제는 최적화를 통해 줄이거나 완전히 회피 가능

로그 구조화 스토리지가 순차 I/O에 관한 것이라고 말하지만, 데이터베이스 시스템은 여러 쓰기 스트림(예를 들어, 데이터 레코드 쓰기와 병렬적인 로그 쓰기)을 가질 수 있다는 점을 명심해야 합니다 [YANG14]. 하드웨어 수준에서 고려할 때, 인터리빙된 순차 쓰기 스트림은 동일한 순차 패턴으로 변환되지 않을 수 있습니다: 블록들이 반드시 쓰기 순서대로 배치되는 것은 아닙니다. 그림 7-16은 여러 스트림이 시간에 따라 겹치며, 하드웨어 페이지 크기와 정렬되지 않은 크기의 레코드를 쓰는 것을 보여줍니다.

- LSS가 이상적으로는 순차 I/O를 추구하지만, 현실의 데이터베이스 시스템에서는 여러 쓰기 흐름이 동시에 존재하여 순수하게 순차적인 쓰기 패턴을 유지하기 어려움
- 애플리케이션 관점에서 각각의 스트림은 순차적일 수 있지만, 이들이 뒤섞여 디스크에 도달하면 물리적으로는 데이터가 순서대로 기록되지 않음. 이는 사실상 랜덤 쓰기와 유사한 패턴을 만들어 LSS의 장점을 상쇄시킴.

이는 우리가 피하려고 했던 단편화를 초래합니다. 인터리빙을 줄이기 위해, 일부 데이터베이스 벤더들은 워크로드를 격리하고 성능 및 접근 패턴을 독립적으로 추론할 수 있도록 로그를 별도의 장치에 보관할 것을 권장합니다. 그러나 파티션을 기본 하드웨어에 정렬하고 [INTEL14] 쓰기를 페이지 크기에 맞춰 정렬하는 것이 더 중요합니다 [KIM12].

- 그림 7-16에서 어떻게 여러 쓰기 스트림이 물리적 페이지 위에서 단편화를 유발하는지를 보여주는지 볼 수 있음
- 해결 방법 1
  - WAL 파일과 실제 데이터 파일을 물리적으로 다른 저장 장치에 두는 방법
  - 이를 통해 각 쓰기 스트림(워크로드)을 분리하여 서로 간섭하지 않게 만들고, 각 장치에서는 더 순차적인 쓰기 패턴을 유지할 수 있음.
- 해결 방법 2
  - 물리적 장치를 분리하는 것보다 더 근본적이고 중요한 해결책 → 쓰기를 페이지 크기에 맞춰 정렬
  - 파티션 정렬 → 파일 시스템의 파티션 시작 주소를 SSD 같은 하드웨어의 물리적 블록 시작 주소와 일치시키는 것
  - 쓰기 정렬 → 데이터베이스의 쓰기 단위를 하드웨어의 페이지 크기와 일치시키는 것

## LLAMA and Mindful Stacking

> 음, 믿지 못하시겠지만, 당신이 보고 있는 저 라마는 한때 인간이었습니다. 그리고 그냥 평범한 인간이 아니었죠. 그는 황제였습니다. 부유하고, 강력하며, 카리스마 넘치는 존재였죠.

- Kuzco from The Emperor’s New Groove
  >

"Bw-트리"에서, 우리는 Bw-트리라고 불리는 불변 B-트리 버전에 대해 논의했습니다. Bw-트리는 래치-프리(latch-free), 로그-구조화(log-structured), 접근-방식-인지(access-method aware) (LLAMA) 스토리지 하위 시스템 위에 계층화되어 있습니다. 이 계층화는 Bw-트리가 동적으로 커지고 작아질 수 있게 하면서, 가비지 컬렉션과 페이지 관리를 트리에 대해 투명하게 만듭니다. 여기서 우리는 소프트웨어 계층 간의 협력의 이점을 보여주는 접근-방식-인지 부분에 가장 관심이 있습니다.

- 이전 장에서 Bw 트리에 대해 다뤘음. 핵심은 불변성…
- Bw-트리가 LLAMA라는 특정 스토리지 시스템 위에서 동작함
- LLAMA의 세 가지 주요 특징을 나열함.
  - 래치-프리(latch-free): Bw-트리는 동시성 제어를 위해 래치(간단한 잠금 장치)를 사용하지 않음. 이는 다중 코어 환경에서 병렬 처리 성능을 저해하는 요소를 제거하여 확장성을 높임. 대신 CAS(Compare-And-Swap)와 같은 원자적 연산을 활용함.
  - 로그-구조화(log-structured): 모든 데이터를 디스크에 로그처럼 순차적으로 추가하는 방식으로 기록함. 이는 디스크의 임의 쓰기(random write)를 피하고 순차 쓰기(sequential write)를 수행하여 쓰기 성능을 크게 향상시킴.
  - 접근-방식-인지(access-method aware): LLAMA 시스템이 상위 계층인 Bw-트리의 데이터 구조와 작동 방식을 '알고 있음'을 의미함. 이는 범용 스토리지 시스템과 달리, Bw-트리의 특성에 맞게 가비지 컬렉션 등의 작업을 최적화할 수 있게 함.
- LLAMA를 사용함으로써 얻는 이점
  - Bw-트리 계층은 데이터의 논리적 구조 관리에만 집중할 수 있음.
  - 실제 물리적인 데이터 관리(페이지 할당, 불필요한 데이터 정리 등)는 하위 LLAMA 계층이 알아서 처리해주므로, 상위 계층에서는 이러한 복잡성을 신경 쓸 필요가 없음. 이를 '투명하다(transparent)'고 표현
- LLAMA의 여러 특징 중 '접근-방식-인지'가 가장 핵심적인 개념
  - 이 특성은 상위 계층(Bw-트리)과 하위 계층(LLAMA)이 분리되어 있으면서도 서로의 정보를 활용하여 전체 시스템을 최적화하는 'Mindful Stacking'의 기반이 됨.

복습해보자면, 논리적 Bw-트리 노드는 물리적 델타 노드들의 연결 리스트, 즉 가장 최신의 업데이트부터 가장 오래된 업데이트까지 이어져 베이스 노드에서 끝나는 업데이트 체인으로 구성됩니다. 논리적 노드들은 인-메모리 매핑 테이블을 사용하여 연결되며, 이 테이블은 디스크 상의 최신 업데이트 위치를 가리킵니다. 키와 값은 논리적 노드에 추가되거나 제거되지만, 그들의 물리적 표현은 불변으로 유지됩니다.

- Bw 트리의 물리적 구조
  - Bw-트리의 노드는 단일 물리적 공간이 아닌 '논리적' 단위임.
  - 하나의 논리적 노드는 최초 데이터를 담은 '베이스 노드(base node)'와 그 이후의 모든 변경사항(삽입, 삭제 등)을 기록한 '델타 노드(delta node)'들의 연결 리스트(chain)로 구성됨.
- 논리적 노드와 물리적 페이지가 분리되는 시작점이었다는 이야기를 했었음…
  - Bw-트리는 각 논리 노드의 ID와, 해당 노드의 최신 데이터(가장 최근의 델타 노드)가 저장된 실제 디스크 주소를 매핑하는 '인메모리 매핑 테이블'을 관리함.
  - 이를 통해 특정 데이터를 찾을 때, 매핑 테이블을 거쳐 디스크 상의 정확한 최신 데이터 위치로 빠르게 접근할 수 있음
  - “키와 값은 논리적 노드에 추가되거나 제거되지만, 그들의 물리적 표현은 불변으로 유지됩니다” → 사용자 입장에서는 데이터가 수정되는 것처럼 보이지만(논리적 변경), 실제 디스크에서는 기존 데이터가 변경되지 않고 새로운 델타 노드가 추가될 뿐임(물리적 불변성).
    - Bw-Tree의 모든 데이터 변경 작업은 '추가 전용(append-only)' 방식으로 처리되었음
    - 기존 데이터를 수정하는 대신, 새로운 변경사항(삽입, 삭제 등)을 담은 델타 노드를 생성하여 기존 델타 체인의 '맨 앞에' 추가함 (prepending).
    - 매핑 테이블은 이제 이 새로운 델타 노드를 가리키도록 원자적으로 갱신

로그 구조화 스토리지는 노드 업데이트(델타 노드)를 4MB 플러시 버퍼에 함께 버퍼링합니다. 페이지가 가득 차면 디스크에 플러시됩니다. 주기적으로, 가비지 컬렉션은 사용되지 않는 델타 및 베이스 노드가 차지하는 공간을 회수하고, 활성 노드들을 재배치하여 조각난 페이지를 해제합니다.

- LLAMA 스토리지 시스템
  - 모든 델타 노드는 생성될 때마다 즉시 디스크에 기록되는 것이 아님.
  - 대신, 여러 델타 노드들을 메모리 상의 '플러시 버퍼(flush buffer)'(예: 4MB 크기)에 모음.
  - 이 버퍼가 가득 차면, 버퍼에 쌓인 델타 노드들을 하나의 큰 묶음으로 묶어 디스크에 순차적으로(sequentially) 기록함.
  - 이는 수많은 작은 랜덤 쓰기를 하나의 큰 순차 쓰기로 변환하여 디스크 I/O 효율을 극대화하는 로그 구조화 저장소의 핵심 원리임.
- 주기적인 GC
  - 시간이 지남에 따라 델타 체인이 길어지고, 구버전의 델타 노드나 베이스 노드는 더 이상 필요 없게 됨.
  - '가비지 컬렉션(garbage collection)' 프로세스가 주기적으로 실행되어 이러한 불필요한 노드들이 차지하는 디스크 공간을 회수함.
  - 또한, 여전히 유효한('live') 노드들을 새로운 위치로 모아서 재배치함으로써 디스크 단편화(fragmentation)를 해결함.

접근 방식 인식 없이는, 서로 다른 논리적 노드에 속한 인터리빙된 델타 노드들이 삽입 순서대로 쓰여질 것입니다. LLAMA의 Bw-Tree 인식은 여러 델타 노드를 단일의 연속적인 물리적 위치로 통합할 수 있게 합니다. 만약 델타 노드에 있는 두 업데이트가 서로를 상쇄한다면(예: 삽입 후 삭제), 그들의 논리적 통합 또한 수행될 수 있으며, 후자의 삭제만 영구 저장될 수 있습니다.

- LLAMA의 '접근 방식 인식(access-method awareness)' 기능이 없는 경우의 문제점?
  - 만약 스토리지 시스템(LLAMA)이 자신이 저장하는 데이터가 Bw-Tree 구조라는 것을 모른다면, 플러시 버퍼에 쌓인 델타 노드들을 단순히 도착한 순서대로 디스크에 기록할 것임.
  - 이 경우, 서로 다른 논리적 노드에 속한 델타 노드들이 디스크 상에 뒤섞여(interleaved) 저장되어 데이터 지역성(locality)이 떨어짐.
- '접근 방식 인식'의 또 다른 최적화 사례
  - LLAMA가 같은 논리적 노드에 대해 '어떤 키를 삽입하는 델타'와 '바로 그 키를 삭제하는 델타'가 연달아 있는 것을 발견하면, 이 두 연산이 서로를 상쇄(cancel out)함
  - 따라서 불필요한 삽입 연산은 디스크에 기록하지 않고, 최종 결과인 삭제 연산(tomestone)만 기록하여 쓰기 양을 줄이고 효율을 높임. 이게 '논리적 통합(logical consolidation)'을 말하는 것

LSS 가비지 컬렉션은 논리적 Bw-Tree 노드 내용을 통합하는 작업도 처리할 수 있습니다. 이는 가비지 컬렉션이 단순히 여유 공간을 회수할 뿐만 아니라, 물리적인 노드 단편화를 상당히 줄여준다는 것을 의미합니다. 만약 가비지 컬렉션이 여러 델타 노드를 연속적으로 재작성하기만 한다면, 그들은 여전히 같은 양의 공간을 차지할 것이고, 독자(reader)는 델타 업데이트를 베이스 노드에 적용하는 작업을 수행해야 할 것입니다. 동시에, 만약 상위 시스템이 노드를 통합하고 새로운 위치에 연속적으로 기록했다면, LSS는 여전히 이전 버전을 가비지 컬렉션해야 할 것입니다.

- LSS의 가비지 컬렉션(GC)은 단순히 삭제된 데이터를 정리하는 역할만 하는 게 아님
  - 이 분산된 델타 노드들을 베이스 노드와 합쳐서 하나의 완성된 노드로 만드는 '통합(Consolidation)' 작업을 수행할 수 있음
  - GC가 통합 작업을 수행하면, 여러 조각으로 나뉘어 있던 노드 데이터가 하나의 연속된 공간에 저장되므로 단편화가 해소되고 읽기 성능이 향상됨
- 만약 가비지 컬렉션이 여러 델타 노드를 연속적으로 재작성하기만 한다면?
  - 공간은 그대로 차지하고, 데이터를 읽을 때마다 독자(reader)가 여전히 베이스 노드에 모든 델타를 순서대로 적용해야 하는 부담이 사라지지 않음.
- 반대로, 상위 계층인 Bw-Tree가 자체적으로 노드 통합(consolidation)을 수행하고, LSS는 이를 인지하지 못하는 상황이라면?
- Bw-Tree가 통합된 새 노드를 쓰면, LSS 입장에서는 이전 버전의 베이스 노드와 델타 노드들이 '쓰레기'가 되므로, 별도로 GC 작업을 또 수행해야 함. 즉, 작업이 중복됨.

Bw-Tree의 시맨틱을 인지함으로써, 가비지 컬렉션 중에 여러 델타가 모든 델타가 이미 적용된 단일 베이스 노드로 재작성될 수 있습니다. 이는 이 Bw-Tree 노드를 표현하는 데 사용되는 총 공간과 폐기된 페이지가 차지하는 공간을 회수하면서 페이지를 읽는 데 필요한 지연 시간을 줄여줍니다.

- 가장 효율적인 해결책 → LSS의 GC 프로세스가 Bw-Tree의 구조(베이스 + 델타 체인)를 이해하고, GC를 수행하는 김에 노드 통합까지 한 번에 처리하는 것임.
- 이것이 바로 'Access-method aware'의 핵심 아이디어임. 하위 시스템(LSS)이 상위 시스템(Bw-Tree)의 데이터 구조와 작동 방식을 인지하고 협력하여 최적화를 이룸.
- 두 가지 이점
  1. 공간 감소: 여러 델타 노드가 하나의 베이스 노드로 합쳐지므로 전체 저장 공간이 줄어듦.
  2. 읽기 지연 시간 감소: 읽을 때마다 델타 체인을 따라가며 데이터를 재구성할 필요 없이, 통합된 노드를 한 번에 읽으면 되므로 읽기 속도가 빨라짐.

신중하게 고려하면, 스택킹(stacking)이 많은 이점을 가져올 수 있음을 알 수 있습니다. 항상 밀접하게 결합된 단일 레벨 구조를 만들 필요는 없습니다. 좋은 API와 올바른 정보를 노출하는 것이 효율성을 크게 향상시킬 수 있습니다.

- Bw-Tree와 LSS의 관계처럼, 서로 다른 역할을 하는 시스템 계층을 쌓는(stacking) 구조가 가지는 장점이 있음.
- 두 시스템이 하나의 거대한 시스템으로 합쳐져 있지 않아도(tightly coupled single-level), 각자의 역할을 하면서 좋은 API를 통해 필요한 정보(예: Bw-Tree의 시맨틱)를 서로에게 노출하고 공유하면 훨씬 더 효율적인 시스템을 만들 수 있음

### Open-Channel SSDs

소프트웨어 계층을 쌓는 것에 대한 대안은 모든 간접 계층을 건너뛰고 하드웨어를 직접 사용하는 것입니다. 예를 들어, 파일 시스템과 플래시 변환 계층을 사용하지 않고 오픈채널 SSD용으로 개발하는 것이 가능합니다. 이런 방식으로, 우리는 최소 두 개의 로그 계층을 피할 수 있고 마모 평준화, 가비지 컬렉션, 데이터 배치 및 스케줄링에 대해 더 많은 제어권을 가질 수 있습니다. 이 접근 방식을 사용하는 구현 중 하나는 LOCS(오픈채널 SSD 기반의 LSM 트리 키-값 저장소) [ZHANG13]입니다. 오픈채널 SSD를 사용하는 또 다른 예는 리눅스 커널에 구현된 LightNVM [BJØRLING17]입니다.

- 소프트웨어 계층을 중첩하는 대신, 파일 시스템과 같은 중간 추상화 계층(indirection layers)을 모두 제거하고 애플리케이션이 스토리지 하드웨어와 직접 통신하는 방법도 있음 → 복잡성 낮음
- 예시로 오픈 채널 SSD가 있음 → FTL을 제거하고 SSD의 내부 구조(물리적 페이지, 블록 등)를 호스트(애플리케이션)에 직접 노출하는 SSD임. 이를 통해 애플리케이션이 플래시 메모리를 직접 제어할 수 있게 됨.
- 제어권 확보: 애플리케이션이 직접 하드웨어를 제어하므로, SSD의 수명과 성능에 직결되는 핵심 기능들을 애플리케이션의 특성에 맞게 최적화할 수 있음.
  - 마모 평준화(Wear-leveling): 모든 플래시 메모리 셀이 균등하게 사용되도록 쓰기 작업을 분산시켜 SSD의 수명을 늘리는 기술.
  - 가비지 컬렉션(Garbage Collection): 유효하지 않은 데이터가 차지하는 공간을 회수하여 새로운 데이터를 쓸 수 있도록 만드는 과정.
  - 데이터 배치(Data placement): 데이터를 SSD의 물리적 공간 어디에 쓸지 결정하는 것.
  - 스케줄링(Scheduling): I/O 요청을 어떤 순서로 처리할지 결정하는 것.
- 두 가지 구현 사례 → LOCS, LightNVM

플래시 변환 계층은 일반적으로 데이터 배치, 가비지 컬렉션 및 페이지 재배치를 처리합니다. 오픈채널 SSD는 FTL을 거치지 않고 내부 구조, 드라이브 관리 및 I/O 스케줄링을 노출합니다. 이는 개발자 관점에서 훨씬 더 세부적인 주의를 요구하지만, 이 접근 방식은 상당한 성능 향상을 가져올 수 있습니다. 더 나은 제어권을 제공하지만 수동 페이지 관리가 필요한 커널 페이지 캐시를 우회하기 위해 O_DIRECT 플래그를 사용하는 것과 유사점을 찾을 수 있습니다.

- FTL 기능 복습
  - 데이터 배치(Data placement): 논리적 주소를 실제 플래시 메모리의 물리적 위치로 변환함.
  - 가비지 컬렉션(Garbage collection): 더는 유효하지 않은 데이터가 저장된 공간을 정리하여 재사용할 수 있도록 함.
  - 페이지 재배치(Page relocation): 가비지 컬렉션 과정에서 아직 유효한 데이터를 다른 곳으로 복사하는 작업임.
- 단점
  - FTL이 하던 모든 저수준(low-level) 제어를 개발자가 직접 구현해야 하므로 개발 복잡도가 크게 증가함.
- 장점
  - 애플리케이션의 데이터 접근 패턴에 가장 최적화된 방식으로 SSD를 제어할 수 있으므로, 일반 SSD보다 훨씬 높은 성능을 얻을 수 있음.
- O_DIRECT와 비슷. 커널 페이지 우회하는 대신 직접 디비 페이지 캐시를 구현했던…

소프트웨어 정의 플래시(SDF) [OUYANG14]는 하드웨어/소프트웨어 공동 설계된 오픈 채널 SSD 시스템으로, SSD의 특성을 고려한 비대칭 I/O 인터페이스를 노출합니다. 읽기 단위와 쓰기 단위의 크기가 다르며, 쓰기 단위 크기는 삭제 단위 크기(블록)와 일치하여 쓰기 증폭을 크게 줄입니다. 이 설정은 가비지 컬렉션을 수행하고 페이지를 재배치하는 소프트웨어 계층이 하나뿐이므로 로그 구조화 저장소에 이상적입니다. 또한, SDF의 모든 채널이 별도의 블록 장치로 노출되므로 개발자는 내부 SSD 병렬성에 접근하여 성능을 더욱 향상시킬 수 있습니다.

- '소프트웨어 정의 플래시(SDF)'는 기존 SSD처럼 FTL(Flash Translation Layer)이 모든 것을 관리하고 숨기는 대신, 하드웨어의 내부 구조와 동작 방식을 소프트웨어(애플리케이션)에 직접 노출하는 오픈 채널 SSD 기술임.
  - '하드웨어/소프트웨어 공동 설계' → 하드웨어의 특성을 소프트웨어가 인지하고 적극적으로 활용하여 최적화할 수 있도록 함께 설계되었다는 의미
  - '비대칭 I/O 인터페이스' → SSD의 물리적 특성상 읽기(read), 쓰기(program), 삭제(erase)의 단위와 방식이 모두 다른 점을 그대로 인터페이스에 반영했다는 뜻
- “읽기 단위와 쓰기 단위의 크기가 다르며, 쓰기 단위 크기는 삭제 단위 크기(블록)와 일치하여 쓰기 증폭을 크게 줄입니다.”
  - SDF는 쓰기 단위를 삭제 단위인 '블록' 크기와 맞춤. 이는 소프트웨어가 블록 단위로 데이터를 모아서 한 번에 쓰도록 유도하며, 불필요한 읽기-수정-쓰기 작업을 없애 쓰기 증폭을 획기적으로 줄일 수 있음.
  - SDF를 사용하면 FTL의 GC가 필요 없어지고, 데이터베이스 같은 상위 소프트웨어 계층이 직접 스토리지의 GC와 페이지 재배치를 관리하게 됨. 중복된 GC 작업이 사라져 매우 효율적이며, 특히 로그 구조화 저장(LSS) 방식과 잘 맞음.
- SSD는 내부에 여러 개의 낸드 플래시 칩(채널)을 가지고 있어 동시에 여러 작업을 처리할 수 있는 병렬성을 가짐.
  - SDF는 이 채널들을 각각 별개의 장치처럼 노출시켜, 개발자가 데이터를 여러 채널에 분산하여 동시에 기록함으로써 SSD의 하드웨어 성능을 최대한으로 끌어낼 수 있게 함.

단순한 API 뒤에 복잡성을 숨기는 것은 매력적으로 들릴 수 있지만, 소프트웨어 계층들이 서로 다른 의미 체계를 갖는 경우 복잡한 문제를 야기할 수 있습니다. 일부 하위 시스템 내부를 노출하는 것이 더 나은 통합에 도움이 될 수 있습니다.

- FTL이나 파일 시스템은 복잡한 하드웨어 특성을 숨기고 단순한 API를 제공하지만, 이것이 항상 좋은 것은 아님.
- 예를 들어, 데이터베이스와 파일 시스템이 모두 로그 구조화 방식을 사용하면 각자의 GC 타이밍과 정책이 충돌하여 비효율을 유발할 수 있음. 즉 '서로 다른 의미 체계'가 문제를 일으키는 상황
- “일부 하위 시스템 내부를 노출하는 것이 더 나은 통합에 도움이 될 수 있습니다.” → 하위 계층(SSD)의 내부 동작 방식을 무작정 숨기기보다, 상위 계층(데이터베이스)에 일부 노출하여 서로의 동작을 인지하고 최적화할 수 있도록 만드는 것이 전체 시스템의 성능 향상에 더 유리하다는 이야기. 이게 Mindful Stacking의 아이디어기도 함
- 같이 보면 좋은 소재) 누출된 추상화
