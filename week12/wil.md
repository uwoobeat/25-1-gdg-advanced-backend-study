# 고급 백엔드 스터디 12주차

## Bw-Trees

쓰기 증폭은 B-트리의 제자리 수정(in-place update) 구현에서 가장 중요한 문제 중 하나입니다. B-트리 페이지에 대한 후속 업데이트는 매 업데이트마다 디스크에 상주하는 페이지 사본을 업데이트해야 할 수 있습니다. 두 번째 문제는 공간 증폭입니다. 우리는 업데이트를 가능하게 하기 위해 추가 공간을 예약합니다. 이는 또한 요청된 데이터를 담고 있는 전송된 모든 유용한 바이트에 대해, 일부 빈 바이트와 페이지의 나머지 부분을 전송해야 함을 의미합니다. 세 번째 문제는 동시성 문제를 해결하고 래치를 다루는 복잡성입니다.

- 제자리 갱신은 세 가지 문제점이 있음
- 쓰기 증폭
  - 비트리에서는 페이지 단위로 읽고 쓰기를 수행함. 즉 실제로 변경이 발생한 부분이 바이트 단위더라도 페이지 단위로 통째로 다시 써야 함. 이걸 쓰기 증폭이라고 함
- 공간 증폭
  - 전주차에 불변성 이야기하면서 CoW가 공간 오버헤드를 줄이는 데 도움이 된다고 했었음. 왜? 원래 제자리 갱신은 미래의 삽입에 대비하여 노드를 꽉 채우지 않고 살짝 비워둠. 그래서 실제로 필요한 공간보다 더 많은 공간을 차지함. 이걸 공간 증폭이라 함. 하지만 Cow가 보장하는 불변성은 이미 쓰인 노드에 대해 수정을 허용하지 않기 떄문에 미리 공간을 확보해둘 필요가 없게 해줌
- 동시성 및 래치 복잡성
  - 이것도 전 주차에서 이야기 했었음. 제자리 갱신은 같은 페이지에 대한 동시성 작업 시 읽기와 쓰기가 물리적으로 같은 위치에 접근함. 쓰기 도중 읽거나 (읽기-쓰기 충돌) 쓰기가 겹치는 (쓰기-쓰기 충돌) 경우가 없도록 물리적 무결성 보장을 위해 래치를 잡아야 함. 이때 래치 크래빙 기법이라는 복잡한 구현이 필요하다는 아쉬운 점이 있었음

세 가지 문제를 한 번에 해결하기 위해, 우리는 지금까지 논의한 것과는 완전히 다른 접근 방식을 취해야 합니다. 업데이트를 버퍼링하는 것은 쓰기 및 공간 증폭에 도움이 되지만, 동시성 문제에 대한 해결책은 제공하지 못합니다.

- 이것도 전 주에 했음. Lazy B-Tree, 그 중에서도 WiredTiger는 쓰기 작업을 메모리에 모아뒀다가 쓰기 버퍼가 꽉 차면 플러시하는 방식으로 버퍼링을 수행함. 이를 통해 같은 노드에 대한 분할 / 병합을 지연시킬 수 있으며 불필요한 중가 단계 변경도 방지할 수 있었음. 이러한 버퍼링은 쓰기 증폭을 완화할 수 있음.
- 또한 배칭은 공간 증폭 완화에도 완화를 줌. 왜인가 싶겠지만, 우리가 공간 증폭은 결국 예상치 못한 분할을 위해서 공간을 절반 이상 비워두는 것이라는 사실을 알아야 함. 하지만 버퍼링을 하면 이런 분할과 병합을 지연시키므로 당연히 공간 증폭과도 관련이 있다는 생각을 해볼 수 있음. 더 자세히 알아보자면, 이것도 전에 배웠던 내용. 정렬된 데이터를 넣기 때문에 페이지 밀도가 더 높아짐. 따라서 공간 증폭이 완화됨.

우리는 추가 전용 스토리지를 사용하여 다른 노드에 대한 업데이트를 일괄 처리하고, 노드들을 체인으로 함께 연결하며, 단일 compare-and-swap 연산으로 노드 간의 포인터를 설치할 수 있게 해주는 인메모리 데이터 구조를 사용하여 트리를 락프리로 만듭니다. 이 접근 방식은 Buzzword-Tree(Bw-Tree)라고 불립니다 [LEVANDOSKI14].

- Bw-Tree의 특징을 알아보자
- append-only 스토리지로 모든 변경사항을 파일의 끝에 순차적으로 추가함. 이렇게 하면 모든 쓰기가 순차적으로 발생하므로 당연히 쓰기 증폭이 줄어듦
- 노드를 체인으로 연결
  - 원래 하나의 노드는 하나의 페이지와 대응됐었음. 하지만 Bw Tree에서는 조금 다름. 베이스 노드를 두고 이후 변경사항이 담긴 델타 노드를 연결 리스트로 구성함.
  - 그리고 새로운 변경이 발생하면 기존 체인 맨 앞에 델타 노드를 추가함. 그러면 전체 페이지를 다시 쓰지 않아도 됨
- 단일 CAS 연산을 통한 포인터 관리
  - CAS가 언급되었으니 당연히 동시성 이야기가 나올 거라는 예상.
  - 논리적 노드의 ID와 실제 디스크, 즉 가장 초신 델타 노드 주소를 매핑하는 인메모리 매핑 테이블이 존재함
  - 여러 스레드가 노드를 동시에 수정하려고 하면, 복잡한 래치 대신 CAS를 사용하여 매핑 테이블 포인터를 갱신함.
  - 즉 한 스레드가 CAS에 성공하면 수정사항이 반영되는 거고 실패하면 재시도함. CAS를 조금 더 자세히 보면, `cas(메모리 주소, 예상 값, 바꿀 값)` 과 같은 구조임. 이는 ‘메모리 주소에 있는 그 값이 예상 값과 일치하면 바꿀 값으로 원자적으로 교체하고, 불일치하면 실패해라’ 라는 뜻임.
  - 원래였다면 동시성 문제를 해결하기 위해 특정 스레드가 해당 페이지에 대한 래치를 걸고 다른 스레드가 접근하는 것을 막는 ‘블로킹’ 방식을 취했을 것임. 그리고 다른 스레드는 이 작업이 끝날 때까지 기다려야 함(wait). 당연히 데드락의 위험도 있음. 성능도 안 좋고…
  - 하지만 cas 이 방식은 다른 스레드가 수정을 수행하여 예상 값과 일치하지 않는 경우 바로 실패 결과를 전달받으므로 래치를 걸지 않아도 되며 이로 인해 다른 스레드가 작업이 완료될 때까지 기다리지 않아도 됨. 대신 실패하면 다시 시도하는 방식. 이런 걸 논블로킹 방식이라고 함.
  - 락을 쓰지 않는 것은 덤

### Update Chains

Bw-Tree는 베이스 노드를 그것의 수정사항과 분리하여 기록합니다. 수정사항(델타 노드)은 체인을 형성합니다: 가장 최신의 수정사항부터 오래된 수정사항들을 거쳐, 마지막에는 베이스 노드가 있는 연결 리스트입니다. 각 업데이트는 디스크 상의 기존 노드를 다시 쓸 필요 없이 별도로 저장될 수 있습니다. 델타 노드는 삽입, 업데이트(삽입과 구별할 수 없음), 또는 삭제를 나타낼 수 있습니다.

- Bw Tree는 원본에 해당하는 베이스 노드를 두고 수정사항을 분리해서 저장함. 즉 원본은 변경하지 않고 그대로 둔다.
- 수정사항은 델타 노드라고 부르는 작은 단위로 나눠서 관리함. 이 델타 노드는 링크드 리스트 형태로 연결되어 체인을 형성함. 체인의 맨 앞에는 가장 최신 변경사항이 위치하고, 포인터를 따라가면 이전 변경사항의 스냅샷이 순차적으로 존재하며 가장 끝으로 가면 원본에 해당하는 베이스 노드가 있음
  - 마치 블록체인 같이…
- 모든 수정사항은 절대 덮어쓰기로 발생하지 않으며 항상 별도 델타 노드로 저장됨. 이렇게 하면, 쓰기 작업이 랜덤한 위치에 쓰는 랜덤 쓰기가 아닌 항상 파일의 끝인 순차 쓰기가 됨. 순차 쓰기가 좋은 이유는 우리가 4장에서 다뤘었음. 오른쪽에 쓰기가 몰렸던 그 내용…
- 델타 노드는 모든 작업이 다 반영됨. 우리가 생각하는 CUD 작업. 즉 이전과 다른 새로운 스냅샷이면 다 됨. 사실 항상 뒤에 새로 맨 뒤에 삽입하기 때문에 수정과 삽입은 구별이 가지 않음.

베이스 노드와 델타 노드의 크기는 페이지에 정렬될 가능성이 낮기 때문에, 그것들을 연속적으로 저장하는 것이 합리적이며, 베이스 노드나 델타 노드 모두 업데이트 중에 수정되지 않기 때문에(모든 수정사항은 단지 기존 연결 리스트에 노드를 앞에 추가할 뿐입니다), 우리는 추가 공간을 예약할 필요가 없습니다.

- 하지만 스냅샷이라고 하기엔 뭔가 조금 다름. 베이스 노드와 델타 노드가 고정된 크기를 가지지 않아도 됨. 즉 가변 크기임. 즉 조금 수정했으면 변경사항도 작으므로 델타 노드는 작고 변경사항이 크면 델타 노드도 큼. 재밌는 것은 베이스 노드 역시 가변 크기라는 것인. 처음에 데이터를 적게 넣었으면 작게 많이 넣었으면 크게 저장함. 이는 우리가 항상 고정 페이지 단위로 써야 한다고 배웠던 것과는 상반되는 개념임. 이래도 괜찮을까?
- 우리가 페이지 단위로 썼던 건 단편화를 막기 위해서였음. 당연히 이걸 페이지 단위로 맞춰서 저장하면 단편화가 발생함. 그럼 어떻게 해야 할까. 그냥 순차적으로 이어붙여서 저장하면 됨. 앞뒤를 항상 이어붙여서 저장한다면 아무런 문제가 없는 거잖아? 같은 느낌. 마침 체인이니까 아무 문제도 없음.
- 추가로 Bw-Tree 핵심은 불변성임. 베이스 노드 및 델타 노드는 절대 수정되지 않음. 그래서 변경은 항상 링크드 리스트, 즉 파일 맨 끝에서 발생함. 랜덤 쓰기, 즉 향후 중간 삽입/수정으로 인한 오버플로를 걱정하지 않으므로 미리 공간을 남겨둘 필요도 없음. 공간 증폭 역시 발생하지 않는다
- 노드의 불변성을 보장하면 노드 일부 공간을 비워둘 필요가 없다는 것은 앞에서 이미 충분히 다뤘던 내용이므로 스킵

노드를 물리적 실체가 아닌 논리적 실체로 갖는 것은 흥미로운 패러다임 변화입니다: 우리는 공간을 미리 할당하거나, 노드가 고정된 크기를 갖도록 요구하거나, 심지어 연속적인 메모리 세그먼트에 보관할 필요도 없습니다. 이것은 확실히 단점이 있습니다: 읽기 중에, 실제 노드 상태를 재구성하기 위해 모든 델타를 순회하고 베이스 노드에 적용해야 합니다. 이것은 LA-Tree가 하는 것과 다소 유사합니다(“Lazy-Adaptive Tree” 참조): 업데이트를 주 구조와 분리하여 유지하고 읽기 시에 그것들을 재생합니다.

- 우리는 지금까지 노드와 페이지를 거의 동일한 의미로 사용했었음 (Page Structure 부분인가에서 이야기했었음)
- 그러한 점 때문에 우리가 B-Tree의 노드라고 하면 항상 고정된 크기를 가진 디스크 상의 (연속된) 물리적 공간을 생각하게 됐었음.
- 이러한 점 때문에 우리가 데이터를 추가하거나 수정하기 위해서는 항상 공간을 미리 확보했어야 했고, 페이지 단위로 읽고 써야 한다는 물리적 제약 하에서 동작했어야 했음. B-Tree의 노드는 어떠한 물리적 제약과는 관련이 없는 자료구조였음에도 말이다…!
- 하지만 우리는 편의상 지금까지 노드 == 페이지 개념을 유지해왔음. 왜? 딱히 그럴 이유가 없었으니까. 하지만 이제 와서야 논리적인 비트리의 노드와 물리적인 페이지를 분리해서 이해해야 할 필요성이 생김. Bw-Tree의 노드가 가지는 물리적 특성은 우리가 기존에 생각하는 온디스크 페이지가 가지는 물리적 특성과는 전혀 다르기 때문.
- Bw-Tree의 노드는 여러 개의 물리적 조각, 즉 베이스 노드와 여러 개의 델타 노드로 이어진 연결 리스트임. 각 노드는 가변 크기를 가지며 디스크에 흩어져서 저장될 수 있음. 따라서 매우 유연해진다는 장점.
- 하지만 단점도 있음. Bw-Tree는 쓰기 증폭을 비롯하여 쓰기 과정에서 발생하는 문제를 해결하기 위해 나온 만큼, 쓰기 중심 워크로드일때 이점을 가짐. 반대로 말하면 세상 모든 건 trade-off 이므로 읽기에서는 뭔가 손해를 본다는 것. Bw-Tree 역시 그러한데, 어떠한 데이터의 데이터 최신 상태를 읽기 위해서는 우리가 전통적인 온디스크 비트리 페이지에서 그랬던 것처럼 페이지 단위로만 딱 읽어오면 끝나지 않음. 가장 최신 델타 노드부터 베이스 노드까지 모든 연결 리스트를 읽어온 다음 변경사항을 순차적으로 적용해야 함.
- 이는 우리가 앞에서 했던 LA-Tree와 매우 비슷함. 다만 LA-Tree는 이걸 온디스크 페이지 단위를 유지하면서 원본 페이지는 그대로 냅둔 뒤 쓰기 버퍼를 두고 그 페이지에 변경사항을 순차로 반영하는 식으로 했었음
- ‘업데이트를 주 구조와 분리하여 유지하고 읽기 시 그것을 재생(replay)한다’ 라고 했는데 이것이 CQRS 하면 국밥같이 나오는 이벤트 소싱과 관련되어 있다는 것도 같이 알아두면 좋음

### Taming Concurrency with Compare-and-Swap

자식 노드에 아이템을 앞에 추가(prepending)하는 것을 허용하는 디스크 상의 트리 구조를 유지하는 것은 꽤 비용이 많이 들 것입니다: 이는 가장 최신 델타를 가리키는 포인터로 부모 노드를 계속해서 업데이트해야 하기 때문입니다. 이것이 델타 체인과 베이스 노드로 구성된 Bw-Tree 노드가 논리적 식별자를 갖고, 그 식별자로부터 디스크 상의 위치로의 인-메모리(in-memory) 매핑 테이블을 사용하는 이유입니다. 이 매핑을 사용하는 것은 또한 우리가 래치(latches)를 제거하는 데 도움을 줍니다: 쓰기 시간에 배타적 소유권을 갖는 대신, Bw-Tree는 매핑 테이블의 물리적 오프셋에 대해 비교-그리고-교체(compare-and-swap) 연산을 사용합니다.

- 이제 Bw-Tree가 1) 부모 노드의 잦은 업데이트 문제 그리고 2) 동시성 제어를 위한 래칭 문제를 어떻게 해결하는지를 알아보자. 핵심은 매핑 테이블이라는 간접적인 계층을 도입하는 것임.
- 첫 번째는 우리가 생각하는 연결 리스트의 구조와 관련있음. 연결 리스트는 현재 노드가 다음 노드를 가리키는 포인터를 가져야 한다는 점 정도는 알고 있을 거임.
- Bw-Tree는 업데이트 시 새로운 델타 노드를 기존 체인의 '앞에 추가(prepend)'함. 만약 부모 노드가 자식 노드의 물리적 주소를 직접 가리킨다면, 자식에게 새로운 델타가 추가될 때마다 자식 체인의 시작 주소가 바뀌므로 부모 노드의 포인터 값을 계속해서 수정해야 함.
- 부모 노드를 수정하는 것은 디스크에 직접 접근해 데이터를 덮어쓰는 '제자리 수정' 작업임. 이는 Bw-Tree가 피하고자 하는 비싼 연산이며, 이 수정이 연쇄적으로 상위 노드까지 전파될 수 있어 전체 시스템에 큰 비용을 유발함.
- 그래서 Bw-Tree는 직접적인 물리 주소 포인터 대신, 한 단계 추상화 계층을 추가함.
  - 먼저 모든 논리적 노드에 고유한 ID(ex: NodeID 101)를 부여함. 부모 노드는 자식의 물리적 주소가 아닌 이 논리적 ID를 저장함. 이 ID는 절대 변하지 않음.
  - 메모리에 '논리적 ID'와 '실제 디스크 상의 물리적 주소'를 연결해주는 매핑 테이블을 유지함. 이 테이블은 특정 논리적 노드가 현재 디스크의 어느 위치에 저장되어 있는지를 알려줌.
  - 자식 노드에 새로운 델타가 추가되어 물리적 주소가 바뀌더라도, 비싼 부모 노드를 수정할 필요가 없음. 대신, 메모리에서 빠르게 접근할 수 있는 **매핑 테이블의 해당 항목 하나만** 새로운 주소로 갱신하면 됨. 이로써 논리적인 트리 구조와 물리적인 데이터 배치가 분리됨.
- 이렇게 추가된 매핑 테이블은 추가적인 이점도 있음. 바로 동시성 제어 문제를 해결해준다는 것
  - 앞에서 간단히 언급했지만 다시 설명해보자면…
  - Bw-Tree는 래치 대신 CAS라는 원자적(atomic) 연산을 사용함. CAS는 "메모리 주소의 현재 값이 A와 같다면, 그 값을 B로 바꾸고 성공을 반환하라. 만약 A가 아니라면 아무것도 하지 말고 실패를 반환하라"는 작업을 CPU 수준에서 한 번에 처리함.
  - 스레드가 매핑 테이블의 주소를 바꾸려 할 때 CAS를 사용함. 만약 다른 스레드가 먼저 값을 변경했다면 CAS 연산은 실패하고, 해당 스레드는 작업이 충돌했음을 인지하고 재시도를 할 수 있음. 이 방식을 통해 여러 스레드가 잠금 없이도 안전하게 매핑 테이블을 업데이트할 수 있음. 자세한 내용은 섹션 소개 다시 볼 것

[그림 6-7]은 간단한 Bw-Tree를 보여줍니다. 각 논리적 노드는 하나의 베이스 노드와 여러 개의 연결된 델타 노드로 구성됩니다.

- 사진을 보면 감이 팍 온다. 그냥 하나의 노드가 링크드 리스트로 되어있는 게 보이는가?

Bw-Tree 노드를 업데이트하기 위해, 알고리즘은 다음 단계를 실행합니다:

1. 루트에서 리프까지 트리를 순회하여 대상 논리적 리프 노드를 찾습니다. 매핑 테이블은 대상 베이스 노드나 업데이트 체인의 가장 최신 델타 노드를 가리키는 가상 링크를 포함하고 있습니다.
2. 1단계에서 찾은 베이스 노드(또는 가장 최신 델타 노드)를 가리키는 포인터를 가진 새로운 델타 노드가 생성됩니다.
3. 2단계에서 생성된 새로운 델타 노드를 가리키는 포인터로 매핑 테이블이 업데이트됩니다.

3단계의 업데이트 연산은 원자적 연산인 비교-그리고-교체(compare-and-swap)를 사용하여 수행될 수 있으므로, 포인터 업데이트와 동시에 발생하는 모든 읽기는 읽는 스레드나 쓰는 스레드 어느 쪽도 차단하지 않으면서 쓰기 전 또는 후로 순서가 정해집니다. 쓰기 전으로 순서가 정해진 읽기는 이전 포인터를 따라가며 아직 설치되지 않은 새로운 델타 노드를 보지 못합니다. 쓰기 후로 순서가 정해진 읽기는 새로운 포인터를 따라가 업데이트를 관찰합니다. 만약 두 스레드가 동일한 논리적 노드에 새로운 델타 노드를 설치하려고 시도하면, 그중 하나만 성공할 수 있으며 다른 하나는 연산을 재시도해야 합니다.

- 먼저 1단계를 보자. 업데이트를 시작하기 위해 먼저 데이터가 위치한 논리적 리프 노드를 찾아야 한다.
  - 이 과정은 루트에서 시작해 자식 노드로 내려가는 전통적인 트리 순회와 유사하지만, 노드 간의 연결은 매핑 테이블에 저장된 가상 링크를 통해 이루어짐
  - 즉, 부모 노드는 자식 노드의 물리적 주소가 아닌 논리적 ID를 가지고 있으며, 이 ID를 통해 매핑 테이블에서 자식 노드 체인의 가장 최신 델타(또는 베이스) 노드의 실제 위치를 찾는 식
- 2단계. 대상 논리 노드를 찾았다면, 이제 실제 변경 사항을 담은 새로운 델타 노드를 메모리에 생성함
  - 이 새로운 델타 노드는 포인터를 통해, 1단계에서 찾았던 기존 체인의 가장 최신 노드(이전 델타 또는 베이스 노드)를 가리키도록 설정됨
  - 아까 전에 이전 델타 노드가 다음 델타 노드를 가리키면 안돼서 매핑 테이블 쓴 거 아닌가요? 맞음. 과거 노드가 미래 노드를 가리키는 건 안됨. 하지만 미래 노드가 과거 노드를 가리키는 건 아무 문제 없음.
- 3단계. 이렇게 수정사항이 담긴 델타 노드가 만들어졌다면 이제 매핑 테이블을 업데이트 해야 함.
  - CAS 연산을 통해서 매핑 테이블 포인터를 우리가 새로 만든 델타 노드 주소로 ‘원자적’ 업데이트 함.
  - 이렇게 하면 업데이트가 진행 중일 때 읽기 요청이 오더라도 멈추지 않음. 왜? CAS 이전의 읽기는 이전 포인터를 통해 과거 데이터를 읽을 것이고, CAS 이후의 읽기는 새로운 포인터를 통해 신규 데이터를 읽게 될 것. 어느 쪽이든 (래칭을 쓰지 않으므로) 블로킹되지 않음
  - 만약 두 개의 스레드가 동시에 같은 논리적 노드를 업데이트하려고 시도하면, 두 스레드 모두 각자의 새로운 델타 노드를 만들고 CAS를 시도함. 이 중 먼저 CAS 연산을 실행한 스레드만 성공하고, 다른 스레드는 매핑 테이블의 값이 이미 변경되었으므로 CAS에 실패함. 실패한 스레드는 자신의 연산을 포기하고, 변경된 최신 상태를 다시 읽어와 업데이트 과정을 재시도함 (논블로킹)

### Structural Modification Operations

Bw-Tree는 논리적으로 B-Tree처럼 구조화되어 있는데, 이는 노드들이 여전히 너무 커지거나(오버플로우) 거의 비게 되어(언더플로우) 분할(split) 및 병합(merge)과 같은 구조 변경 연산(SMO)을 필요로 할 수 있음을 의미합니다. 여기서 분할과 병합의 의미(semantics)는 B-Tree의 그것과 유사하지만(B-Tree 노드 분할" 및 "B-Tree 노드 병합" 참조), 그 구현 방식은 다릅니다.

- 구조 자체는 비트리와 비슷하지만 그 노드를 구성하는 물리적 구현이 일반적인 비트리와 다르다는 점을 다뤘었음. 그러면 일반적인 비트리에서 오버플로나 언더플로가 발생했을 때 분할과 병합을 다루는 방식 역시 다를 것이라는 예상이 가능.
  - 하나 참고해야 할 점은 이 챕터 앞 부분에서 우리가 비트리로서의 공통점과 변형 비트리로서의 차이점을 언급했다는 사실임. 즉 비트리라면 당연히 분할과 병합은 발생함. 분할과 병합이 생기지 않는다면 비트리가 아님. 그 방식이 다를 뿐…
  - 책에서는 ‘분할과 병합이라는 목표와 논리적 결과는 같지만, 구현 방식이 다르다’ 라는 표현을 쓰고 있음

분할 SMO는 분할 대상 노드의 논리적 내용을 통합하고, 델타들을 베이스 노드에 적용하며, 분할점의 오른쪽에 있는 요소들을 포함하는 새로운 페이지를 생성하는 것으로 시작합니다. 이 후, 프로세스는 두 단계로 진행됩니다:

1. **분할** — 진행 중인 분할에 대해 읽기 스레드들에게 알리기 위해 특별한 분할 델타 노드가 분할 대상 노드에 추가됩니다. 이 분할 델타 노드는 분할 대상 노드의 레코드들을 무효화하기 위한 중간 분리 키와, 새로운 논리적 형제 노드를 가리키는 링크를 가집니다.
2. **부모 업데이트** — 이 시점에서 상황은 Blink-Tree의 절반 분할("Blink-Trees" 참조)과 유사합니다. 왜냐하면 노드가 분할 델타 노드 포인터를 통해 접근 가능하지만 아직 부모에 의해 참조되지 않고 있으며, 읽기 스레드들은 새로운 형제 노드에 도달하기 위해 이전 노드를 거쳐 형제 포인터를 따라가야 하기 때문입니다. 읽기 스레드들이 분할 대상 노드를 통해 리디렉션되는 대신 직접 도달할 수 있도록 새로운 노드가 부모 노드의 자식으로 추가되고, 이로써 분할이 완료됩니다.

- 분할을 시작하기 전, 먼저 분할 대상이 되는 논리적 노드의 현재 상태를 확정해야 함. 해당 노드의 업데이트 체인에 있는 모든 델타 노드들을 원본인 베이스 노드에 적용하여 하나의 상태로 만듦. 이를 통합(consolidating) 이라고 함.
- 통합된 데이터 집합을 분할점(split point) 을 기준으로 둘로 나누어, 분할점 오른쪽의 데이터들을 담을 새로운 페이지(형제 노드) 를 미리 생성함.
- 이제 두 단계로 진행됨.
  - 1단계 → 분할
    - 목표는 진행 중인 구조 변경을 다른 스레드에게 알리는 것
    - 기존 노드의 업데이트 체인 가장 앞에 ‘분할 델타 노드’ 라는 특별한 마킹 노드를 원자적으로(atomically) 추가함. 두 가지 정보를 가짐
      - 중간 분리 키 → 지금 이 키보다 큰 데이터는 더 이상 이 (오래된) 노드에 없으니, 다른 곳에서 찾아라"는 의미의 경계 값. 이를 통해서 기존 노드에 있던 데이터의 절반을 논리적으로 무효화시킴
      - 새로운 논리적 형제 노드 링크 → 새로운 형제 노드를 찾아갈 수 있는 포인터임
    - 이 분할 델타 노드가 추가된 순간부터, 이 노드에 접근하는 모든 스레드는 분할이 진행 중임을 인지하고, 필요시 형제 노드까지 찾아갈 수 있게 됨
  - 2단계 → 부모 업데이트
    - 실제로 분할된 노드를 이제 트리의 일부로 만들어야 함. 지금은 분할 델타 노드만 추가된 상태고 새로 분할된 형제 노드는 아직 부모에 반영 안됨.
    - Blink-Tree 절반 분할과의 비슷한 점이 있음. 분할 델타 노드만 추가된 상태에서는, 부모 노드는 아직 새로운 형제 노드의 존재를 모름. 새로운 형제 노드에 접근하려면 반드시 이전 노드를 거쳐 분할 델타의 형제 링크를 따라가야만 함. 이는 부모가 아직 모르는 "절반만 완료된 분할" 상태라는 점에서 Blink-Tree의 개념과 유사함
    - 부모 노드의 업데이트 체인에 새로운 델타를 추가하여, **새로운 형제 노드를 가리키는 포인터**를 기록함

부모 포인터를 업데이트하는 것은 성능 최적화입니다: 모든 노드와 그 요소들은 부모 포인터가 업데이트되지 않더라도 계속 접근 가능합니다. Bw-Tree는 래치-프리(latch-free)이므로, 어떤 스레드든 불완전한 SMO(구조 변경 연산)를 마주칠 수 있습니다. 해당 스레드는 다음 작업을 진행하기 전에 여러 단계의 SMO를 이어받아 마무리함으로써 협력해야 합니다. 다음 스레드는 설치된 부모 포인터를 따라갈 것이므로 형제 포인터를 거칠 필요가 없을 것입니다.

- 부모 포인터에 새로운 형제 노드를 업데이트시켜주는 게 필수는 아니고 성능 최적화에 불과함.
- 왜 필수가 아니냐? 새롭게 추가된 노드는 부모 포인터에 반영되지 않더라도 왼쪽에 있는 노드를 통해서형제 노드 포인터를 통해 접근 가능하기 때문임.
- Bw Tree는 래치 프리, 즉 래치를 사용하지 않음. 그래서 한 스레드가 여러 단계의 구조 변경 연산, 즉 SMO를 하게 되면 그 중간 상태를 다른 스레드가 관찰하게 될 수도 있음. 왜? 1단계까지만 원자적이었지 1단계과 2단계가 원자적일 수는 없기 때문.
- 이 경우 두 번째 스레드는 기다리는 대신 (당연히 래치가 없고 블로킹도 아니니 기다리지는 않을 것) ‘협력적으로’ 다른 스레드가 하던 작업을 이어서 마무리함
- 만약 A 스레드가 노드 분할을 수행하고, 분할 델타 노드만 현재 체인 맨 앞에 마킹해놨다고 하자. 그리고 B 스레드가 여기에 도착했다면? B 트리는 부모 노드 업데이트를 자기가 처리해버림
- 즉 ‘중간 상태’를 처음 목격한 스레드가 바로 그 현장을 수습해버려서 다음 스레드가 ‘중간 상태’를 못 보게 작업을 완료해버리는 것.

병합 SMO도 비슷한 방식으로 작동합니다:

1. **형제 노드 제거** — 병합 SMO의 시작을 알리고 오른쪽 형제 노드를 삭제 대상으로 표시하기 위해 특별한 `제거 델타 노드`가 생성되어 오른쪽 형제 노드에 추가됩니다.
2. **병합** — 왼쪽 형제 노드에 `병합 델타 노드`가 생성되어 오른쪽 형제 노드의 내용을 가리키고, 이를 왼쪽 형제 노드의 논리적인 일부로 만듭니다.
3. **부모 업데이트** — 그 시점에서, 오른쪽 형제 노드의 내용은 왼쪽 노드를 통해 접근 가능합니다. 병합 과정을 끝내기 위해, 부모로부터 오른쪽 형제 노드를 가리키는 링크가 제거되어야 합니다.

- 병합도 비슷함. 앞에서는 ‘분할 델타 노드’ 마킹 후 ‘부모 업데이트’였음. 여기서는…
  - 1단계 → 형제 노드 제거
    - 비슷하게 ‘제거 델타 노드’ 를 ‘없어질 오른쪽 형제 노드’에 추가
    - 이 델타 노드의 역할은 "이 노드는 곧 삭제될 거고, 더 이상 새로운 데이터를 추가하거나 수정하지 마십셔"라고 다른 스레드들에게 알리는 것
    - 이로써 오른쪽 노드를 삭제 대상으로 표시(marking) 하고 병합 작업의 시작을 기록
  - 2단계 → 병합
    - 이제 살아남을 왼쪽 형제 노드에 ‘병합 델타 노드’ 를 추가
    - 이 델타 노드는 오른쪽 형제 노드의 내용 전체를 가리키는 포인터를 가지고 있음
    - 이 작업이 완료되면, 왼쪽 노드를 통해 오른쪽 노드의 모든 데이터에 접근할 수 있게 됨. 즉, 두 노드가 하나의 논리적 단위로 합쳐짐 (하나의 링크드 리스트가 됨)
  - 3단계 → 부모 업데이트
    - 2단계가 끝나면, 이미 모든 데이터는 왼쪽 노드를 통해 접근 가능함.
    - 이제 부모 노드에 있는, 더 이상 필요 없어진 오른쪽 형제 노드로의 링크(포인터) 를 제거하여 트리 구조를 최종적으로 정리함
    - 분할(Split)과 마찬가지로, 이 작업은 시스템의 정확성이 아닌 성능 최적화를 위한 것. 이 작업이 완료되면 다른 스레드들이 더 이상 존재하지 않는 노드로 접근하려는 시도를 하지 않게 되어 효율성이 높아짐

동시성 SMO는 부모 노드에 추가적인 `중단 델타 노드(abort delta node)`를 설치하여 동시적인 분할 및 병합을 방지해야 합니다. `중단 델타`는 쓰기 잠금(write lock)과 유사하게 작동합니다: 한 번에 하나의 스레드만 쓰기 접근 권한을 가질 수 있으며, 이 델타 노드에 새로운 레코드를 추가하려는 모든 스레드는 중단됩니다. SMO가 완료되면, `중단 델타`는 부모로부터 제거될 수 있습니다.

Bw-Tree의 높이는 루트 노드 분할 중에 증가합니다. 루트 노드가 너무 커지면, 둘로 분할되고, 이전 루트와 새로 생성된 형제 노드를 자식으로 하는 새로운 루트가 이전 루트의 자리에 생성됩니다.

- 중단 델타 노드?
  - Bw-Tree는 래치(latch)가 없는(latch-free) 구조이므로, 여러 스레드가 동시에 Bw-Tree의 같은 부분에 접근하여 구조를 변경(분할 또는 병합)하려고 시도할 수 있음.
  - 이러한 동시적 구조 변경은 데이터의 일관성을 깨뜨릴 수 있는 경쟁 상태(race condition)를 유발함.
  - 이를 해결하기 위해, 구조 변경 연산(Structural Modification Operation, SMO)을 시작하려는 스레드는 먼저 변경이 일어날 노드의 부모 노드에 '중단 델타 노드'라는 특별한 종류의 델타 노드를 설치함.
  - 이 '중단 델타 노드'는 일종의 잠금(lock) 역할을 하여 다른 스레드가 동시에 해당 부모 아래에서 또 다른 구조 변경을 시작하는 것을 막음.
- 어떻게?
  - 한 스레드가 부모 노드에 중단 델타를 성공적으로 설치하면, 그 스레드는 해당 노드에 대한 배타적인 쓰기 권한(구조 변경 권한)을 획득함.
  - 이 상태에서 다른 스레드가 같은 부모 노드에 또 다른 델타 노드(구조 변경을 위해서…)를 추가하려고 시도하면 그 작업은 실패하고 '중단(abort)'됨.
  - 이를 통해 특정 서브트리에 대한 구조 변경 작업이 한 번에 하나씩 순차적으로 일어나도록 보장함.
  - 완료 이후
    - 구조 변경 작업(분할 또는 병합)이 성공적으로 완료되면, 잠금 역할을 하던 '중단 델타 노드'는 부모 노드에서 제거됨.
    - 중단 델타가 제거되면 다른 스레드들이 다시 해당 노드에 대한 구조 변경 작업을 시도할 수 있게 됨.
- 높이 변화
  - 일반적인 B-Tree와 마찬가지로, 리프 노드나 내부 노드의 분할은 트리를 수평적으로 확장시킬 뿐 높이를 증가시키지 않음.
  - 오직 최상위 계층인 루트 노드가 분할될 때만 트리 전체의 높이가 1 증가함.
  - 과정
    1. 데이터가 계속 추가되어 루트 노드가 가득 차면, 루트 노드를 두 개의 노드로 분할함.
    2. 분할 기준이 된 키(separator key)를 가진 새로운 루트 노드를 생성함.
    3. 기존의 루트 노드(이제 내용물의 절반만 가짐)와 분할을 통해 새로 만들어진 형제 노드는 모두 새로운 루트 노드의 자식 노드가 됨.

### Consolidation and Garbage Collection

델타 체인은 별도의 조치 없이 임의로 길어질 수 있습니다. 델타 체인이 길어질수록 읽기 비용이 비싸지기 때문에, 델타 체인의 길이를 합리적인 범위 내에서 유지해야 합니다. 설정 가능한 임계값에 도달하면, 기본 노드의 내용과 모든 델타를 병합하여 노드를 재구성하고, 이를 하나의 새로운 기본 노드로 통합합니다. 그러면 새로운 노드는 디스크의 새로운 위치에 기록되고 매핑 테이블의 노드 포인터는 해당 위치를 가리키도록 업데이트됩니다. 기본이 되는 로그 구조 저장소가 가비지 컬렉션, 노드 통합 및 재배치를 담당하므로, 이 과정은 'LLAMA와 Mindful Stacking'에서 더 자세히 논의합니다.

- 델타 체인을 무한정 늘릴 수는 없음. 왜? 가장 최신 상태를 알기 위해서는 베이스 노드에 델타 노드를 모조리 반영해야 하는데 델타 노드가 많다면 읽기가 비싸짐. 따라서 델타 체인 길이를 적절히 조정해야 함.
- 앞에서 베이스 노드에 델타 노드를 합치는 걸 병합 또는 통합(consolidation) 이라고 했었음. 즉 하나의 베이스 노드로 만들어버리는 것.
- 그러면 새로운 노드는 어떻게 하냐? 당연히 append-only이니 이 병합된 노드는 새로운 위치에 쓰여짐. 그리고 매핑 테이블이 거기를 가리키도록 업데이트
- 그렇다면 기존 델타 체인 및 베이스 노드는 더 이상 유효하지 않기 때문에 회수되어야 함.
- 이 모든 내용은 뒤에서 다룸…

노드가 통합되자마자, 그것의 오래된 내용(기본 노드와 모든 델타 노드)은 더 이상 매핑 테이블에서 주소 지정되지 않습니다. 하지만, 진행 중인 작업에 의해 여전히 사용되고 있을 수 있기 때문에, 그것들이 차지하는 메모리를 즉시 해제할 수는 없습니다. 읽기 작업에 의해 유지되는 래치가 없으므로(읽기 작업자는 노드에 접근하기 위해 어떠한 종류의 장벽도 통과하거나 등록할 필요가 없었으므로), 우리는 활성 페이지를 추적할 다른 수단을 찾아야 합니다.

- 역시 동시성 문제가 있음. 이걸 알아보자.
- 노드가 통합(consolidate)되는 순간 메모리 매핑 테이블은 새로운 클린 베이스 노드를 가리키도록 업데이트 됨.
- 하지만 그렇다고 해서 오래된 스레드 중에서 그 가비지 베이스 노드 / 델타 노드 체인을 참조하고 있는 게 있을 수도 있음.
  - 이게 앞에서 LMDB에서도 문제였음. LMDB에서는 freelist + reader table를 통해 가장 오래 전에 해제된 대상이 현재 읽기 트랜잭션이 만들어진 시간보다 오래됐으면 절대 이게 참조되고 있을 가능성이 없으니까 해제하는 전략을 취했었음
- LMDB도 그렇고 이것도 그렇고 래치 프리하기 때문에 활성 페이지를 추적할 수 있어야 함. 어렵게 말하긴 했는데 LMDB처럼 ‘이걸 절대 안 쓴다’ 라는 보장이 있어야 한다는 것.

특정 노드를 마주쳤을 수 있는 스레드와 절대 보았을 리 없는 스레드를 분리하기 위해, Bw-Tree는 에포크 기반 회수(epoch-based reclamation)라는 기법을 사용합니다. 만약 어떤 노드와 델타들이 특정 에포크 동안 그것들을 대체한 통합(consolidation)으로 인해 매핑 테이블에서 제거되었다면, 원본 노드들은 동일한 에포크 또는 그 이전 에포크에서 시작된 모든 읽기 작업자가 끝날 때까지 보존됩니다. 그 이후에는, 나중에 시작한 읽기 작업자들은 그들이 시작할 시점에 해당 노드들이 주소 지정될 수 없었기 때문에 그 노드들을 절대 본 적이 없다고 보장되므로, 안전하게 가비지 컬렉션될 수 있습니다.

- ‘봤을 법한 스레드’와 ‘절대 봤을 리 없는 스레드’를 분리하기 위한 방법이 에포크 기반 회수
- 에포크가 뭐냐? 그냥 항상 증가하는 시간 번호표 같은 거라고 생각하면 됨. 모든 스레드는 작업 시점에 에포크 번호를 부여받음. 이걸 써먹는 것.
- 목적은 스레드를 두 개로 구분
  - 봤을 법한 스레드 → 즉 오래된 데이터를 잠재적으로 참조할 수도 있는 ‘과거’ 스레드
  - 절대 봤을 리 없는 스레드 → 즉 오래된 데이터를 모를 수밖에 없음이 보장되는 ‘미래’ 스레드
- 어떻게?
  - 노드 통합으로 인해 가비지, 즉 낡은 베이스 노드 및 델타 체인이 생김. 그러면 이 가비지가 언제 발생했는지에 대한 에포크를 기록. 즉 “에포크 10에 가비지 발생” 기록
  - 가비지를 바로 회수하지 않고 보관
  - “에포크 10” 이전, 즉 “에포크 9, 8, …”에 시작됐던 모든 스레드가 작업이 끝날 때까지 기다림.
  - 지금 시점에서 가장 오래된 스레드가 “에포크 11”이라면 “에포크 10”에 가비지 처리된 노드를 절대 봤을 리가 없을 것
- 비유하자면…
  - 1900년에 태어나서 1999년 까지 산 A라는 사람이 있다고 하자
  - 2000년부터 시작해서 지금 2025년에 태어난 사람들은 절대 A를 본 적이 없을 것임
  - 하지만 2000년 이전에 태어났다면 A를 봤을 법도 할 것임. 1899년에 태어난 B는 A를 본 적이 있을 수도 있음. 한편 1950년에 태어난 C도 A를 봤을 수도 있음.
- LMDB랑 똑같음. 그냥 에포크를 쓰냐 freelist / reader-table을 쓰냐 차이인 거지…

Bw-Tree는 쓰기 증폭(write amplification), 논블로킹 접근(nonblocking access), 그리고 캐시 친화성(cache friendliness)과 같은 몇 가지 중요한 측면에서 개선을 이룬 흥미로운 B-Tree 변종입니다. 수정된 버전이 실험적인 스토리지 엔진인 Sled에 구현되었습니다. CMU 데이터베이스 그룹은 OpenBw-Tree라고 불리는 Bw-Tree의 인메모리 버전을 개발했으며, 실용적인 구현 가이드[WANG18]를 발표했습니다. 우리는 이 장에서 B-Tree와 관련된 더 높은 수준의 Bw-Tree 개념들만 다루었으며, 'LLAMA와 Mindful Stacking'에서 기본이 되는 로그 구조 저장소에 대한 논의를 포함하여 그것들에 대한 논의를 계속할 것입니다.

- Bw-Tree가 해결한 문제를 정리해보자
- 쓰기 증폭 → 제자리 갱신 대신 append-only 방식을 통해 불필요한 쓰기를 줄일 수 있음
- 논블로킹 → 래치를 쓰지 않고 원자적 연산과 에포크 기반 회수를 통해 동시성을 보장
- 캐시 친화성 → 이건 왜 안나왔지…?
- sled라는 스토리지 엔진으로도 구현됐다고도 함. 그리고 OpenBw-Tree도 개발된 적 있다고 함
- 하지만 지금까지는 Bw-Tree의 B-Tree적 특징에 집중해서 다루었음. 하지만 이를 더 잘 이해하기 위해서는 로그 구조 저장소에 대해 알아야 함. 이건 7장에서 등장.

## Cache-Oblivious B-Trees

블록 크기, 노드 크기, 캐시 라인 정렬 및 기타 구성 가능한 매개변수는 B-Tree 성능에 영향을 미칩니다. 캐시 부주의(cache-oblivious) 구조 [DEMAINE02]라고 불리는 새로운 종류의 자료 구조는 기반이 되는 메모리 계층 구조와 이러한 매개변수를 조정할 필요성에 관계없이 점근적으로 최적의 성능을 제공합니다. 이는 알고리즘이 캐시 라인, 파일시스템 블록, 디스크 페이지의 크기를 알 필요가 없다는 것을 의미합니다. 캐시 부주의 구조는 서로 다른 구성을 가진 여러 머신에서 수정 없이 잘 수행되도록 설계되었습니다.

- 비트리 성능은 하드웨어 상에서 조정할 수 있는 파라미터에 영향을 많이 받음. 가령…
  - 블록 크기 → 디스크 블록 크기로 비트리 페이지를 구성했음.
  - 노드 크기 → 비트리 팬아웃을 결정하고, 이는 높이를 결정하며, 다시 탐색 횟수와 IO 횟수에 영향을 줌
  - 캐시라인 정렬 → 캐시라인 단위로 데이터를 가져오기 때문에 데이터를 캐시라인에 맞춰 정렬해야 캐시 효율이 좋아짐
  - 이걸 하드웨어 별로 잘 튜닝해야 비트리가 효율적으로 작동할 수 있음. 반대로 말하면 하드웨어마다 다르게 세팅해줘야 하므로 이식성이 별로라는 것
- 캐시 무시 자료구조는 이러한 하드웨어 파라미터를 알 필요 없이도 잘 작동할 수 있게 해줌.
  - ‘점근적으로(asymptotically) 최적 성능’을 발휘한다고 했는데, 이게 데이터 크기가 무한히 커지면 이론적으로 최상의 성능을 낸다. 즉 하드웨어 튜닝 없어도 최적 성능을 보장한다 라는 뜻.
- 따라서 이식성이 높아지므로 다양한 유형의 하드웨어에서 별다른 수정 없이도 잘 돌아감.

지금까지 저희는 주로 2단계 메모리 계층(two-level memory hierarchy) 관점에서 B-Tree를 살펴보았습니다 (“Copy-on-Write”에서 설명한 LMDB는 예외입니다). B-Tree 노드는 디스크 상주 페이지(disk-resident pages)에 저장되며, 주 메모리(main memory)에서 효율적인 접근을 허용하기 위해 페이지 캐시(page cache)가 사용됩니다. 이 계층의 두 단계는 페이지 캐시(더 빠르지만 공간이 제한적임)와 디스크(일반적으로 더 느리지만 용량이 더 큼)입니다 [AGGARWAL88].

- 2단계 메모리 계층 → 즉 메인 메모리(RAM)와 스토리지(HDD, SDD)를 말하는 것. LDMB는 조금 달랐음. 왜? LMDB는 mmap을 썼으니… 애플리케이션 레벨에서 DB 페이지 캐시 쓰는 대신 OS 페이지 캐시를 썼음. 그래서 이 설명하고는 잘 안 맞아서 스킵
- 비트리 노드는 디스크 상주 페이지에 저장 / 메인 메모리에서의 페이지 캐시 → 즉 노드가 디스크 페이지에 저장된다는 이야기. 하지만 디스크에서 데이터를 읽는 건 매우 비싸기 때문에 한번 읽은 건 메모리에 올려둠. 이게 페이지 캐시 (혹은 버퍼 풀) 이라 했었음. 그리고 항상 페이지 캐시를 거치게 한다는 것 역시 배웠음.
- 정리하자면 하나는 빠른 대신 공간이 작고 하나는 느린 대신 공간이 큼.

디스크는 블록으로 분할되고, 데이터는 디스크와 캐시 사이에서 블록 단위로 전송됩니다. 알고리즘이 블록 내의 단일 항목을 찾아야 할 때조차도 전체 블록이 로드되어야 합니다. 이 접근 방식은 캐시 인식(cache-aware) 방식입니다.

- HDD든 SSD든 OS에 의해 블록 단위로 관리됨. 블록 단위라는 건 운영체제가 최소한 이 단위로만 읽고 쓴다는 것. (데이터베이스는 페이지 단위였다)
- OS 역시 블록 내에 일부만 찾고 싶어도 전체 블록을 메모리로 로드해야 함.
- 이걸 캐시 인식(cache-aware) 방식이라고 함. 왜 캐시 인식이라고 하냐면, 우리가 알고리즘을 짤 때 캐시, 즉 하드웨어의 작동 방식을 명확하게 ‘인식하고’ 하드웨어 성능 최적화를 위해 의도적으로 그런 방식으로 짰다는 걸 드러내기 위함임.
  - 이러한 블록 단위 읽고 쓰기가 캐시 인식인 이유를 더 알아보자면…
  - 공간 지역성 (즉 근처 것까지 같이 가져오는) 개념도 있고
  - IO 횟수 줄이기 (디스크 접근은 비싸므로 접근 횟수를 줄이려는 목적 담김)
  - 이 모든 게 이 알고리즘이 캐시 인식, 즉 하드웨어와 관련된 컨텍스트를 인지한 상태에서 만들어졌다는 걸 의미함

성능이 중요한 소프트웨어를 개발할 때, 저희는 종종 CPU 캐시와 때로는 디스크 계층(핫/콜드 스토리지 또는 HDD/SSD/NVM 계층을 구축하고 한 단계에서 다른 단계로 데이터를 이동시키는 것과 같은)까지 고려하는 더 복잡한 모델을 위해 프로그래밍합니다. 대부분의 경우 이러한 노력은 일반화하기 어렵습니다. “메모리 기반 DBMS와 디스크 기반 DBMS 비교(Memory- Versus Disk-Based DBMS)”에서, 저희는 디스크 접근이 주 메모리 접근보다 몇 배나 더 느리다는 사실에 대해 이야기했으며, 이는 데이터베이스 구현자들이 이러한 차이에 맞춰 최적화하도록 동기를 부여했습니다.

- 성능을 극한까지 최적화하기 위해서 앞에서 봤던 2단계 모델, 즉 메모리(주기억장치)와 스토리지(보조기억장치) 사이의 차이만 고려하는 게 아니라, 그 이상까지 고려해야 하는 경우가 많음.
  - 가령 보조기억장치 역시 SSD / HDD / NVM으로 매우 다양함. 이 셋은 물리적 특징이 매우매우 다름. 근데 접근 빈도(핫/콜드)에 따라서 이 셋을 섞어서 쓰기도 함.
  - 또 CPU와 메모리 사이에도 L1 L2 L3 다양함.
  - 이걸 모두 일반화해서 최적화하기에는 어려움. 다양한 요소들이 상호작용하는 trade-off를 만들어냄
- 결국 핵심 원인은 뭐냐? 우리가 1장에서 말했듯 메모리 기반과 디스크 기반 DBMS 사이에서 엄청난 성능 차이가 존재하기 때문임. 그걸 극복하기 위해서 우리가 페이지 캐시, 버퍼링, 그 외 복잡한 계층 등 다양한 최적화 기법을 배워왔던 것

캐시 망각 알고리즘(Cache-oblivious algorithms)은 다단계 계층 모델의 이점을 제공하면서도, 2단계 메모리 모델의 관점에서 자료 구조에 대해 추론하는 것을 허용합니다. 이 접근 방식은 플랫폼별 매개변수가 없어도 계층의 두 단계 사이의 전송 횟수가 상수 배수 이내임을 보장합니다. 만약 자료 구조가 임의의 두 메모리 계층 단계에 대해 최적으로 수행되도록 최적화되어 있다면, 이는 두 인접한 계층 단계에 대해서도 최적으로 작동합니다. 이는 가능한 한 가장 높은 캐시 단계에서 작업함으로써 달성됩니다.

- 캐시 무시 알고리즘은 이러한 하드웨어 세부 정보를 몰라도 전반적으로 효율적으로 작동하게 하는 것. 어떻게? 위에서 말했듯, CPU/L1/L2/L3, 주기억장치, 보조기억장치(SSD, HDD, NVM) 등 복잡한 계층을 ‘빠른 메모리’와 ‘느린 메모리’로 추상화해서 분석하는 것. 이 둘 사이에서 데이터 전송 횟수를 최소화하면 실제 환경에서도 충분히 괜찮지 않을까? 라는 아이디어.
- “이 접근 방식은 플랫폼별 매개변수가 없어도 계층의 두 단계 사이의 전송 횟수가 상수 배수 이내임을 보장합니다.”
  - 즉 캐시라인 크기나 디스크 페이지 크기를 알고리즘에 전혀 사용하지 않음
  - 그렇지만 임의의 두 계층 사이에서 발생하는 데이터 전송(I/O) 횟수가 가장 최적 값의 상수 배임을 수학적으로 보장함. 만약 10번이 가장 최적이라면 10번 \* 상수 배인 것. 데이터 전송 횟수는 보통 매우 빠른 시간 안에 끝나기 때문에 상수 배에서는 유의미한 차이를 보이지 않음. 빅오에서 상수 무시했던 것 생각해보자. (물론 리얼월드에서는 그까잇 상수 배가 아주 중요하다는 이야기를 앞에서 하긴 했지만…… 일단 넘어가자)
  - 하드웨어를 바꿔도 항상 준수한 성능을 낸다는 게 핵심
- “만약 자료 구조가 임의의 두 메모리 계층 단계에 대해 최적으로 수행되도록 최적화되어 있다면, 이는 두 인접한 계층 단계에 대해서도 최적으로 작동합니다.”
  - 이게 많이 어려움. 재귀적 최적화라는 개념을 알아야 함. 그리고 캐시 무시 알고리즘의 핵심이기도 함. 만약 인접한 계층인 L1과 L2 사이에서 우리가 최적으로 동작하도록 설계했다고 해보자. 그러면 동일한 알고리즘이 L2과 L3 사이에서, 그리고 RAM과 SSD 사이에서도 최적으로 동작한다는 뜻임.
  - 그리고 뒤에서 나올 van Emde Boas 레이아웃이라는 재귀적 분할 기법을 통해 데이터를 배치함으로써 이를 구현할 수 있음.
- “이는 가능한 한 가장 높은 캐시 단계에서 작업함으로써 달성됩니다.”
  - 그러면 우리가 특정 작업을 수행할 때 필요한 데이터가 가능한 한 가장 빠른 메모리에 함꼐 있도록 만들 수 있음.
  - 알고리즘을 간단하게 알아보면, 일단 한번 상위 캐시로 데이터를 가져오면, 그 데이터 내에서 최대한 많은 작업을 처리한 다음 하위 계층으로의 접근을 최소화하는 방식을 동작함. 이렇게 하면 메모리 계층 간 이동을 줄일 수 있음

### van Emde Boas Layout

캐시 망각 B-Tree(cache-oblivious B-Tree)는 정적 B-Tree(static B-Tree)와 팩트 배열(packed array) 구조로 구성됩니다 [BENDER05]. 정적 B-Tree는 van Emde Boas 레이아웃을 사용하여 구축됩니다. 이는 간선(edge)의 중간 레벨에서 트리를 분할합니다. 그런 다음 각 하위 트리(subtree)는 비슷한 방식으로 재귀적으로 분할되어, 크기가 sqrt(N) 인 하위 트리를 생성합니다. 이 레이아웃의 핵심 아이디어는 모든 재귀적인 트리가 연속적인 메모리 블록에 저장된다는 것입니다.

- 캐시 무시 비트리는 두 부분으로 구성됨
  - 정적 비트리
    - 상위 레벨에서 인덱스 역할을 하고, 거의 변하지 않는다고 가정하고 만들어짐. vEB 레이아웃으로 구성됨. vEB는 논리적으로 가까우면 물리적으로도 가깝게 배치하며 캐시 효율을 높임. 다시 설명
  - packed 배열
    - 최하위 레벨에서 실제 데이터를 저장. 리프 노드같은 역할. 삽입 삭제가 가능하도록 동적으로 관리. 간단하게만 보고 스킵
- vEB 레이아웃
  - “이는 간선(edge)의 중간 레벨에서 트리를 분할합니다. 그런 다음 각 하위 트리(subtree)는 비슷한 방식으로 재귀적으로 분할되어, 크기가 sqrt(N) 인 하위 트리를 생성합니다.”
    - 재귀적 분할이 핵심임. 다시 설명하자면…
    - 전체 트리를 높이 중간 지점에서 잘라, 윗부분과 여러 개의 아랫부분으로 나눔
    - 각 아랫부분의 서브트리에 대하여 이를 재귀적으로 반복함
    - 그러면 sqrt(N) 크기의 서브트리가 만들어짐.
  - “이 레이아웃의 핵심 아이디어는 모든 재귀적인 트리가 연속적인 메모리 블록에 저장된다는 것입니다.”
    - 이렇게 하면 6-8 그림에서 볼 수 있듯이 모든 분할된 서브트리가 메모리 상 연속된 공간에 저장됨. 이러면 로컬리티가 좋아짐

그림 6-8에서 van Emde Boas 레이아웃의 예시를 볼 수 있습니다. 논리적으로 함께 그룹화된 노드들은 물리적으로 가깝게 배치됩니다. 위쪽에서는 논리적 레이아웃 표현(즉, 노드가 어떻게 트리를 형성하는지)을 볼 수 있고, 아래쪽에서는 트리 노드가 메모리와 디스크에 어떻게 배치되는지를 볼 수 있습니다.

- 위의 트리에서 점선으로 표시된 논리적 레이아웃, 즉 노드가 어떻게 배치되는지와 아래에서 점선으로 표시된 물리적 레이아웃, 즉 데이터가 어떻게 저장되는지가 서로 관련성이 있다는 걸 확인 가능

자료 구조를 동적으로 만들기 위해(즉, 삽입, 업데이트, 삭제를 허용하기 위해), 캐시 망각 트리는 팩트 배열(packed array) 자료 구조를 사용합니다. 이는 요소를 저장하기 위해 연속적인 메모리 세그먼트를 사용하지만, 향후 삽입될 요소를 위해 예약된 간격(gap)을 포함합니다. 간격은 밀도 임계값(density threshold)에 따라 배치됩니다. 그림 6-9는 간격을 만들기 위해 요소들이 배치된 팩트 배열 구조를 보여줍니다.

이 접근 방식은 더 적은 재배치로 트리에 항목을 삽입하는 것을 허용합니다. 항목들은 새로 삽입될 요소를 위한 간격이 없는 경우에만, 그 간격을 만들기 위해 재배치되어야 합니다. 팩트 배열이 너무 빽빽해지거나 희소해지면, 배열을 늘리거나 줄이기 위해 구조를 재구축해야 합니다.

- CUD 연산을 처리하기 위해서 캐시 무시 트리는 packed 배열이라는 걸 씀.
  - 연속적인 메모리 공간에 저장됨
  - 갭 존재 → 데이터 사이에 의도적으로 빈 공간을 둠. 새 데이터가 삽입될 때 사용하기 위해서.
  - 삽입 효율성 → 삽입 시 정렬된 배열처럼 모든 배열을 뒤로 한 칸씩 밀 필요가 없고 그냥 빈 간격 아무 데나 넣으면 되기 때문에 비용이 저렴.
- 간격이 어떻게 정해지냐?
  - 너무 빽빽해져도 문제고(삽입 공간 부족) 너무 듬성듬성해도 문제임(메모리 낭비)
  - 이걸 위해서 밀도 임계값이 존재함
  - 만약 특정 구간 밀도가 임계값을 넘으면 그 구간의 모든 요소를 재배치하여 다시 갭을 균등하기 만들어줌. 이것도 비용이 들긴 하지만 다른 작업보다는 훨씬 쌈. 그리고 한 쪽에서 드는 비용을 나눠서 받게 해주는 효과도 있음.
- 재배치
  - 만약 맨 앞에 데이터가 추가되면 모든 배열을 한 칸씩 밀어야 하는데 그거에 비하면 일부 구간을 갭 껴서 재배치하는 건 아주 선녀같이 저렴한 작업
  - 왜냐면 전체 배열을 재배치하는 게 아니라 주변 일부만 띄엄띄엄 되도록 지역적으로 발생하는 작업이기 때문임

정적 트리는 최하위 레벨의 팩트 배열을 위한 인덱스로 사용되며, 최하위 레벨의 올바른 요소를 가리키도록 재배치된 요소에 맞춰 업데이트되어야 합니다.

- 정적 트리는 실제 데이터가 어디있는지 알려주는 역할. 그래서 탐색을 빠르게 할 수 있게 해주는 게 목표.
- 반면 packed 배열은 실제 데이터가 위치하는 곳. 삽입과 삭제가 발생하는 곳.
- 우리가 탐색을 위해서는 정적 트리를 거쳐서 해당 데이터가 팩드 배열의 어디에 위치하는지 포인터 정보를 얻어야 함.
- 하지만 팩트 배열은 동적이기에 (우리가 페이지 포맷하면서 했던 것처럼) 데이터가 삽입 삭제되며 때로는 재구축을 통해 기존 요소들도 바뀜. 따라서 인덱스가 가리키는 주소가 올바른 위치를 가리키도록 동기화해줄 필요가 있음.
- 재배치가 발생하면 당연히 그 변경사항을 정적 트리에 반영해줘야 함.

이는 흥미로운 접근 방식이며, 여기에서 얻은 아이디어들은 효율적인 B-Tree 구현을 구축하는 데 사용될 수 있습니다. 이는 주 메모리용 구조가 구축되는 방식과 매우 유사한 방식으로 디스크 상주 구조를 구축할 수 있게 해줍니다. 하지만, 이 글을 쓰는 시점에서, 저는 학술적인 용도가 아닌 캐시 망각 B-Tree 구현에 대해서는 알지 못합니다.

- 캐시 무시 비트리는 좋은 아이디어를 많이 갖고 있음
- 원래 전통적인 온디스크 비트리는 하드웨어적 고려사항이 많음. 하지만 이런 접근법을 취하면 더 단순하게 설계 가능할지도 모름
- 근데 아직까지는 이론 단계고 실제 구현은 없음 ㅎ << 장난함…?

이에 대한 한 가지 가능한 이유는 캐시 로딩이 추상화될 때, 데이터가 블록 단위로 로드되고 다시 쓰이더라도 페이징과 축출은 여전히 결과에 부정적인 영향을 미친다는 가정입니다. 또 다른 가능한 이유는 블록 전송의 관점에서 볼 때, 캐시 망각 B-Tree의 복잡도는 그것의 캐시 인식 대응물과 동일하다는 것입니다. 이는 더 효율적인 비휘발성 바이트 주소 지정 가능 저장 장치가 더 널리 보급되면 바뀔 수 있습니다.

- 캐시 무시 알고리즘은 블록 크기를 모른다고 했었음. 그래서 메모리의 캐싱 동작을 추상화하여 세부적으로 고려하지 않음. 하지만 실제에서는 우리가 메모리 퇴출(eviction)을 통해 메모리가 꽉 차면 페이지를 퇴출시킴. 다양한 퇴출 정책들이 존재하며 성능에도 영향을 줌
- 퇴출 정책에서 중요한 건 어떤 페이지가 다시 쓰일지임. cache-aware이 좋았던 건 어떤 페이지를 피닝, 즉 고정하는 게 좋을지 알려줄 수 있다는 거였음. 근데 캐시 무시는 이런 최적화가 불가능함. 이론의 한계가 그럼 그렇지…
- “또 다른 가능한 이유는 블록 전송의 관점에서 볼 때, 캐시 망각 B-Tree의 복잡도는 그것의 캐시 인식 대응물과 동일하다는 것입니다.”
  - 온디스크 자료구조 성능은 보통 디스크 IO 횟수로 측정됨
  - 근데 캐시 무시 알고리즘의 ‘점근적’ IO 복잡도는 O(log{B}{N})으로 일반적인 B-Tree 복잡도와 동일함. 즉 기존 구현과 비교했을 때 크게 다를 바가 없음
- “이는 더 효율적인 비휘발성 바이트 주소 지정 가능 저장 장치가 더 널리 보급되면 바뀔 수 있습니다.”
  - NVM 같은 게 잘 보급되면 블록 단위에 더더욱 덜 얽메이게 될 테니 캐시 무시 구조가 떡상하지 않을까 하는 행복회로를 굴리고 있음
