# 고급 백엔드 스터디 14주차

## Read, Write, and Space Amplication

최적의 압축 전략을 구현할 때, 우리는 여러 요소를 고려해야 합니다. 한 가지 접근법은 중복된 레코드가 차지하는 공간을 회수하고 공간 오버헤드를 줄이는 것이며, 이는 테이블을 계속해서 다시 작성함으로써 발생하는 더 높은 쓰기 증폭을 야기합니다. 다른 대안은 데이터를 계속해서 다시 작성하는 것을 피하는 것이며, 이는 읽기 증폭(읽기 중에 동일한 키와 연관된 데이터 레코드를 조정하는 오버헤드)과 공간 증폭(중복 레코드가 더 오랜 시간 동안 보존되기 때문)을 증가시킵니다.

- 컴팩션 수행 시 고려사항
- LSM 트리에서는 특정한 키에 대하여 여러 개의 값이 존재할 수 있다고 했음. 뿐만 아니라 삭제 마커(tombstone)도 여러 sstable에 나눠져서 저장됨. 컴팩션을 자주 돌려서 하나의 버전으로 통합할 수 있음. 이게 우리가 지금까지 배운 방법임
- 이렇게 하면 공간 증폭은 낮출 수 있지만 컴팩션이 돌면서 매번 새로운 쓰기를 수행해야 함. 즉 컴팩션을 한다는 것은 우리가 저장해야 하는 데이터와 실제로 저장되는 데이터 크기가 달라진다는 것. 실제로 필요한 것보다 많이 쓰는 것을 쓰기 증폭이라고 함. 즉 컴팩션은 쓰기 증폭의 원인이 됨
- 다른 방법이 있음. 컴팩션을 조금 약하게 돌려서 쓰기 증폭을 줄이는 것. 대신 쓰기 증폭을 줄인 댓가로 읽기 증폭이 증가함. 왜? 압축을 최대한 땡기면 하나의 SSTable만 보면 되지만, 압축을 덜 하면 아예 안했을 때보단 덜하지만 그도 여러 개의 SSTable을 봐야 하기 때문임. 그리고 컴팩션 직후에는 아예 없었던 조정(reconcile) 과정도 다시 필요.
- 또한, 우리가 컴팩션을 통해 해소했던 공간 증폭도 다시 증가하게 됨

노트) 데이터베이스 커뮤니티의 주요 논쟁 중 하나는 B-Tree와 LSM-Tree 중 어느 쪽이 더 낮은 쓰기 증폭(write amplification)을 갖는지에 관한 것입니다. 두 경우 모두에서 쓰기 증폭의 원인을 이해하는 것은 극히 중요합니다. B-Tree에서는 쓰기 증폭이 쓰기 후 기록(writeback) 작업과 동일 노드에 대한 후속 업데이트에서 비롯됩니다. LSM-Tree에서는 쓰기 증폭이 압축(compaction) 중 한 파일에서 다른 파일로 데이터를 이전하는 과정에서 발생합니다. 이 둘을 직접적으로 비교하면 잘못된 가정으로 이어질 수 있습니다.

- 그래서 둘 중 쓰기 증폭 뭐가 덜 발생함? 이라는 질문 하면 안됨. 하지만 애초부터 비교가 애매한 부분. 앞에서 설명했듯이 비트리와 LSM트리에서 쓰기 증폭이 발생하는 원인도 다르고 이걸 해결하기 위한 방법도 다르기 때문임.
- 비트리에서는 두 가지 이유로 발생함
  - writeback 작업 → 변경된 데이터를 페이지 버퍼에 적용하고 페이지 버퍼를 플러시하는 구조로 동작하므로, 항상 페이지 단위로 쓰기를 수행. 셀 하나만 수정하더라도 전체 페이지를 수정해야 하므로 쓰기 증폭 발생
  - 동일 노드에 대한 후속 업데이트 → 같은 노드에 대하여 수정이 계속 발생하면, 매번 같은 페이지를 계속 써야 함.
- LSM 트리에서는 아래 이유로 발생
  - LSM 트리는 불변 구조이고 append-only 구조임. 하나의 키에 대해 여러 값 존재 가능. 그래서 여러 버전을 머지할 때 새로운 SSTable로 써야 함. 즉 변경된 값을 썼는데도 추가로 컴팩션 과정에서 또 써야 함. 그래서 쓰기 증폭이 발생
- 단기적으로는 append-only 방식으로 작동하는 LSM 트리의 쓰기 성능이 좋아보이지만 백그라운드 컴팩션 작업이 쓰로풋에 영향을 줄 수 있음. 하지만 B-Tree는 단기적인 쓰기 작업이 페이지 단위이기 때문에 비용이 비쌀 수도 있지만 장기적으로는 추가적인 쓰기 오버헤드를 만들지 않기 때문에 예측 가능한 성능을 보여줌.

요약하자면, 데이터를 불변의(immutable) 방식으로 디스크에 저장할 때, 우리는 세 가지 문제에 직면합니다:

**읽기 증폭 (Read amplification)**
데이터를 검색하기 위해 여러 테이블을 참조해야 할 필요성에서 발생합니다.

**쓰기 증폭 (Write amplification)**
압축(compaction) 프로세스에 의한 지속적인 재작성으로 인해 발생합니다.

**공간 증폭 (Space amplification)**
동일한 키와 연관된 여러 레코드를 저장하는 것에서 비롯됩니다.

우리는 챕터의 나머지 부분에서 이들 각각을 다룰 것입니다.

- LSM 트리의 불변성 때문에 세 가지 증폭 문제가 발생함
  - 읽기 증폭 → 우리가 원하는 데이터는 여러 개의 SSTable에 존재
  - 쓰기 증폭 → 컴팩션 과정에서 실제로 쓴 데이터 외에 추가적인 쓰기 발생
  - 공간 증폭 → 하나의 키에 대해 여러 버전 데이터가 존재 가능

### RUM Conjecture

RUM 추측(Conjecture) 스토리지 구조를 위한 대중적인 비용 모델 중 하나는 읽기(Read), 업데이트(Update), 메모리(Memory) 오버헤드라는 세 가지 요소를 고려합니다. 이를 RUM 추측 [ATHANASSOULIS16]이라고 합니다.

- 럼 추측은 이런 세 가지 요소를 종합적으로 고려하여 평가하는 틀을 말함 → 읽기(R) 업데이트(U) 메모리(M)
- R → 하나의 데이터를 읽기 위해서 얼마나 많은 추가 작업이 필요한지
- U → 하나의 데이터를 쓰거나 수정하기 위해 얼마나 많은 추가 작업이 필요한지
- …

RUM 추측은 이 오버헤드 중 두 가지를 줄이면 필연적으로 나머지 하나의 오버헤드가 악화되며, 최적화는 세 가지 매개변수 중 하나를 희생해야만 이루어질 수 있다고 말합니다. 우리는 이 세 가지 매개변수를 기준으로 다양한 스토리지 엔진을 비교하여 어떤 것을 최적화하는지, 그리고 이것이 어떤 잠재적 트레이드오프를 의미하는지 이해할 수 있습니다.

- 핵심은 세 요소 간의 트레이드오프 관계임. 두 가지 요소 간의 트레이드오프 관계를 딜레마라고 했던 것처럼, 세 요소 트레이드오프 관계를 트릴레마라고 함.
- 왜 이걸 해야 하냐? → 특정 스토리지 엔진이 어떤 워크로드 (읽기인지 쓰기인지) 에 적합한지 판단하는데 도움이 됨. 가령 우리 서비스의 메인 워크로드가 read-intensive하다면 RUM 세 요소 중 뭘 우선시할까 고민해볼 수 있음

이상적인 해결책은 낮은 메모리 및 쓰기 오버헤드를 유지하면서 가장 낮은 읽기 비용을 제공하는 것이지만, 현실적으로 이는 달성할 수 없으며 우리는 트레이드오프(trade-off)에 직면하게 됩니다.

- RUM을 모두 최적화하고 싶겠지만 그걸 못하기 때문에 트레이드오프 관계인 거고 트릴레마라고 하는 것

B-Tree는 읽기에 최적화되어 있습니다. B-Tree에 쓰기 위해서는 디스크에서 레코드를 찾아야 하며, 동일한 페이지에 대한 후속 쓰기는 디스크의 페이지를 여러 번 업데이트해야 할 수 있습니다. 향후 업데이트 및 삭제를 위해 예약된 추가 공간은 공간 오버헤드를 증가시킵니다.

- 비트리는 R 최적화. 대신 W을 희생함. 쓰려면 항상 먼저 읽어야 하고, 어디에 쓸 지 알아야 함. M도 마찬가지로 희생. 삽입에 대비해 여유 공간을 둬야 함,

LSM 트리는 쓰기 중에 디스크에서 레코드를 찾을 필요가 없으며 향후 쓰기를 위해 추가 공간을 예약하지 않습니다. 중복 레코드를 저장하는 데서 오는 약간의 공간 오버헤드는 여전히 존재합니다. 기본 구성에서는 완전한 결과를 반환하기 위해 여러 테이블에 접근해야 하므로 읽기 비용이 더 비쌉니다. 그러나 이 챕터에서 논의하는 최적화 기법들이 이 문제를 완화하는 데 도움이 됩니다.

- LSM 트리는 W 최적화. 맨 뒤에다가 순차적으로 쓰기만 하면 됨. M도 나쁘지 않음. 불변이므로 데이터를 꽉 채워서 수정할 수 있음. 하지만 중복 데이터 존재하므로 이건 손해. R은 확실한 희생. 모든 memtable와 SSTable을 다 뒤져야 함.
- R을 최적화하기 위한 방법들을 알아보게 될 것

B-Tree에 대한 챕터에서 보았고 이 챕터에서도 보게 될 것처럼, 다양한 최적화를 적용하여 이러한 특성들을 개선할 방법들이 있습니다.

- 비트리가 W를 희생했다고 해서 그냥 있던 게 아니라 W를 최적화하기 위한 여러 방법을 배웠었음. LSM 트리도 그렇게 할 것임

이 비용 모델은 지연 시간(latency), 접근 패턴, 구현 복잡성, 유지보수 오버헤드, 그리고 하드웨어 관련 특성과 같은 다른 중요한 지표들을 고려하지 않기 때문에 완벽하지 않습니다. 일관성 영향 및 복제 오버헤드와 같이 분산 데이터베이스에 중요한 상위 수준의 개념들도 고려되지 않습니다. 그러나 이 모델은 스토리지 엔진이 무엇을 제공하는지 이해하는 데 도움을 주기 때문에, 첫 번째 근사치 및 경험적 규칙으로 사용될 수 있습니다.

- 그래서 RUM이 꼭 완벽한 모델인 건 아님. 실제로는 더 많은 변수들을 고려해야 하기 때문. 레이턴시, 접근 패턴(실제로 어떤 워크로드가 많은지, 어떤 식의 접근이 발생하는지), 구현 복잡성(구현하는데 너무 어려우면 좀…),메인터넌스 오버헤드(컴팩션이나 리밸런싱 같은 백그라운드 프로세스의 비용도 따져야), 하드웨어 특성(HDD, SSD, NVM) 따져야 함
- 뿐만 아니라 분산 환경도 고려해야 함
- 하지만 특정 스토리지 엔진이 뭘 얻었고 뭘 포기했는지 대략적으로 이해하기엔 좋음

## Implementation Details

우리는 LSM 트리의 기본적인 역학, 즉 데이터가 어떻게 읽히고, 쓰이고, 압축되는지에 대해 다루었습니다. 하지만 많은 LSM 트리 구현체가 공통적으로 가지고 있어 논의할 가치가 있는 다른 몇 가지 사항들이 있습니다. 메모리 및 디스크 기반 테이블이 어떻게 구현되는지, 보조 인덱스가 어떻게 작동하는지, 읽기 중에 접근하는 디스크 기반 테이블의 수를 줄이는 방법, 그리고 마지막으로 로그 구조 저장소와 관련된 새로운 아이디어들입니다.

- 앞에서 읽기 / 쓰기 / 압축 세 가지 기본적인 동작에 대해 배움
- 이 세 가지 동작 외에도 LSM 구현체들이 공통적으로 고려해야 하는 내용들을 알아볼 것
  - 메모리 및 디스크 기반 테이블 구현 (즉 memtable, sstable 구현)
  - 보조 인덱스 → PK 말고 다른 필드로 검색하게 해주는 보조 인덱스를 어떻게 구현하고 관리하는지?
  - 읽기 중 접근하는 SSTable 줄이기 → 읽기 증폭 줄이기
  - 로그 구조 저장소 관련 새로운 아이디어 → …

### Sorted String Tables

지금까지 LSM 트리의 계층적이고 논리적인 구조(메모리와 디스크 상주 컴포넌트로 구성된다는 점)에 대해 논의했지만, 디스크 상주 테이블이 어떻게 구현되고 그 디자인이 나머지 시스템과 어떻게 함께 작동하는지에 대해서는 아직 논의하지 않았습니다.

- 하나는 메모리 하나는 디스크지만 구체적인 구현은 아직 안다뤘음

디스크 상주 테이블은 종종 정렬된 문자열 테이블(SSTable)을 사용하여 구현됩니다. 이름에서 알 수 있듯이 SSTable의 데이터 레코드는 키 순서로 정렬되고 배치됩니다. SSTable은 일반적으로 인덱스 파일과 데이터 파일의 두 가지 컴포넌트로 구성됩니다. 인덱스 파일은 B-트리와 같이 로그 시간 조회를 허용하는 구조나 해시테이블과 같이 상수 시간 조회를 허용하는 구조를 사용하여 구현됩니다.

- 디스크 상주 테이블 → SSTable로 구현
- 정렬된 문자열이라는 이름에서 알 수 있듯 모든 키-값 엔트리는 키를 기준으로 정렬된 상태로 저장됨
- 두 부분으로 나눠짐
  - 데이터 파일 → 실제 데이터가 저장된 곳. 정렬돼있음
  - 인덱스 파일 → 키를 찾기 위해 O(1)이 가능한 구조가 되어야 함. 해시 테이블 같이…

데이터 파일이 키 순서로 레코드를 보유하기 때문에, 인덱싱에 해시테이블을 사용하더라도 범위 스캔을 구현하는 것을 막지는 않습니다. 왜냐하면 해시테이블은 범위 내의 첫 번째 키를 찾는 데만 접근하고, 범위 자체는 범위 조건자가 일치하는 동안 데이터 파일에서 순차적으로 읽을 수 있기 때문입니다.

- SSTable과 해시 테이블 인덱스가 어떻게 상호작용할까?
- 해시테이블은 O(1)로 키를 접근할 수 있으므로 범위 스캔 시작점을 찾기에 좋음.
- 그리고 데이터 파일은 키 순서 정렬이기 때문에 범위 스캔 시작점을 찾고 나서 순차적으로 읽기만 하면 됨. 즉 해시 테이블 자체는 정렬되어 있지 않기에 범위 스캔에 불리할 것 같지만, 해시 테이블로 범위 스캔 시작점을 찾고 데이터 파일에서 범위 스캔을 수행하면 유리함. 테크놀로지아…

인덱스 컴포넌트는 키와 데이터 엔트리(실제 데이터 레코드가 위치한 데이터 파일 내의 오프셋)를 보유합니다. 데이터 컴포넌트는 연결된 키-값 쌍으로 구성됩니다. 3장에서 논의했던 셀 디자인과 데이터 레코드 형식은 SSTable에 대체로 적용 가능합니다. 여기서 주요 차이점은 셀이 순차적으로 쓰여지고 SSTable의 수명 주기 동안 수정되지 않는다는 것입니다.

- 인덱스 파일 → 키와 데이터 엔트리 (데이터 파일의 어디서부터 시작하는지에 대한 오프셋)
- 데이터 컴포넌트 → 그냥 키-값 쌍의 나열 형태.
- 3장에서 바이너리 인코딩.. 가변 길이 처리, 셀 구조 등 일반적인 파일 포맷팅이 여기서도 대부분 적용 가능
- 하지만 차이는? 불변성. 그리고 순차 쓰기.

memtable이 플러시될 때, 그 내용은 디스크에 쓰여지고, SSTable 기본 키 인덱스와 함께 보조 인덱스 파일이 생성됩니다. 인덱스 파일은 데이터 파일에 저장된 데이터 레코드에 대한 포인터를 보유하므로, 인덱스가 생성될 때 그 오프셋을 알고 있어야 합니다.

- 인덱스는 memtable이 플러시되는 그 시점에 sstable과 함께 생성됨. 기본 키 인덱스 말고 보조 인덱스도 같이 생성
- 인덱스는 그 데이터가 어디있는지 가리키고 있는 것이므로 인덱스를 만들기 위해서는 먼저 어디에 저장될지가 확정되어야 함. 즉 데이터 파일에 데이터를 쓰고 → 오프셋이 결정되고 → 이걸 가지고 인덱스를 만든다는 것

컴팩션(compaction) 중에는 데이터 파일의 데이터 레코드가 이미 정렬되어 있으므로, 인덱스 컴포넌트를 참조하지 않고 순차적으로 읽을 수 있습니다. 컴팩션 중에 병합되는 테이블들은 동일한 순서를 가지고 있고, 병합-반복(merge-iteration)은 순서를 보존하므로, 결과로 나오는 병합된 테이블 역시 한 번의 실행으로 데이터 레코드를 순차적으로 기록하여 생성됩니다. 파일이 완전히 쓰여지자마자, 그것은 불변(immutable)으로 간주되며, 디스크에 상주하는 그 내용은 수정되지 않습니다.

- 여러 개의 SSTable을 합치는 컴팩션 도중에는 이미 정렬된 상태이기 때문에 인덱스를 참조하지 않아도 됨. 이거에 대해서는 전 주에 배웠었음. 머지 소트 같은 느낌으로..
- 컴팩션 이후 만들어진 파일도 불변.

Note) SSTABLE 연결 보조 인덱스

LSM 트리 인덱싱 분야의 흥미로운 발전 중 하나는 아파치 카산드라에 구현된 SSTable-연결 보조 인덱스(SASI)입니다. 기본 키뿐만 아니라 다른 어떤 필드로도 테이블 내용을 인덱싱할 수 있도록, 인덱스 구조와 그 수명 주기는 SSTable의 수명 주기와 연결되며, 인덱스는 SSTable 당 하나씩 생성됩니다. memtable이 플러시될 때, 그 내용은 디스크에 쓰여지고, SSTable 기본 키 인덱스와 함께 보조 인덱스 파일이 생성됩니다.

- LSM 트리를 사용하는 아파치 카산드라에서 보조 인덱스를 구현하기 위해 SASI 라는 기술을 씀
- 보조 인덱스 → 기본 키 말고 다른 필드로도 빠르게 조회하는 것.
- 인덱스의 수명주기는 SSTable의 수명 주기와 연결됨. 즉 앞에서 말했듯 memtable 플러시 되고 SSTable이 생성되면서 기본 키 인덱스와 보조 인덱스가 같이 만들어짐

LSM 트리는 데이터를 메모리에 버퍼링하고 인덱스는 디스크 상주 콘텐츠뿐만 아니라 메모리 상주 콘텐츠에 대해서도 작동해야 하므로, SASI는 memtable 콘텐츠를 인덱싱하는 별도의 인메모리 구조를 유지합니다.

- LSM 트리의 경우 데이터를 읽기 위해서는 SSTable 말고 memtable도 봐야 함. 그래서 디스크 데이터 말고 메모리 데이터도 같이 인덱싱해야 한다는 요구사항이 있음.
- 그래서 디스크에 있는 SSTable 인덱스와 별개로(!) memetable 전용 인덱스 구조를 인메모리로 관리함

읽기 중에는 인덱스 내용을 검색하고 병합하여 검색된 레코드의 기본 키를 찾고, 데이터 레코드는 일반적으로 LSM 트리에서 조회가 작동하는 방식과 유사하게 병합 및 조정됩니다.

- 읽기는 어떻게?
  - 먼저 여러 보조 인덱스를 모두 검색하여 기본 키를 찾는다. 그리고 각 인덱스의 결과를 병합한다
  - 그리고 다시 일반적인 LSM 트리와 동일하게 읽기를 수행한다. 즉 여러 SSTable 및 memtable을 참고하여 값을 결정하는 reconcile 작업이 포함됨

SSTable 수명 주기에 편승하는 것의 장점 중 하나는 memtable 플러시 또는 컴팩션 중에 인덱스를 생성할 수 있다는 것입니다.

- 왜 굳이 SSTable 수명주기에 같이 하냐? 어차피 하는 김에 겸사겸사가 된다는 것. 어차피 우리는 플러시나 컴팩션 과정에서 모든 데이터를 다 읽어야 했음. 이 단계에서 인덱스를 만들면 겸사겸사 가능함.

### Bloom Filters

LSM 트리에서 읽기 증폭(read amplification)의 원인은 읽기 작업을 완료하기 위해 여러 디스크 상주 테이블(disk-resident tables)을 참조해야 한다는 것입니다. 이는 디스크 상주 테이블이 검색된 키에 대한 데이터 레코드를 포함하고 있는지 미리 항상 알 수 없기 때문에 발생합니다.

- 읽기 증폭의 핵심은 우리가 찾는 키에 대하여 SSTable을 직접 확인하기 전까지 그 키가 안에 존재하는지 알 수 없다는 것

테이블 조회를 방지하는 한 가지 방법은 메타데이터에 키 범위(해당 테이블에 저장된 가장 작은 키와 가장 큰 키)를 저장하고, 검색된 키가 해당 테이블의 범위에 속하는지 확인하는 것입니다. 이 정보는 부정확하며 데이터 레코드가 테이블에 존재할 수 있는지 여부만 알려줄 수 있습니다. 이 상황을 개선하기 위해 Apache Cassandra 및 RocksDB를 포함한 많은 구현에서는 블룸 필터(Bloom filter)라는 데이터 구조를 사용합니다.

- 범위 메타데이터 활용. SSTable 생성 시 가장 큰 값과 작은 값 메타데이터로 같이 기록.
- 읽기 요청 오면 검색 키가 그 범위 안에 포함되는지 확인. 범위 밖이라면 스킵
- ‘없는지’는 확실하게 알려줄 수 있지만 ‘있는지’는 확실하게 알려줄 수 없음. 즉 ‘확실한 존재성’을 보장 X
  - 아래 케이스 가능
    - 범위에 포함되고 진짜 있음
    - 범위에 포함되지만 없을 수도 있음 → 이게 문제
  - 그래서 범위에 포함되면 매번 직접 뒤져봐야 함
- 이걸 해결하는 방법 → 블룸 필터
  - SSTable마다 블룸 필터를 둬서 해당 SSTable에 있는 모든 키 정보를 저장함
  - 블룸 필터에서 “키가 없다” → 그럼 패스
  - 블룸 핕터에서 “키가 있다” → 그럼 직접 읽기
- 어라 똑같은 거 아닌가요? 있다고 하면 직접 읽는 건 똑같으니까
  - 하지만 비율이 다름. 없다고 하면 패스하는 건 같음. 하지만 없다고 하는 빈도 수가 압도적으로 다름. 범위 방식은 그냥 범위 사이에 있으면 없다고 하지만, 블룸 필터는 SSTable 안에 있는 키에 대해서 없을 때만 없다고 함.
  - 즉 “있다고 했는데 실제로는 없는” 거짓 양성이 있을 수 있냐? 둘 다 있음. 하지만 거짓 양성 비율이 블룸 필터에서는 낮은 것.
  - 실제로는 ‘매우 낮음’ 왜? 해시로 판단하기 때문임. 해시 충돌이 발생하지 않는다면 거의 정확하게 판단함. 있다고 하면 정말로 매우매우 높은 확률로 정말 있는 거임.

NOTE) 확률적 데이터 구조는 일반적으로 "일반적인" 데이터 구조보다 공간 효율성이 더 높습니다. 예를 들어, 집합의 멤버십, 카디널리티(집합 내 고유한 원소의 수를 찾는 것), 또는 빈도(특정 원소가 몇 번 나타났는지 찾는 것)를 확인하기 위해서는 모든 집합 원소를 저장하고 전체 데이터셋을 거쳐야 결과를 찾을 수 있습니다. 확률적 구조는 근사 정보를 저장하고 불확실성의 요소를 가진 결과를 산출하는 쿼리를 수행할 수 있게 합니다. 이러한 데이터 구조의 일반적으로 알려진 예로는 블룸 필터(집합 멤버십용), HyperLogLog(카디널리티 추정용) [FLAJOLET12], 그리고 Count-Min Sketch(빈도 추정용) [CORMODE12]가 있습니다.

- 블룸 필터 → ‘확률적 데이터 구조’ 이고 공간 효율성 더 좋음
- 일반적인 데이터 구조 → 결정론적으로 작동. 즉 항상 100% 정확한 답을 제공하며 확률에 따라 성공하거나 실패하지 않음
- 하지만 확률적 구조는 확률적으로 같은 입력에 대해서 다른 답을 내놓는 오류 가능성 존재. 즉 비결정론적으로 작동. 대신 더 적은 메모리 공간을 차지
- 반변 집합 멤버십, 유니크한 개수 세기, 등장 빈도 → 이런 건 직접 전체 데이터 셋을 거쳐야 함.
- 이러 확률적 구조로는 이런 게 있다..

970년 버튼 하워드 블룸(Burton Howard Bloom)에 의해 고안된 블룸 필터[BLOOM70]는 어떤 원소가 집합의 구성원인지 아닌지를 테스트하는 데 사용될 수 있는 공간 효율적인 확률적 자료구조입니다. 이것은 거짓 양성(false-positive) 일치를 생성할 수는 있지만(원소가 실제로는 없는데 집합의 구성원이라고 말하는 경우), 거짓 음성(false-negative)을 생성할 수는 없습니다(음성 일치가 반환되면, 그 원소는 집합의 구성원이 아님이 보장됩니다).

- 확률적 자료구조
- 거짓 양성 가능 → 있다고 했는데 없는 경우 가능
- 거짓 음성 불가능 → 없다고 했는데 있는 경우 없음. 즉 없다고 했으면 100% 없음

다시 말해, 블룸 필터는 키가 테이블에 있을 수도 있는지, 아니면 확실히 없는지를 말하는 데 사용될 수 있습니다. 블룸 필터가 음성 일치를 반환하는 파일들은 쿼리 중에 건너뜁니다. 나머지 파일들은 데이터 레코드가 실제로 존재하는지 확인하기 위해 접근합니다. 디스크 상주 테이블과 연관된 블룸 필터를 사용하면 읽기 중에 접근하는 테이블의 수를 상당히 줄이는 데 도움이 됩니다.

- 왜 이게 좋냐? 없다고 했는데 100% 없음이 보장되면 건너뛰어서 여러 SSTable 읽는 작업을 생략할 수 있기 때문임
- 단 있다고 했으면 무조건 읽어서 실제로 있는지 체크 필요

블룸 필터는 큰 비트 배열과 여러 개의 해시 함수를 사용합니다. 해시 함수들은 테이블에 있는 레코드의 키에 적용되어 비트 배열 내의 인덱스를 찾고, 해당 위치의 비트들은 1로 설정됩니다. 해시 함수에 의해 결정된 모든 위치의 비트가 1로 설정된 것은 해당 키가 집합에 존재함을 나타냅니다. 조회 중에 블룸 필터에서 원소의 존재를 확인할 때, 키에 대해 해시 함수들이 다시 계산되고, 만약 모든 해시 함수에 의해 결정된 비트들이 1이라면, 우리는 해당 항목이 특정 확률로 집합의 구성원이라고 명시하는 긍정적인 결과를 반환합니다. 만약 비트 중 하나라도 0이라면, 우리는 그 원소가 집합에 존재하지 않는다고 정확하게 말할 수 있습니다.

- 큰 비트 배열과 여러 개의 해시 함수 사용
  - 비트 배열 → 상태 저장
  - 해시 함수 → 하나의 키에 대해 여러 개의 해시 값 생성
- 블룸 필터에 원소를 추가하는 과정을 보자
  - 여러 키를 추가할 때, 여러 해시 함수 각각의 키를 입력함
  - 그러면 해시 함수가 값을 반환할 것. 이 값을 비트 배열의 ‘인덱스’로 사용
  - 그러면 인덱스에 있는 비트 배열의 값을 0 → 1로 바꿈. 1이면 그대로 둠
  - 1은 그 키가 집합에 있다는 걸 의미함
- 이제 조회하는 과정을 보자
  - 추가할 때와 마찬가지로 동일한 해시 함수들에 대하여 값들을 계산함
  - 그러면 각각의 값, 즉 인덱스들이 나올 것임
  - 이때 모든 해시 값에 의한 인덱스에 있는 비트 값이 1이면 “키가 존재한다”고 판단함
- 하나라도 0인 경우 해당 키가 추가된 적이 없다고 100% 보증 가능
  - 왜? 추가되었다면 해당 비트는 반드시 1이었을 것이기 때문. 앞에서 봤듯이.
  - 그래서 거짓 음성. 즉 없는데 실제로는 있는 경우가 없고 없다고 했으면 100% 없다고 보증 가능

서로 다른 키에 적용된 해시 함수들이 동일한 비트 위치를 반환하여 해시 충돌을 일으킬 수 있으며, 1로 설정된 비트들은 단지 어떤 해시 함수가 어떤 키에 대해 이 비트 위치를 산출했다는 것만을 의미합니다.

- 거짓 양성은 해시 충돌 때문에 발생함. 가령 다른 키가 해시 함수에 의해 같은 값, 즉 같은 인덱스를 가리킬 수 있음

거짓 양성(false-positive)의 확률은 비트 집합의 크기와 해시 함수의 개수를 구성하여 관리됩니다. 더 큰 비트 집합에서는 충돌의 가능성이 더 작아지며, 유사하게 더 많은 해시 함수를 가지면 더 많은 비트를 확인할 수 있고 더 정확한 결과를 얻을 수 있습니다.

- 거짓 양성의 가능성을 낮추기 위해서 비트 집합의 크기와 해시 함수를 조정할 수 있음
- 비트 집합이 크면, 서로 다른 키의 해시 값이 같은 칸을 가리킬(충돌) 확률이 줄어듦
- 해시 함수의 개수가 많으면 하나의 키를 여러 개의 비트로 표현할 수 있음. 즉 하나의 키가 다양한 패턴을 만들어 낼 수 있음. 그러면 우연히 “모든 비트”가 겹칠 확률이 줄어들 것임

더 큰 비트 집합은 더 많은 메모리를 차지하고, 더 많은 해시 함수의 결과를 계산하는 것은 성능에 부정적인 영향을 미칠 수 있으므로, 우리는 수용 가능한 확률과 발생하는 오버헤드 사이에서 합리적인 중간 지점을 찾아야 합니다. 확률은 예상되는 집합 크기로부터 계산될 수 있습니다. LSM 트리의 테이블들은 불변(immutable)이므로, 집합 크기(테이블에 있는 키의 수)는 미리 알려져 있습니다.

- 하지만 거짓 양성을 낮추기 위해서 비트 배열을 늘리면 메모리를 많이 먹음
- 또 해시 함수를 많이 만들면 추가 조회 시마다 매번 연산해야 할 게 많아지므로 성능에 영향을 줌
- 따라서 최적지점을 골라야 함
- 어떻게? LSM 트리는 SSTable에 대하여 저장되는 키의 개수는 결정되어 있음. 불변이고 꽉 채워서 쓰기 때문

그림 7-7에 보이는 간단한 예시를 살펴보겠습니다. 우리는 16칸짜리 비트 배열과 3개의 해시 함수를 가지고 있으며, 이 함수들은 `key1`에 대해 3, 5, 10이라는 값을 산출합니다. 이제 우리는 이 위치들의 비트를 설정합니다. 다음 키가 추가되고 해시 함수들은 `key2`에 대해 5, 8, 14라는 값을 산출하며, 이 비트들 또한 설정합니다.

이제, 우리는 `key3`가 집합에 존재하는지 확인하려 하고, 해시 함수들은 3, 10, 14를 산출합니다. `key1`과 `key2`를 추가할 때 이 세 비트가 모두 설정되었기 때문에, 우리는 블룸 필터가 거짓 양성(false positive)을 반환하는 상황에 놓입니다. 즉, `key3`는 그곳에 추가된 적이 없지만, 계산된 모든 비트가 설정되어 있는 것입니다. 하지만 블룸 필터는 단지 원소가 테이블에 '있을 수도 있다'고 주장할 뿐이므로, 이 결과는 수용 가능합니다.

만약 우리가 `key4`에 대한 조회를 수행하여 5, 9, 15라는 값을 받는다면, 우리는 5번 비트만 설정되어 있고 나머지 두 비트는 설정되어 있지 않다는 것을 발견합니다. 비트 중 단 하나라도 설정되어 있지 않다면, 우리는 그 원소가 필터에 추가된 적이 없다고 확신할 수 있습니다.

- 위는 예시 설명…

### Skiplist

메모리에 정렬된 데이터를 유지하기 위한 다양한 자료 구조가 있으며, 그중 단순함 덕분에 최근 인기를 얻고 있는 것이 스킵리스트(skiplist) [PUGH90b]입니다. 구현 측면에서 스킵리스트는 단일 연결 리스트(singly-linked list)보다 훨씬 복잡하지 않으며, 확률적 복잡도 보장은 탐색 트리(search trees)의 그것과 가깝습니다.

- 메모리 상에서 정렬된 상태를 유지하는 니즈가 있을 수 있음. memtable도 그렇고 앞에서도 한 번 등장했음
- 보통 AVL 트리, 레드 블랙 트리 같은 걸 썼지만 밸런싱 로직이 복잡함.
- 확률로 이를 해결하기 때문에 구현이 간단. 그래서 여러 DBMS에서 memtable 구현체로 선택됨

스킵리스트는 삽입 및 업데이트를 위해 회전(rotation)이나 재배치(relocation)가 필요 없으며, 대신 확률적 균형 조정(probabilistic balancing)을 사용합니다. 스킵리스트 노드는 작고 메모리에 무작위로 할당되기 때문에, 일반적으로 인메모리 B-Tree보다 캐시 친화적이지 않습니다. 일부 구현에서는 언롤드 연결 리스트(unrolled linked lists)를 사용하여 이 상황을 개선합니다.

- 밸런스드 바이너리 트리의 가장 복잡한 부분은 삽입 삭제 시 밸런싱을 위해서 회전이나 재배치하는 로직임. 자료구조 했던 사람이라면 공감할 것
- 스킵리스트는 이런 밸런싱 로직 없이 높이를 랜덤하게 정하고 포인터만 조정하면 됨. 이게 확률적 밸런싱.
- 또 캐시 친화성도 중요함. 관련있는 데이터를 메모리에 연속적으로 모아두는 것… 즉 공간적 지역성이 확보되면 좋음
- 비트리는 페이지 단위로 키를 저장한 후 페이지 단위로 읽으므로 노드 하나만 읽으면 여러 개의 키가 한번에 캐시에 올라가는 식으로 공간적 지역성을 보장
- 스킵리스트의 각 노드는 메모리 상에 랜덤하게 할당될 가능성이 높고, 탐색 과정에서 캐시에 없는 메모리 주소로 점프하므로 캐시 미스가 빈번하게 발생. 즉 캐시 친화적이지 않음
- 이걸 해결하기 위해서 언롤드 링크드 리스트, 즉 연결 리스트의 각 노드를 단일 요소가 아니라 여러 요소를 저장할 수 있게 함. 그러면 여러 키가 연속된 공간에 저장되므로 굿

스킵 리스트는 다양한 높이의 노드 시리즈로 구성되며, 아이템들의 범위를 건너뛸 수 있도록 하는 연결된 계층 구조를 만듭니다. 각 노드는 키를 가지며, 연결 리스트의 노드와 달리 일부 노드는 하나 이상의 후속 노드를 가집니다. 높이 h의 노드는 최대 h 높이를 가진 하나 이상의 이전 노드로부터 연결됩니다. 가장 낮은 레벨의 노드들은 어떤 높이의 노드로부터도 연결될 수 있습니다.

- 정렬된 링크드 리스트인데, 추가적인 계층(레벨)이 존재함
- 이 계층 구조가 핵심. 상위 레벨 포인터를 통해서 수많은 노드를 ‘스킵’ 하여 탐색 속도를 향상시킴
- 모든 노드는 키를 가짐.
- 보통 일반적인 링크드 리스트는 후속 포인터를 (다음 노드를 가리킴) 가지지만, 스킵 리스크의 노드는 높이에 따라서 여러 개의 후속 포인터를 가질 수 있음. 즉 다중 포인터임
- 가령 높이 h의 노드는 높이가 h 이상인 ‘이전 노드’들의 ‘후속 포인터’에 의해 연결될 수 있음 → 즉 높아지는 방향으로만 연결 가능
- 가장 낮은 레벨, 즉 레벨 1 노드는 조금 특별한데, 높이와 무관하게 연결할 수 있음. 일종의 원형 링크드 리스트처럼 계속해서 다음 노드를 탐색할 수 있게 해주는 역할임.

노드 높이는 무작위 함수에 의해 결정되며 삽입 시 계산됩니다. 같은 높이를 가진 노드들은 하나의 레벨을 형성합니다. 레벨의 수는 무한한 성장을 피하기 위해 제한되며, 최대 높이는 구조가 얼마나 많은 아이템을 담을 수 있는지에 따라 선택됩니다. 각 다음 레벨에는 기하급수적으로 더 적은 수의 노드가 있습니다.

- 핵심은 노드 높이가 랜덤하게 결정된다는 것
- 같은 높이를 가진 노드들이 모여 하나의 레벨이 됨
- 최대 높이에는 제한이 있음. 보통 log(N) 비례 값. 즉 저장할 키 개수에 따라 달라짐
- 레벨별로 노드 분포가 다름. 노드 높이가 1 증가할 확률을 1/p (보통 p=2) 라고 설정하면 상위 레벨로 갈 수록 레벨 수는 기하급수적으로 줄어듦.

탐색은 가장 높은 레벨의 노드 포인터를 따라가는 방식으로 동작합니다. 탐색 중 찾으려는 키보다 더 큰 키를 가진 노드를 만나면, 그 이전 노드(predecessor)의 다음 레벨 노드를 가리키는 링크를 따라갑니다. 즉, 찾으려는 키가 현재 노드의 키보다 크면 탐색은 앞으로 계속 진행됩니다. 만약 찾으려는 키가 현재 노드의 키보다 작으면, 탐색은 이전 노드에서 시작하여 다음 레벨로 내려가 계속됩니다. 이 과정은 찾으려는 키나 그 이전 노드를 찾을 때까지 재귀적으로 반복됩니다.

- 가장 높은 레벨에서 시작
- 두 가지 연산이 있음
  - 오른쪽으로 이동 → 찾는 키가 더 크면 이동 성공
  - 아랫쪽으로 이동 → 찾는 키가 더 작으면 갈 수 없기 때문에 돌아옴. 그리고 아래로 가서 다시 오른쪽으로 이동 시도

삽입 시에는, 앞서 언급된 알고리즘을 사용하여 삽입 지점(키를 가진 노드 또는 그 이전 노드)을 찾고, 새로운 노드를 생성합니다. 트리와 유사한 계층 구조를 만들고 균형을 유지하기 위해, 확률 분포에 따라 생성된 난수를 사용하여 노드의 높이를 결정합니다. 새로 생성된 노드의 키보다 작은 키를 가진 이전 노드들의 포인터들은 새로 생성된 노드를 가리키도록 연결됩니다. 그들의 더 높은 레벨 포인터들은 그대로 유지됩니다. 새로 생성된 노드의 포인터들은 각 레벨의 해당하는 다음 노드에 연결됩니다.

- 삽입하려면 어디에 넣을지 알아야 함. 조회 알고리즘을 써서 위치를 결정. 그리고 노드를 생성
- 이제 노드 높이를 결정해야 함. 이게 랜덤임 → 1/2 확률로 높이를 하나씩 높이는 방식으로 결정
- 이제 기존 리스트의 특정 높이에 그 노드를 넣고 연결해야 함
  - 해당 노드 높이보다 작은 각 레벨에 대하여…
  - 새 노드를 가리켜야 하는 이전 노드 포인터를 갱신해줌
    - 즉 이전 노드 → 새 노드가 되도록 포인터 갱신
  - 새 노드 → 다음 노드가 되도록 갱신
  - 일반적인 링크드 리스트와 비슷…

삭제 시에는, 제거된 노드의 순방향 포인터들이 각 레벨에 해당하는 이전 노드들에 배치됩니다.

- 삭제는 삽입의 역순…
- 제거할 노드를 조회
- 이 노드를 가리키고 있던 이전 노드들을 포인터를 확인
- 이 이전 노드들이 제거할 노드의 다음 노드를 가리키도록 변경

추가적인 `fully_linked` 플래그를 사용하는 선형화 기법을 구현하여 스킵리스트의 동시성 버전을 만들 수 있으며, 이 플래그는 노드 포인터가 완전히 업데이트되었는지 여부를 결정합니다. 이 플래그는 compare-and-swap을 사용하여 설정할 수 있습니다 [HERLIHY10]. 이는 스킵리스트 구조를 완전히 복원하기 위해 여러 레벨에서 노드 포인터를 업데이트해야 하기 때문에 필요합니다.

- 스킵리스트의 동시성 제어 역시 빼놓을 수 없다.
- 새 노드 삽입 시 여러 노드에 결친 포인터를 수정해야 함. 이 작업이 완전히 끝나기 전까지는 동시성 문제 발생 가능
- fully_linked 플래그를 통해서 모든 포인터 연결이 완료되었는지 나타낼 수 있음.
- 그리고 여러 스레드가 동시에 플래그를 수정할 수도 있음. 이를 위해서 CAS를 통해 하나의 스레드만 원자적으로 플래그 값을 변경할 수 있게 함
- fully_linked가 필요한 이유는 삽입 삭제 시 여러 레벨 포인터 변경이 발생하기 때문… 앞에서 이 기법에 대해서 배웠으므로 익숙할 것

관리되지 않는 메모리 모델을 가진 언어에서는, 참조 카운팅이나 해저드 포인터를 사용하여 현재 참조되는 노드가 동시적으로 접근되는 동안 해제되지 않도록 보장할 수 있습니다 [RUSSEL12]. 이 알고리즘은 노드가 항상 더 높은 레벨에서 접근되기 때문에 교착 상태가 없습니다.

- C/C++ 같은 unmanaged memory model 언어에서는 직접 메모리를 관리함.
- 한 스레드가 노드를 읽는 중에 다른 스레드가 그 노드를 삭제하고 메모리를 할당 해제하면 유효하지 않은 메모리에 접근하게 됨
- 참조 카운팅 → 메모리 해제 시 해당 노드에 접근하는 스레드 개수를 세고 있다가, 0이 될 때만 해제
- 해저드 포인터 → 각 스레드가 자신이 어느 노드에 접근하고 있는지 해저드 포인터 리스트에 등록함. 그래서 메모리를 해제하려고 할 때 이 리스트를 뒤져봐서 없는 경우에만 해제
- 읽기 및 수정이 항상 위에서 → 아래로 가는 방향성을 가짐. 전에 비트리에서도 비슷한 상황 있었음. 왼 → 오와 오 → 왼이 충돌해서 데드락이 발생했었던…
- 정해진 순서로 접근하면 경합 없음

Apache Cassandra는 2차 인덱스 멤테이블 구현을 위해 스킵리스트를 사용합니다. WiredTiger는 일부 인메모리 작업을 위해 스킵리스트를 사용합니다.

- 카산드라는 보조 인덱스 memtable 구현을 위해 스킵리스트 사용. WiredTiger도 인메모리 구조 관리를 위해 사용한다고 전에 배웠었음

### Disk Access

대부분의 테이블 내용은 디스크에 상주하고, 저장 장치는 일반적으로 데이터에 블록 단위로 접근하는 것을 허용하기 때문에, 많은 LSM 트리 구현체는 디스크 접근과 중간 캐싱을 위해 페이지 캐시를 사용합니다. “버퍼 관리”에서 설명한 페이지 축출 및 고정과 같은 많은 기술들은 로그 구조화 스토리지에도 여전히 적용됩니다.

- 대부분의 데이터는 디스크에 저장됨. 그리고 디스크는 블록 단위로 읽고 씀.
- 그리고 디스크 IO는 메모리 접근보다 느리기 때문에 자주 IO가 발생하지 않도록 페이지 캐시를 사용한다고 했었음
- 비트리에서만 그러는 게 아니라 LSM 트리도 마찬가지임
- 또한 페이지 캐시에 대하여 페이지 축출 알고리즘 (LRU) 같은 것들도 동일하게 적용됨

가장 주목할 만한 차이점은 인메모리 콘텐츠가 불변(immutable)이므로 동시성 접근을 위해 추가적인 잠금(lock)이나 래치(latch)가 필요하지 않다는 것입니다. 참조 카운팅은 현재 접근 중인 페이지가 메모리에서 축출되지 않도록 하고, 컴팩션 중에 기본 파일이 제거되기 전에 진행 중인 요청이 완료되도록 보장하기 위해 적용됩니다.

- 그럼 차이는 뭐냐? 이것도 불변성 때문이 아닐까? 라고 하면 대부분 정답임
- 비트리는 당연히 제자리 갱신이었어서 동시 접근 시 래치를 걸었었음
- LSM 트리는 참조 카운팅을 씀. 이것도 동시성 문제 해결하는 방법.
  - 가령 특정 SSTable 읽을 때 해당 파일의 참조 카운트를 1 증가.
  - 컴팩션 프로세스가 머지를 위해서 이 SSTable에 접근한다고 해보자. 그럼 참조 카운트가 0보다 크면 바로 지우지 않고 기다림.
  - LMDB에서도 비슷한 방식이었음. 결국 읽는 도중에 없애진 않겠다 라는 목적은 같음. 하지만 얘는 전역적인 테이블을 썼었고 참조 카운팅은 파일 단위로 카운트를 관리한다 라는 차이가 있음.

또 다른 차이점은 LSM 트리의 데이터 레코드가 반드시 페이지에 정렬될 필요는 없으며, 주소 지정을 위해 페이지 ID 대신 절대 오프셋을 사용하여 포인터를 구현할 수 있다는 것입니다. 그림 7-9에서는 디스크 블록과 정렬되지 않은 내용을 가진 레코드를 볼 수 있습니다. 일부 레코드는 페이지 경계를 넘어서므로 메모리에 여러 페이지를 로드해야 합니다.

- 비트리에서는 페이지 단위로 데이터를 관리하고 페이지 ID와 오프셋을 사용하여 우리가 원하는 레코드에 접근했음. 이걸 page aligned 라고 했음
- 근데 SSTable은 그냥 모든 레코드를 정렬된 순서대로 이어붙인 것. 그래서 페이지 ID와 오프셋을 조합해서 접근할 필요 없이 그냥 절대적인 단일 오프셋 값으로 접근할 수 있음.
- 하지만 레코드는 페이지 단위 정렬이 아니기 때문에 디스크 블록 경계에 걸쳐서 저장될 수도 있음. 그러면 하나의 레코드를 읽기 위해서 페이지 2개를 로드해야 함. 이건 단점이긴 함…

### Compression

우리는 이미 B-Tree의 맥락에서 압축에 대해 논의했습니다("압축" 참조). 유사한 아이디어는 LSM Tree에도 적용할 수 있습니다. 여기서 주된 차이점은 LSM Tree 테이블은 불변(immutable)이며, 일반적으로 단일 패스(single pass)로 작성된다는 것입니다. 데이터를 페이지 단위로 압축할 때, 압축된 페이지는 크기가 압축되지 않은 페이지보다 작기 때문에 페이지 정렬(page aligned)되지 않습니다.

- 비트리에서의 압축(compression, compaction과 다름)에 대해서 배웠었음
  - page-wise 압축. 그리고 압축되면서 페이지 정렬이 깨지므로, 이를 0-padding으로 채움. 하지만 비효율적
  - 파일 전체 압축 → 전체를 수정해야 하므로 비효율적
  - 행/열 단위 압축 → 굿…
- 차이는 불변이고 데이터가 싱글 패스, 즉 순차적으로 쓰인다는 것.
- 이런 불변성 + 싱글 패스 특성 고려하여 압축 방식도 다르게 해야 함
- 위에서 LSM 트리도 디스크에 데이터를 저장하고 디스크는 블록 단위로 데이터에 접근하므로 B-Tree에서 봤던 페이지 캐시를 사용한 방식도 유효하다는 이야기를 했었음
- 따라서 디스크 상에서 압축된 페이지 주소와 메모리에서 압축 해제된 페이지 캐시의 주소가 일치하지 않음

압축된 페이지를 주소 지정(address)할 수 있으려면, 내용을 쓸 때 주소 경계를 추적해야 합니다. 압축된 페이지를 0으로 채워서 페이지 크기에 맞게 정렬할 수도 있지만, 그렇게 하면 압축의 이점을 잃게 됩니다.

- 따라서 둘을 매핑해주는 테이블이 있어야 함. 디스크에 압축해서 저장할 때 0 패딩을 넣어줄 수 있겠지만 그럼 압축 이점을 못 살림. B-Tree에서도 0 패딩 방식은 별로라고 했었음

압축된 페이지를 주소 지정할 수 있도록 하려면, 압축된 페이지의 오프셋과 크기를 저장하는 간접 계층(indirection layer)이 필요합니다. 그림 7-10은 압축된 블록과 압축되지 않은 블록 간의 매핑을 보여줍니다. 압축된 페이지는 항상 원본보다 작으며, 그렇지 않다면 압축할 이유가 없습니다.

- 간접 계층 → 즉 매핑 테이블이 필요함. 즉 페이지 캐시 번호에 해당하는, 디스크에서의 압축된 페이지 시작 위치를 알아야 함.

컴팩션(compaction)과 플러시(flush) 중에는 압축된 페이지들이 순차적으로 추가되며, 압축 정보(원본 압축 해제된 페이지 오프셋과 실제 압축된 페이지 오프셋)는 별도의 파일 세그먼트에 저장됩니다. 읽기 중에는 압축된 페이지 오프셋과 그 크기를 조회한 후, 해당 페이지를 압축 해제하여 메모리에 구체화(materialized)할 수 있습니다.

- 컴팩션과 플러시를 통해 압축된 페이지들이 순서대로 추가됨. 즉 우리는 memtable을 플러시한다고 러프하게 이해했었지만 이것도 사실 디스크가 블록 단위이므로 페이지들을 순차적으로 추가하는 작업임.
- 그리고 압축 정보는 별도 파일 세그먼트에 저장
  - 원본 압축 해제된 페이지 오프셋 → 페이지 캐시 상에서 페이지 번호.
  - 실제 압축된 페이지 오프셋 → 디스크 상에서 물리적인 오프셋.
- 읽기 시 SSTable에서 페이지 단위로 읽을 것임. 이때 이 압축된 페이지가 어디에 있는지 알아야 함. LSM 트리도 페이지 캐시를 관리한다고 했으니.. 이 페이지 캐시에 대응하는 디스크 상에서의 위치를 알기 위해 이 매핑 테이블로 접근. 압축된 페이지 오프셋과 크기 (즉 디스크에서의 위치와 크기)를 알아내서 그만큼 디스크에서서 읽고 메모리 상의 페이지 캐시로 가져옴.

## Unordered LSM Storage

지금까지 논의된 대부분의 저장 구조는 데이터를 순서대로 저장합니다. 변경 가능한 B-Tree 페이지와 변경 불가능한 B-Tree 페이지, FD-Tree의 정렬된 런(sorted run), 그리고 LSM Tree의 SSTable은 데이터 레코드를 키 순서로 저장합니다. 이 구조들에서 순서는 다른 방식으로 유지됩니다: B-Tree 페이지는 제자리에서 업데이트되고, FD-Tree 런은 두 런의 내용을 병합하여 생성되며, SSTable은 메모리에서 데이터 레코드를 버퍼링하고 정렬하여 생성됩니다.

이 섹션에서는 레코드를 무작위 순서로 저장하는 구조에 대해 논의합니다. 정렬되지 않은 저장소는 일반적으로 별도의 로그가 필요하지 않으며 데이터 레코드를 삽입 순서로 저장하여 쓰기 비용을 줄일 수 있습니다.

- 세 가지 중 하나인 정렬성을 언급할 차례
- 우리는 지금까지 정렬된 구조에 대해서 다뤘음.
  - B-Tree → 페이지 내부는 정렬된 상태
  - FD-Tree → 정렬된 불변 런
  - SSTable → 정렬된 상태
- 하지만 정렬성을 유지하는 방식은 다름
  - B-Tree → 정렬된 상태가 되도록 삽입 후 제자리 갱신
  - FD-Tree → 기존에 정렬된 상태인 런들을 머지하여 정렬된 런들을 만드는 식.
  - SSTable → memtable에 정렬된 상태로 유지했다가 플러시
- 이제 정렬된 구조 대신, 무작위 순서로 다루는 방법을 알아볼 것 → 일반적으로 삽입 순서를 말함
- 비정렬 방식의 장점
  - 로그 불필요 → WAL이 삽입 순서대로 기록하는 역할이었음. 근데 비정렬은 애초부터 삽입 순서대로 기록하므로, 즉 데이터 파일이 로그 파일처럼 동작하기에 필요 없음
  - 쓰기 비용 → 이젠 진짜 파일 끝에다가 들어오는 대로 붙이기만 하면 됨.

### Bitcask

Riak에서 사용되는 스토리지 엔진 중 하나인 Bitcask는 순서가 없는 로그 구조 스토리지 엔진입니다[SHEEHY10b]. 지금까지 논의된 로그 구조 스토리지 구현과 달리, 버퍼링을 위해 memtable을 사용하지 않고 데이터 레코드를 로그 파일에 직접 저장합니다.

- memtable은 버퍼링 목적도 있지만 정렬된 상태로 유지하는 역할도 했었음. 이제 정렬을 유지할 필요가 없으므로 memtable을 아예 없애도 됨.
- 대신 쓰기 요청이 오면 로그 파일에 바로 기록

값을 검색할 수 있도록 만들기 위해, Bitcask는 keydir이라는 데이터 구조를 사용하며, 이는 해당 키에 대한 최신 데이터 레코드의 참조를 유지합니다. 오래된 데이터 레코드는 디스크에 계속 존재할 수 있지만, keydir에서 참조되지 않으며 압축(compaction) 중에 가비지 컬렉션됩니다. Keydir는 인메모리 해시맵으로 구현되며 시작 시 로그 파일로부터 재구성되어야 합니다.

- 키가 정렬되지 않지 때문에 keydir이라는 자료구조를 사용
- 각 키에 대해 최신 버전의 데이터가 디스크 상에 어디에 존재하는지 알려줌
- 데이터가 변경되면 새로운 값이 로그 파일에 바로 추가되고 keydir 포인터만 갱신됨.
- 이전 버전 데이터는 디스크 상에 남아있다가 컴팩션 과정에서 없어짐
- keydir는 인메모리 해시맵으로 구현. 하지만 모든 키를 메모리에 저장해야 하고 인메모리 구조이기 때문에 데이터베이스 재시작 시 모든 로그 파일을 스캔해서 다시 만들어야 함

쓰기 작업 동안, 키와 데이터 레코드는 순차적으로 로그 파일에 추가되고, 새로 작성된 데이터 레코드 위치에 대한 포인터가 keydir에 배치됩니다.

- 쓰기 과정
  - 쓰기 요청이 오면 키와 데이터를 로그 파일에 추가
  - 해당 키에 대한 데이터 위치를 가리키는 keydir를 갱신

읽기 작업은 keydir를 확인하여 검색된 키를 찾고 연관된 포인터를 따라 로그 파일로 이동하여 데이터 레코드를 찾습니다. 특정 순간에 keydir의 키와 연관된 값은 하나만 있을 수 있으므로, 포인트 쿼리(point queries)는 여러 소스의 데이터를 병합할 필요가 없습니다.

- 읽기 과정
  - keydir에서 키를 찾고 키가 가리키는 디스크 상의 데이터 위치로 이동
- keydir에는 특정 키에 대해 하나의 값만이 존재하므로 포인트 쿼리가 매우 빠르고 단순

그림 7-11은 Bitcask의 데이터 파일에 있는 키와 레코드 간의 매핑을 보여줍니다. 로그 파일은 데이터 레코드를 저장하고, keydir은 각 키와 연결된 최신 라이브 데이터 레코드를 가리킵니다. 데이터 파일에서 가려진(shadowed) 레코드(이후의 쓰기나 삭제에 의해 대체된 레코드)는 회색으로 표시됩니다.

- 그렇구나…

압축(compaction) 중에는 모든 로그파일의 내용을 순차적으로 읽고, 병합하고, 새로운 위치에 기록하며, 유효한(live) 데이터 레코드만 보존하고 그림자 레코드(shadowed ones)는 폐기합니다. Keydir는 재배치된 데이터 레코드를 가리키는 새로운 포인터로 업데이트됩니다.

- 똑같이 컴팩션하면 됨. 컴팩션 이후에는 새로운 파일이 생기므로 이것만 keydir에 대하여 갱신해주면 됨

데이터 레코드는 로그파일에 직접 저장되므로, 별도의 미리-쓰기 로그(write-ahead log)를 유지할 필요가 없어 공간 오버헤드와 쓰기 증폭을 모두 줄여줍니다. 이 접근법의 단점은 keydir과 데이터 파일 모두에서 항목들이 정렬되어 있지 않기 때문에 포인트 쿼리(point query)만 제공하고 범위 스캔(range scan)은 허용하지 않는다는 것입니다.

- 데이터를 로그 파일에 직접 저장함.
- 많은 DBMS들이 durability를 위해서 WAL을 사용하고 이로 인한 쓰기 증폭 그리고 공간 증폭을 감수함.
- 하지만 둘 다 감수할 필요가 없음. 로그 파일에 데이터를 저장하므로…
- 단점은 포인트 쿼리만 가능하고 범위 스캔은 불가능. 비정렬 구조이기 때문.

이 접근법의 장점은 단순성과 뛰어난 포인트 쿼리 성능입니다. 데이터 레코드의 여러 버전이 존재하더라도, keydir에서는 최신 버전만 참조합니다. 하지만 모든 키를 메모리에 유지해야 하고 시작 시 keydir을 재구성해야 한다는 점은 일부 사용 사례에서는 결정적인 단점이 될 수 있는 한계입니다.

- 대신 포인트 쿼리가 아주 좋음.
- 인메모리 해시맵이기 때문에 재시작 시 모든 로그 파일을 읽어서 keydir를 재구성해야 한다는 단점. 대용량 데이터가 저장되어 있거나… 메모리 제약사항이 있거나… startup이 빨라야 한다면 못 쓸 듯.

### WiscKey

범위 질의는 많은 애플리케이션에 중요하며, 정렬되지 않은 저장소의 쓰기 및 공간 이점을 가지면서도 범위 스캔을 수행할 수 있는 저장소 구조가 있다면 좋을 것입니다.

- 비정렬 구조 다 좋은데 레인지 스캔만 되면 참 좋을 듯…

WiscKey [LU16]는 키를 LSM 트리에 정렬된 상태로 유지하고 데이터 레코드는 vLogs(값 로그)라고 하는 정렬되지 않은 추가 전용 파일에 유지함으로써 정렬과 가비지 컬렉션을 분리합니다. 이 접근 방식은 Bitcask에 대해 논의할 때 언급된 두 가지 문제, 즉 모든 키를 메모리에 유지해야 하고 시작 시 해시테이블을 재구성해야 하는 문제를 해결할 수 있습니다.

- 키-값 분리가 핵심
- 키
  - 이건 정렬해야 하므로 레인지 스캔 가능한 LSM 트리에 저장
- 값
  - 이건 정렬 필요 없으므로 vLog라는 비정렬 append-only 파일에 저장
- 정렬과 GC를 분리
  - 정렬 → LSM 트리에서 가지는 정렬성을 보장하여, 키에 대한 레인지 쿼리를 지원하는 것
  - GC → Bitcask에서 쉐도우된 데이터 레코드를 회수하는 것

그림 7-12는 WiscKey의 주요 구성 요소와 키와 로그 파일 간의 매핑을 보여줍니다. vLog 파일은 정렬되지 않은 데이터 레코드를 보유합니다. 키는 정렬된 LSM 트리에 저장되어 로그파일에 있는 최신 데이터 레코드를 가리킵니다.

키는 일반적으로 연관된 데이터 레코드보다 훨씬 작기 때문에, 이를 압축(compaction)하는 것이 훨씬 더 효율적입니다. 이 접근 방식은 업데이트와 삭제 비율이 낮아 가비지 컬렉션이 많은 디스크 공간을 확보하지 못하는 사용 사례에 특히 유용할 수 있습니다.

- 키는 데이터에 비해 작기 때문에 컴팩션이 부담스럽지 않음. 컴팩션의 가장 큰 문제는 쓰기 증폭, 이미 쓴 걸 합쳐서 또 써야 한다는 거였는데, 키는 작기 때문에 또 써도 그렇게 부담스럽지 않음.
- WiscKey가 유리한 워크로드
  - 값이 변경되거나 삭제되지 않는 경우가 좋음.
    - 왜? 값(vLog)에서 수행되는 GC, 즉 값 컴팩션은 키에서 수행되는 컴팩션에 비해 부담스러운 작업
    - 따라서 GC가 안 필요하도록 값은 변경이 덜 발생해야 함
  - 이로 인해 딱 한번만 쓰고 읽기는 잦으면서도 write-intensive할 때 장점이 극대화된다 할 수 있음

압축(compaction) 중에는 vLog 파일의 내용을 순차적으로 읽어 병합하고 새로운 위치에 기록합니다. (key LSM Tree의 값인) 포인터는 이 새로운 위치를 가리키도록 업데이트됩니다. 전체 vLog 내용을 스캔하는 것을 피하기 위해, WiscKey는 살아있는(live) 키를 담고 있는 vLog 세그먼트에 대한 정보를 담고 있는 head와 tail 포인터를 사용합니다.

- 키 컴팩션도 하는 것 같은데 책에서는 컴팩션 = 값 컴팩션을 말하는 듯.
  - vLog을 순차적으로 읽는다
  - 그리고 죽은 데이터를 버리고 라이브 데이터만 살리도록 머지한다.
  - 머지한 결과를 새로운 vLog 파일에 쓴다
- 값 위치가 바뀌었으므로 LSM 트리의 값(vLog로의 포인터)를 갱신해줘야 함
- 컴팩션 중에 전체 vLog을 다 읽는 건 비효율적
  - 외부에서 head / tail 포인터를 사용하여 그 부분만 GC를 수행함. head가 현재까지 GC를 완료한 위치. tail이 지금 쓰기가 수행된 위치.

vLog의 데이터는 정렬되어 있지 않고 활성(liveness) 정보가 없기 때문에, 어떤 값이 여전히 살아있는지 찾으려면 키 트리를 스캔해야 합니다. 가비지 컬렉션 중에 이러한 검사를 수행하는 것은 추가적인 복잡성을 야기합니다: 전통적인 LSM 트리는 키 인덱스를 참조하지 않고도 압축 중에 파일 내용을 확인할 수 있습니다.

- vLog는 그냥 삽입 순서대로 저장된 데이터 덩어리라서 이 데이터가 live인지 dead인지 모름.
- 이걸 알기 위해서는 키가 저장된 LSM 트리에서 해당 키가 가리키는 위치가 여기인지 봐야 함.
- GC 도중에 이게 live인지 dead인지 판별하기 위해서 매번 이렇게 검사해야 한다는 건데 좀 복잡함.
- 근데 LSM 트리에서는 SSTable 안에 키-값이 같이 저장되므로 그냥 컴팩션 과정에서 키와 타임스탬프 기반으로 최신성 여부를 알 수 있으므로 키를 따로 봐야 하는 일이 없었음
- 즉 WiskKey는 Bitcask처럼 쓰기 효율성과 공간 효율성을 확보하면서도 범위 스캔에 대한 읽기 성능을 확보하고 싶었음. 그래서 키와 값을 분리했음. 하지만 키와 값이 분리해지면서 컴팩션, 즉 GC가 더 복잡해지는 트레이드오프가 생긴 것.
