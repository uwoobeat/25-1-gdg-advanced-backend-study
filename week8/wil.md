# 고급 백엔드 스터디 8주차

## Transaction Processing and Recovery

이 책에서는 데이터베이스 시스템 개념에 대해 상향식(bottom-up) 접근 방식을 취해왔습니다. 우리는 먼저 스토리지 구조에 대해 배웠습니다. 이제 우리는 데이터베이스 트랜잭션을 이해하기 위한 전제 조건인 버퍼 관리(buffer management), 잠금 관리(lock management) 및 복구(recovery)를 담당하는 더 높은 수준의 구성 요소로 넘어갈 준비가 되었습니다.

- 우리는 지금까지 DBMS에 대하여 바텀업 방식으로 학습해왔음
- 먼저 스토리지 구조에 대해 배웠음 → 그리고 버퍼 관리, 락 관리, 리커버리 관리 등을 담당하는 구성 요소을 배울 것임.
  - 챕터 1에서 DBMS의 구성 요소로 스토리지 엔진 내 트랜잭션 매니저, 락 매니저 등이 있었다고 했었던 점 리마인드
  - 우리가 지금까지 배웠던 것은 데이터가 디스크에 물리적으로 어떻게 저장되어야 하는지에 대한 기작 이해 (파일 포맷, B-Tree, 인덱스, 페이지 레이아웃…)

트랜잭션은 데이터베이스 관리 시스템에서 여러 작업을 단일 단계로 나타낼 수 있게 해주는 분할 불가능한 논리적 작업 단위입니다. 트랜잭션에 의해 실행되는 작업에는 데이터베이스 레코드 읽기 및 쓰기가 포함됩니다. 데이터베이스 트랜잭션은 원자성(atomicity), 일관성(consistency), 독립성(isolation), 그리고 지속성(durability)을 보존해야 합니다. 이러한 속성들은 일반적으로 ACID라고 불립니다 [HAERDER83]:

- 트랜잭션 → DBMS에서 여러 작업을 ‘분할 불가능한’ 하나의 단계로 표현하는 논리적 작업 단위
  - 분할 불가능한 → 원자성을 가지고 있다는 뜻. 뒤에서 설명
  - 논리적 작업 단위 → 비즈니스적으로 의미있는 작업 단위라는 뜻. 가령 ‘계좌 이체’의 경우 A의 돈을 빼고 / B에 그만큼의 돈을 더하는 작업임. 이는 이체라는 의미있는 하나의 작업 단위로 부를 수 있음.
- 트랜잭션은 읽기 및 쓰기 작업으로 이루어져 있음
- 트랜잭션은 ACID 속성을 가져야 함

**원자성(Atomicity)**: 트랜잭션 단계는 분할 불가능합니다. 이는 트랜잭션과 관련된 모든 단계가 성공적으로 실행되거나, 아니면 그 어떤 단계도 실행되지 않음을 의미합니다. 다시 말해, 트랜잭션은 부분적으로 적용되어서는 안 됩니다. 각 트랜잭션은 커밋(commit)될 수 있습니다(트랜잭션 실행 중 수행된 쓰기 작업의 모든 변경 사항을 보이게 함). 또는 중단(abort)될 수 있습니다(아직 반영되지 않은 모든 트랜잭션의 부수 효과를 롤백함). 커밋은 최종 작업입니다. 중단 후에는 트랜잭션을 재시도할 수 있습니다.

- 원자성은 내부의 모든 작업이 여러 단계더라도, 마치 하나의 작업처럼 진행되거나 아니면 전부 진행되지 않은 상태로 되돌려진다는 것을 의미
- 이걸 All or Nothing이라고 함. 가령 계좌 이체의 경우 A에서 5천원이 빠지고 B에 5천원이 더해지는 두 단계 과정으로 이루어짐. 이 두 단계 과정은 모두 성공하거나 모두 실패해서 원래 상태로 되돌어가야 함.
- 즉 일부만 성공하는 경우는 있을 수 없음. 중간에 시스템 에러 등이 발생하여 강제로 중단되는 경우라면 이는 초기 상태로 되돌려져야 함.
- 어떻게? 트랜잭션의 커밋을 알아야 함. 커밋 명령을 받아야 DBMS는 지금까지의 변경사항을 영구적으로 반영하고, 다른 트랜잭션 (혹은 사용자) 가 이 변경사항을 볼 수 있게 함.
- 만약 트랜잭션 실행 중 문제가 발생했거나 이를 의도적으로 되돌리고 싶다면, 중단(혹은 롤백)하면 됨. 만약 문제가 발생한 것이라면 트랜잭션을 재시도할 수 있음.

**일관성(Consistency)**: 일관성은 애플리케이션에 따라 달라지는 보장입니다. 트랜잭션은 모든 데이터베이스 불변 속성(제약 조건, 참조 무결성 등)을 유지하면서 데이터베이스를 하나의 유효한 상태에서 다른 유효한 상태로만 가져와야 합니다. 일관성은 가장 약하게 정의된 속성인데, 이는 아마도 데이터베이스 자체에 의해서만 제어되는 것이 아니라 사용자에 의해서도 제어되는 유일한 속성이기 때문일 것입니다.

- 일관성 → 애플리케이션에서 어떻게 정하냐에 따라 달라지는 보장성임.
- 트랜잭션은 모든 불변식을 지키면서 유효한 상태에서 다른 유효한 상태로 만들어야 함
- 불변식에는 제약조건(특정 값이 nullalble인지 not null인지, 특정 수가 양수인지 등), 참조 무결성(fk가 참조하는 pk가 항상 존재하는 값이어야 한다는 규칙)이 있음
- 트랜잭션은 이러한 불변식을 지키는 ‘논리적으로 유효한’ 상태에서 다른 ‘논리적으로 유효한’ 상태로만 변화시켜야 함. 즉 제약조건 상으로는 양수인데 트랜잭션이 갑자기 양수에서 음수로 변화시킬 수 없다는 뜻
- “일관성은 가장 약하게 정의된 속성”
  - 정의 범위가 느슨하다는 뜻. 즉 엄밀하게 일관성은 이런 거에요! 라고 따질 수 있는 기준이 없음. 왜냐면 ‘유효하다’ 라는 기준이 비즈니스마다 다르기 때문임.
    - 가령 어떤 비즈니스에서는 Money 도메인에 대해 음수 값을 허용할 수도 있고 허용하지 않을 수도 있음. 어떤 자릿수까지 유효하다고 판정할 것인지 역시도 마찬가지임.
  - 앞에서 봤던 원자성 같은 것들은 명확한 기준이 있음. All or Nothing 같이…
- “이는 아마도 데이터베이스 자체에 의해서만 제어되는 것이 아니라 사용자에 의해서도 제어되는 유일한 속성…”
  - DBMS는 스키마에 정의된 PK, FK, UNIQUE, CHECK 등으로 검사를 수행하지만, 사용자가 이를 올바르게 선언하지 않아 빼먹게 된다면 결국 비즈니스 상에서 일관성을 보장할 수 없게 됨. 즉 트랜잭션이 ACID를 어겼다고 판단할 수 없음
  - DBMS가 테이블 이름이 money라고 해서 대충 내부에 amount 필드가 있으면 positive인지 알아서 검증하나? 절대 아님. 사용자가 해당 필드에 제약조건을 추가해줘야 함.

**독립성(Isolation)**: 여러 개의 동시 실행 트랜잭션은 마치 다른 트랜잭션이 동시에 실행되지 않는 것처럼 간섭 없이 실행될 수 있어야 합니다. 독립성은 데이터베이스 상태에 대한 변경 사항이 언제 어떻게 다른 동시 트랜잭션에 보일 수 있는지를 정의합니다. 많은 데이터베이스는 성능상의 이유로 주어진 독립성 정의보다 약한 독립성 수준을 사용합니다. 동시성 제어에 사용되는 방법과 접근 방식에 따라, 트랜잭션에 의해 이루어진 변경 사항이 다른 동시 트랜잭션에 보일 수도 있고 보이지 않을 수도 있습니다 (“독립성 수준” 참조).

- 여러 트랜잭션이 동시에 실행될 때, 각 트랜잭션이 마치 데이터베이스를 혼자 쓰는 것처럼 완벽하게 분리되어 실행될 것을 보장하는 것. 이는 뒤에서 나올 병행 제어 혹은 동시성 제어의 목표라고 할 수 있음
- 제일 편한 방법은 실제로 동시에 실행하지 않는 것임. 즉 트랜잭션을 순차적으로 실행하는 것. 그러면 성능이 매우 저하될 것임… 따라서 병렬적으로 실행하면서도 그 결과는 순차적으로 실행한 것이나 다름없게, 즉 “마치 동시에 실행되지 않는 것처럼 간섭 없이” 실행되어야 함. 이를 직렬성(serialization)이라 함.
- 완벽한 직렬성을 구현하는 것은 어렵고, 시스템에 부하를 줌. 성능과 일관성 상의 trade-off가 발생함. 둘 사이에서 줄타기하기 위해서 트랜잭션 격리 수준(isolation level)이라는 것이 존재. 아마 한번쯤 들어봤을 것.
  - 여기서 SERIALIZABLE 단계가 있는데, 그 직렬성(serialization)이 맞음. 즉 직렬성을 완벽하게 보장하는 격리 수준임. 대신 성능이 안 좋겠지…

**지속성(Durability)**: 일단 트랜잭션이 커밋되면, 모든 데이터베이스 상태 변경 사항은 디스크에 영구적으로 저장되어야 하며, 정전(power outages), 시스템 오류(system failures), 그리고 충돌(crashes)에도 견딜 수 있어야 합니다.

- 일단 커밋되고 나면 그 무슨 일이 있더라도 영구적으로 저장(persist)되어야 함. 먼저 데이터는 이를 위해서 디스크와 같은 비휘발성 저장 매체에 기록됨. 또, 뒤에서 배우겠지만 WAL이라는 기법을 써서 실제 디스크에 쓰기 전에 로그 파일에 해당 내용을 먼저 기록함(write-ahead)
- 만약 충돌이 발생하여 메모리 상 변경사항이 손상되었다면? 그러면 WAL의 로그를 사용하여 리커버리 매니저가 해당 트랜잭션으로 인한 변경사항을 올바르게 살리거나 / 이전의 유효한 상태로 되돌려서 일관된 상태를 유지할 수 있게 함. 이걸 리커버리라고 함. 나중에 다룸

데이터베이스 시스템에서 트랜잭션을 구현하는 것은, 데이터를 디스크에 구성하고 영구적으로 저장하는 저장 구조 외에도, 여러 구성 요소들이 함께 작동하도록 요구합니다. 노드 내부적으로는 트랜잭션 관리자가 트랜잭션과 그 개별 단계를 조율하고, 스케줄링하며, 추적합니다.

- 트랜잭션을 구현하기 위해서는 앞에서 배운 저장 구조 (데이터를 디스크에 구성하고 저장하는…) 외에도 우리가 앞으로 배울 다른 여러 구성 요소(트랜잭션 매니저, 락 매니저, 리커버리 매니저, …)가 필요함.
- 트랜잭션 매니저는 노드 내부에서 다른 구성 요소와 협력하고, 동시에 실행되는 트랜잭션의 실행 순서 및 방식 (ex: 격리 수준)을 스케쥴링하며, 각각의 트랜잭션에 따른 상태 변화 (시작 / 커밋 / 중단) 단계를 추적함

잠금 관리자는 이러한 자원(resources)에 대한 접근을 보호하고 데이터 무결성을 위반할 수 있는 동시 접근을 방지합니다. 잠금이 요청될 때마다 잠금 관리자는 해당 잠금이 다른 트랜잭션에 의해 공유(shared) 모드 또는 배타(exclusive) 모드로 이미 유지되고 있는지 확인하며, 요청된 접근 수준이 모순을 초래하지 않는 경우 접근을 허용합니다. 배타 잠금은 주어진 순간에 최대 하나의 트랜잭션만 유지할 수 있으므로, 이를 요청하는 다른 트랜잭션은 잠금이 해제될 때까지 기다리거나, 중단하고 나중에 다시 시도해야 합니다. 잠금이 해제되거나 트랜잭션이 종료되는 즉시 잠금 관리자는 보류 중인 트랜잭션 중 하나에 이를 알리고 잠금을 획득하여 계속 진행할 수 있도록 합니다.

- 락 매니저는 DB 리소스 (데이터 레코드, 페이지) 등에 대한 접근을 제어하고, 동시에 여러 트랜잭션이 해당 리소스에 접근할 때 발생할 수 있는 무결성 위반을 막음
  - 왜? DBMS에서는 여러 트랜잭션이 동시에 실행될 때, 특정 데이터를 수정하는 동안 다른 트랜잭션이 해당 데이터를 읽거나 수정할 수 있음. 따라서 동시 접근을 락 매니저가 제어하지 않으면 데이터 무결성이 깨질 수 있음 (갱신유실-lost update, 더티 리드 등 아노말리)
  - 이를 막기 위해 락 매니저는 트랜잭션이 특정 리소스에 접근할 때 락을 요청하고 획득하도록 함.
- 락 요청 시 두 가지 모드가 가능. 공유(shared) 혹은 배타(exclusive) 모드. 공유 락(S-Lock)은 읽기에 사용. 배타 락(X-Lock)은 쓰기에 사용.
- 락 매니저는 리소스 요청이 들어오면 해당 리소스의 락 모드를 확인하고 모순이 있는지 확인. 즉 S-Lock이라면 여러 명이 동시에 읽기를 요청하더라도 허용. 하지만 쓰기는 한 명만 허용할 것임.
- 배타 락(X-Lock)은 읽기든 쓰기든 하나의 트랜잭션만 동시에 존재할 수 있으므로 다른 트랜잭션은 대기하거나 중단 후 재시도 수행(무기한 대기로 인한 데드락을 막기 위함)
- 이후 락이 해제되면 (보통 트랜잭션이 대상 리소스 작업을 완료하여 커밋 혹은 롤백될 때 해제됨)… 대기 중인 트랜잭션에게 알림을 보냄 → 즉 대기 큐 상 다음 순서의 트랜잭션에게 허용해줌

페이지 캐시는 영구 저장소(디스크)와 나머지 저장소 엔진 사이의 중개자 역할을 합니다. 이는 메인 메모리에서 상태 변경을 준비하고 영구 저장소에 동기화되지 않은 페이지들을 위한 캐시 역할을 합니다. 데이터베이스 상태에 대한 모든 변경은 먼저 캐시된 페이지에 적용됩니다.

- DBMS는 디스크 엑세스를 통해 읽어온 페이지를 메모리(RAM)에 올림. 디스크 엑세스는 비싼 작업이기 때문에 이를 최소화하는 것이 중요함. 이를 보조하는 것이 페이지 캐시.
- 메인 메모리에서 상태 변경을 준비하고 영구 저장소에 동기화되지 않은 페이지 → 이 말이 좀 헷갈리는데, DBMS 변경사항은 디스크에 바로 쓰이는 게 아니라, 페이지 캐시에 먼저 적용됨. 앞에서 버퍼링 얘기 하면서 여러 번의 쓰기 작업을 하나로 묶을 수 있다고 했음. 이러면 디스크 IO가 줄어든다.
- 핵심은 반복된 읽기에 대하여 읽기 엑세스를 줄이는 캐시 역할 뿐만 아니라 여러 번의 쓰기를 버퍼링하여 플러시하는 쓰기 버퍼의 역할까지 한다는 것

로그 관리자는 캐시된 페이지에 적용되었지만 아직 영구 저장소와 동기화되지 않은 작업 기록(로그 항목)을 보관하여 시스템 충돌 시 데이터 손실을 방지합니다. 즉, 로그는 시스템 시작 시 이러한 작업을 다시 적용(redo)하고 캐시된 상태를 재구성하는 데 사용됩니다. 또한 로그 항목은 중단된 트랜잭션에 의해 수행된 변경 사항을 되돌리는(undo) 데에도 사용될 수 있습니다.

- 로그 매니저는 “캐시된 페이지에 적용되었지만 아직 영구 저장소와 동기화되지 않은 작업 기록(로그 항목)”을 보관한다고 함 → 좀 말이 길다
- 로그 매니저 → 일단 로그를 관리. 주로 메모리에 로드된 페이지 캐시에 적용된(버퍼링된) 변경 사항들을 추적함.
- 로그 엔트리 → 모든 수정 작업에 대한 로그.
- 위에서 페이지 캐시에 적용된 변경사항들은 플러시되기 전까지 영속화(영구 저장)되지 않음. 따라서 이 변경사항이 플러시되고 커밋되기 전에 시스템 충돌이 발생할 경우 해당 변경사항들이 손실됨. 이를 막기 위해서 추가적으로 로그를 남기는 것임. 즉 실제로 DB에 영구적으로 반영되기 전까지는 항상 로그를 남겨둔다. 이렇게 이해하면 될 듯
- 충돌 이후 두 가지가 가능
  - redo → 충돌 후 로그 상의 변경사항을 다시 적용해서, 마지막 상태를 그대로 복구할 수 있음
  - undo → 트랜잭션이 중단되어서 롤백해야 하는 경우, 이미 영속화된 적용된 변경사항 역시 되돌려져야 함. 이때도 로그를 사용. 이걸 undo라고 함. 즉 트랜잭션 시작 전의 valid(consistent)한 상태로 되돌리는 것

분산(다중 파티션) 트랜잭션은 추가적인 조정과 원격 실행이 필요하며, 분산 트랜잭션 프로토콜은 13장에서 논의합니다.

- 현재는 단일 노드 (로컬 노드) 상에서의 트랜잭션 처리가 메인임. 하지만 여러 노드 상에서 발생하는 작업들은 아주아주 복잡한 고려사항들이 필요함. 가령 2PC 같이… 이건 우리 스터디 범위는 아님.

## Buffer Management

대부분의 데이터베이스는 느린 영구 저장소(디스크)와 더 빠른 메인 메모리(RAM)라는 두 가지 수준의 메모리 계층 구조를 사용하여 구축됩니다. 영구 저장소에 대한 접근 횟수를 줄이기 위해 페이지는 메모리에 캐시됩니다. 저장소 계층에서 페이지가 다시 요청될 때, 캐시된 복사본이 반환됩니다.

- DBMS는 두 가지 메모리 계층이 있음
  - 느리지만 영구적인 저장소인 디스크
  - 빠르지만 비영구적인 저장소인 메인 메모리(RAM)
  - 1장에서 이야기했었음…
- 디스크 엑세스는 비싸기 때문에 반복적인 접근을 줄이기 위해서, 페이지를 메모리에 캐시해둠. 그러면 해당 페이지를 요청할 때 매번 디스크 엑세스하는 대신 메모리에 캐시해둔 사본을 반환할 수 있음

메모리에서 사용 가능한 캐시된 페이지는 다른 프로세스가 디스크의 데이터를 수정하지 않았다는 가정하에 재사용될 수 있습니다. 이 접근 방식은 때때로 가상 디스크로 참조되기도 합니다 [BAYER72, 240]. 가상 디스크 읽기는 메모리에 페이지 사본이 없는 경우에만 물리적 저장소에 접근합니다. 같은 개념에 대한 더 일반적인 이름은 페이지 캐시 또는 버퍼 풀입니다. 페이지 캐시는 디스크에서 읽은 페이지를 메모리에 캐싱하는 역할을 합니다. 데이터베이스 시스템 충돌 또는 비정상적인 종료 시 캐시된 내용은 손실됩니다.

- 이러한 페이지 캐시를 ‘가상 디스크’라고 하기도 하는데, 이러한 방식이 메모리 내에 존재하는 페이지 캐시가 마치 가상의 디스크처럼 동작하기 때문에 그렇게 부르는 듯.
- 당연히 가상 디스크 읽기는 페이지 캐시에 사본이 없다면 실제 디스크로 엑세스하게 됨.
- 가상 디스크라는 표현도 쓰지만, 보통 페이지 캐시 혹은 버퍼 풀이라고 부름. 전자는 캐싱 및 더티 페이지 플러싱 등 캐시 처리가 메인임. 후자는 빈 버퍼 프레임을 풀처럼 재사용하여 관리한다. 즉 빈 메모리 공간 관리에 초점을 맞춤
- 페이지 캐시는 당연히 디스크에서 읽어온 페이지를 메모리에 캐싱함. 하지만 메모리는 휘발성이 있기 때문에 시스템 충돌 시 해당 내용은 손실됨.

이 책에서는 '페이지 캐시'라는 용어를 기본으로 사용하는데, 이는 이 구조의 목적을 더 잘 반영하기 때문입니다. '버퍼 풀'이라는 용어는 내용 공유 없이 비어 있는 버퍼를 모으고 재사용하는 것이 주된 목적처럼 들릴 수 있는데, 이는 페이지 캐시의 유용한 부분 또는 별도의 구성 요소가 될 수는 있지만 전체적인 목적을 정확하게 반영하지는 못합니다.

- 책에서는 ‘페이지 캐시’라는 용어를 사용. 즉 전자에 주목하겠다는 뜻. ‘버퍼 풀’ 이라는 네이밍은 (빈 버퍼를 관리한다는 점에서) 페이지 캐시의 유용한 점 중 하나이지만 주된 목적을 드러내기엔 부족함

페이지 캐싱 문제는 데이터베이스에만 국한되지 않습니다. 운영 체제 역시 페이지 캐시 개념을 가지고 있습니다. 운영 체제는 사용되지 않는 메모리 세그먼트를 활용하여 디스크 내용을 투명하게 캐시함으로써 I/O 시스템 호출의 성능을 향상시킵니다. 캐시되지 않은 페이지는 디스크에서 로드될 때 '페이지 인(paged in)'된다고 합니다. 캐시된 페이지에 변경 사항이 생기면, 이러한 변경 사항이 디스크에 다시 플러시될 때까지 해당 페이지는 '더티(dirty)' 상태라고 합니다.

- OS를 배우다 보면 페이지 캐시 내용이 나옴. OS는 현재 프리한 메모리 공간을 활용하여 디스크 데이터를 캐싱함.
- 캐시되지 않았던 페이지가 요청에 의해 캐시될 때 이를 페이지 인이라고 함. 즉 페이지를 디스크에서 읽어와 메모리에 로드하는 것
- 변경사항이 발생하면 이 페이지 캐시에 먼저 적용된다고 했음. 이렇게 변경이 발생하면 ‘더티’ 상태의 페이지가 됨. 변경사항을 디스크에 반영하기 위해서는 이 더티 페이지를 ‘플러시’해야 함. 그러면 해당 변경사항이 디스크에 영속화(영구 저장)됨

캐시된 페이지들이 보관되는 메모리 영역은 전체 데이터셋보다 일반적으로 상당히 작기 때문에, 페이지 캐시는 결국 가득 차게 되고, 새로운 페이지를 페이지인(page in)하기 위해서는 캐시된 페이지 중 하나가 제거(evict)되어야 합니다.

- 메모리는 디스크보다 항상 적은 용량을 가짐. 따라서 디스크에서 다양한 데이터를 요청한다면 페이지 캐시는 빠르게 가득 차게 될 것이고, 여유 공간이 없어짐. 이러면 캐시된 페이지를 방출(evict) 해야 함

Figure 5-1에서 B-Tree 페이지의 논리적 표현, 캐시된 버전, 디스크 상의 페이지 간의 관계를 볼 수 있습니다. 페이지 캐시는 페이지를 자유로운 슬롯에 순서에 상관없이 로드하기 때문에, 디스크에 페이지가 저장된 순서와 메모리에 저장된 순서 사이에 직접적인 매핑은 없습니다.

![image.png](%E1%84%80%E1%85%A9%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%87%E1%85%A2%E1%86%A8%E1%84%8B%E1%85%A6%E1%86%AB%E1%84%83%E1%85%B3%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%208%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20222998e1b709809cbe85de19b92ba4c7/image.png)

- 사진처럼 각 노드 = 페이지임. 진한 색 페이지는 메모리에 올라가있음.

페이지 캐시의 주요 기능은 다음과 같이 요약될 수 있습니다:

- 캐시된 페이지 내용을 메모리에 유지합니다.
- 디스크 상의 페이지 수정 사항을 함께 버퍼링하고 캐시된 버전을 대상으로 수행할 수 있도록 합니다.
- 요청된 페이지가 메모리에 없고 이를 위한 충분한 공간이 있을 경우, 페이지 캐시에 의해 페이지 인(page in)되어 캐시된 버전이 반환됩니다.
- 이미 캐시된 페이지가 요청될 경우, 해당 캐시된 버전이 반환됩니다.
- 새 페이지를 위한 충분한 공간이 없을 경우, 다른 페이지가 제거(evicted)되고 해당 내용은 디스크에 플러시(flushed)됩니다.

→ 앞에서 전부 설명한 내용. 다만 제거될 때 더티 페이지라면 변경사항을 플러시한다는 점만 참고.

### Bypassing the kernel page cache

많은 데이터베이스 시스템은 O_DIRECT 플래그를 사용하여 파일을 엽니다. 이 플래그는 I/O 시스템 호출이 커널 페이지 캐시를 우회하여 디스크에 직접 접근하고, 데이터베이스 고유의 버퍼 관리를 사용하도록 허용합니다. 이는 때때로 운영 체제 전문가들에 의해 좋지 않게 여겨지기도 합니다.

Linus Torvalds는 O_DIRECT 사용이 비동기적이지 않고 커널에 접근 패턴을 지시할 리드어헤드(readahead) 또는 다른 수단이 없기 때문에 이를 비판했습니다. 그러나 운영 체제가 더 나은 메커니즘을 제공하기 전까지는 O_DIRECT가 계속 유용할 것입니다.

우리는 fadvise를 사용하여 커널이 캐시에서 페이지를 제거하는 방식에 대한 일부 제어를 얻을 수 있지만, 이것은 커널에게 우리의 의견을 고려해달라고 요청할 뿐이며 실제로 그렇게 될 것이라고 보장하지는 않습니다. I/O를 수행할 때 시스템 호출을 피하기 위해 메모리 매핑(memory mapping)을 사용할 수 있지만, 그렇게 하면 캐싱에 대한 제어를 잃게 됩니다.

- 보통 일반적인 애플리케이션은 운영체제에 파일 IO를 위임함. 하지만 많은 DBMS는 O_DIRECT 플래그를 사용하여 IO를 수행함.
- 이 O_DIRECT 플래그는 파일 시스템의 버퍼링을 우회하고, 디스크에 직접 접근하도록 지시하는 POSIX 표준 플래그임. 이는 DBMS가 디스크 IO 시 OS의 간섭을 최소화할 수 있음
- 구체적으로는 커널 페이지 캐시를 우회하여 디스크에 직접 접근하고, DBMS의 버퍼 관리를 사용
  - 일반적인 OS의 파일 IO는 데이터를 커널 페이지 캐시에 로드함. 이는 OS가 애플리케이션 IO를 효율적으로 처리하기 위함임. 하지만 DBMS 애플리케이션은 자체적으로 페이지 캐시를 가지고 있음. 따라서 OS의 페이지 캐시와 DBMS의 페이지 캐시가 같이 동작하면 이중으로 메모리를 사용하므로 낭비임. 또 캐시 일관성 유지와 같은 것들이 복잡해질 수 있음
  - O_DIRECT를 쓰면 이러한 OS 페이지 캐시를 건너뛰고 디스크에 직접 명령어를 전달하게 되므로 DBMS가 더 세밀하게 제어할 수 있게 되는 것
  - 이렇게 하는 이유는 DBMS가 OS 페이지 캐시에 비해 어떤 데이터가 얼마나 자주, 어떤 순서로 사용될 지에 대해 알기 때문임 (인덱스 정보 등을 통해서…). 즉 DBMS의 페이지 캐시를 쓰는 쪽이 더 효율적.
  - 참고로 O_DIRECT를 안 쓰는 쪽, 즉 커널 페이지 캐시를 거쳐가는 쪽을 Buffered I/O 라고 하고, 쓰는 쪽을 Direct I/O 라고 함.
- 하지만 이게 별로 안 좋다고 생각하는 OS 개발자들도 있음. 리누스 토르발스…
- O_DIRECT가 비동기적이지 않다
  - 일반적인 쓰기 작업은 Buffered IO이기 때문에, 쓰기 작업은 실제 디스크 쓰기가 아닌 페이지 캐시에 대한 쓰기 작업임. 그래서 페이지 캐시에 변경사항을 반영하고 바로 완료 처리를 함. 이후 커널이 나중에 지연 쓰기를 통해 변경사항을 디스크로 플러시함. 즉, 애플리케이션은 느린 디스크 쓰기를 기다리지 않으므로 비동기적으로 동작함.
    - 여기서 발하는 동기 / 비동기 IO는 변경사항이 디스크에 바로 반영되냐 / 반영되지 않느냐의 차이로 이해해야 함
    - [https://onecellboy.tistory.com/185](https://onecellboy.tistory.com/185)
  - O_DIRECT를 쓰면 페이지 캐시를 거치지 않기 때문에 당연히 변경사항이 디스크로 바로 반영됨. 따라서 동기적 IO… 라고 생각할 수 있지만 정확하게는 아님
    - O_DIRECT → 그냥 OS의 페이지 캐시를 우회하여 DMA로 데이터를 내려보내고 리턴한다는 뜻. 따라서 데이터가 모두 전송됐는지를 보장하지 않음
      - DMA → 주변장치(하드웨어)가 CPU를 거치지 않고 메모리에 직접 IO를 수행할 수 있게 하는 기능임. DMA가 없다면 CPU가 직접 데이터 송수신을 처리해야 함. 이는 디스크 컨트롤러에서 한 바이트씩 읽어와서 메인 메모리로 로드하는 과정을 포함함. 이걸 CPU가 직접 하는 건 비효율적임. 그래서 DMA 컨트롤러라는 걸 두고, CPU는 DMA 컨트롤러에게 명령을 내린 뒤 다른 작업을 수행함. 그리고 데이터 송수신이 완료되면 DMA 컨트롤러는 인터럽트를 보냄. CPU는 해당 인터럽트를 ACK하여 받아온 데이터를 처리함
      - 이때 O_DIRECT로 write()를 호출하면, 커널은 페이지 캐싱을 위해서 페이지인을 하지 않고, 해당 메모리 버퍼에 대응하는 물리적 주소를 찾음. 이를 DMA 컨트롤러로 넘겨줘서 직접 데이터를 쓸 것을 요청함. 그러면 DMA 컨트롤러가 수행.
    - O_SYNC → O_DIRECT에서 DMA로 데이터를 내려보내면 바로 리턴하여 성공을 반환함. 하지만 디스크 컨트롤러 내부에는 내부 캐시(DRAM write-back buffer) 등이 존재함. 만약 O_DIRECT 성공 직후 충돌이 발생했다면 이 캐시에 저장된 정보는 유실됨. 따라서 장치 내부 캐시 역시 flush를 해줘야 내구성을 보장할 수 있음. 그래서 O_DIRECT와 O_SYNC는 같이 쓰인다.
  - “커널에 접근 패턴을 지시할 리드어헤드(readahead) 또는 다른 수단이 없기 때문에 이를 비판했습니다”
    - 커널은 파일 접근 패턴을 예측할 수 있음. 그래서 1번 이후 2번을 읽는 패턴을 감지했다면, 그 다음에는 3번 4번을 읽을 것이라고 예측할 수 있음. 그러면 실제로 3번 4번을 요청하기 전에 선제적으로 이 데이터를 가져와서 페이지 캐시에 저장.
    - O_DIRECT를 통해 페이지 캐시를 우회하면 접근 패턴을 예측할 수 없고, 리드어헤드 최적화 역시 불가능함.
  - “우리는 fadvise를 사용하여 커널이 캐시에서 페이지를 제거하는 방식에 대한 일부 제어를 얻을 수 있지만, 이것은 커널에게 우리의 의견을 고려해달라고 요청할 뿐이며 실제로 그렇게 될 것이라고 보장하지는 않습니다.”
    - fadvice → 위와 같이 페이지 캐시를 우회하면 커널이 접근 패턴을 예측할 수 없기 때문에… fadvice를 통해 특정 파일에 대해 예상 접근 패턴을 커널에 미리 알려줄 수 있음. 가령…
      - `POSIX_FADV_NORMAL`: 특별한 패턴이 없으니, 평소처럼 작동해 주세요. (기본값)
      - `POSIX_FADV_SEQUENTIAL`: 저는 이 파일을 순차적으로 처음부터 끝까지 읽을 예정입니다. (리드어헤드 최적화에 도움)
      - `POSIX_FADV_RANDOM`: 저는 이 파일의 여러 부분을 무작위 순서로 접근할 것입니다. (리드어헤드를 줄여 불필요한 I/O 방지)
      - `POSIX_FADV_DONTNEED`: 저는 이 파일의 특정 부분을 다 사용했으니, 당분간 다시 사용할 일이 없을 것 같습니다. 그러니 해당 데이터가 차지하는 페이지 캐시를 비워도 좋습니다.
    - 이런 게 있으니 괜찮지 않나요? 싶을 수 있겠지만, 이는 ‘요청’일 뿐임 → 실제로 `POSIX_FADV_DONTNEED` 로 알려주더라도, 커널은 해당 데이터의 **페이지 캐시를 지울 수도 있지만, 지우지 않을 수도 있음**
      - 메모리가 충분할 경우 → 시스템에 여유 메모리가 아주 많다면, 굳이 캐시를 비우는 작업을 서두를 이유가 없습니다. "혹시 모르니 일단 그냥 두자"라고 판단할 수 있음
      - 다른 우선순위 작업이 있을 경우 → 커널이 더 시급한 다른 I/O 작업이나 프로세스를 처리하고 있다면, 캐시를 비우는 작업의 우선순위를 뒤로 미룰 수 있음
      - 커널의 자체적인 판단 → 커널의 내부 알고리즘이 지금 캐시를 비우는 것이 시스템 전체 성능에 도움이 되지 않는다고 판단하면 요청을 무시할 수도 있음
  - “I/O를 수행할 때 시스템 호출을 피하기 위해 메모리 매핑(memory mapping)을 사용할 수 있지만, 그렇게 하면 캐싱에 대한 제어를 잃게 됩니다.”
    - OS에서 IO를 수행할 때 어떤 일이 일어나는지 잠깐 복습해보자
      - 애플리케이션이 특정 파일의 특정 부분을 읽어달라고 read() 호출
      - 이때 OS의 제어권이 유저 모드에서 커널 모드로 전환됨. 즉 컨텍스트 스위칭 발생
      - 커널 작업이 수행됨. 커널은 페이지 캐시를 확인하고, 없다면 디스크에서 데이터 읽어와서 전달
      - 그리고 다시 커널 모드에서 유저 모드로 제어권 넘김
    - 메모리 매핑 (mmap) → 파일을 디스크에서 읽는 대신, 파일 내용의 ‘전체’ 혹은 ‘일부’를 애플리케이션의 가상 주소 공간에 직접 매핑함. 매핑이 완료되면 read(), write() 호출하는 대신 메모리에 접근하듯이 포인터 연산으로 접근. 만약 첫 접근으로 인해 아직 로드되지 않은 부분이라면 페이지 폴트 발생. 이후 커널이 자동으로 데이터를 가져옴. 이는 여전히 DMA 거치므로 CPU 이슈 없음
      - 하지만 이 방식을 쓰면 페이지 폴트에 의해서만 데이터가 올라가고 내려감. 그래서 애플리케이션이 페이지 캐시 제어하기 어려워짐. 이때 커널은 페이지 교체 알고리즘에 따라 페이지를 제어. 가령 LRU, FIFO, … 같은 것들

### Caching Semantics

버퍼에 가해진 모든 변경 사항은 결국 디스크에 기록되기 전까지 메모리에 유지됩니다. 다른 프로세스는 백업 파일을 변경할 수 없기 때문에, 이 동기화는 메모리에서 디스크로의 단방향 과정입니다. 페이지 캐시는 데이터베이스가 메모리 관리 및 디스크 접근을 더 잘 제어할 수 있게 해줍니다. 이는 커널 페이지 캐시의 애플리케이션 특화 버전이라고 생각할 수 있습니다. 페이지 캐시는 블록 장치에 직접 접근하고, 유사한 기능을 구현하며, 유사한 목적을 수행합니다. 이는 디스크 접근을 추상화하고, 논리적 쓰기 연산을 물리적 쓰기 연산으로부터 분리합니다.

- 아까 전 우리는 변경사항을 디스크에 바로 쓰지 않고 페이지 캐시에 적용한다고 했었음. 이 변경사항은 디스크로 플러시되기 전까지 메모리에 유지됨
- 데이터 파일은 DBMS만 접근 가능하고 외부에서 변경되지 않는다고 가정하자. 그러면 동기화 방향은 항상 메모리 → 디스크임. 즉 디스크 → 메모리 역방향 동기화는 고려하지 않아도 됨
- DBMS의 페이지 캐시는 커널 페이지 캐시보다 더 효율적임 (앞에서 “어떤 데이터가 얼마나 자주, 어떤 순서로 사용될 지에 대해 알기 때문임 (인덱스 정보 등을 통해서…).” 이라고 했었음)
- 이러한 작업은 추상화되어 있으며 단순히 디스크에 물리적으로 쓰는 것만으로 이루어지지 않음. 즉 논리적인 의미의 쓰기와 물리적인 쓰기 분리. 이를 통해서 지연 쓰기나 페이지 캐시 등의 최적화 수행 가능함

페이지를 캐싱하면 알고리즘을 변경하거나 객체를 메모리에 실체화하지 않고도 트리의 일부를 메모리에 유지할 수 있습니다. 우리가 해야 할 일은 디스크 접근을 페이지 캐시 호출로 대체하는 것뿐입니다.

- 이러한 페이지 캐시를 위해서 뭔가 추가로 바꿔야 하는 게 아님. 그냥 디스크 호출 시 페이지 캐시를 호출하도록 하면 됨.

스토리지 엔진이 페이지에 접근(즉, 페이지를 요청)할 때, 먼저 해당 내용이 이미 캐시되어 있는지 확인하며, 캐시되어 있다면 캐시된 페이지 내용이 반환됩니다. 페이지 내용이 아직 캐시되어 있지 않다면, 캐시는 논리적 페이지 주소 또는 페이지 번호를 물리적 주소로 변환하여 해당 내용을 메모리에 로드한 다음, 캐시된 버전을 스토리지 엔진에 반환합니다. 일단 반환되면, 캐시된 페이지 내용을 담고 있는 버퍼는 '참조(referenced)'되었다고 말하며, 스토리지 엔진은 작업이 완료되면 이를 페이지 캐시로 다시 반환하거나 '참조 해제(dereference)'해야 합니다. 페이지 캐시는 페이지를 '고정(pinning)'하여 제거(evicting)하지 않도록 지시받을 수 있습니다.

- 세부 로직은 간단함. 페이지 요청 시 캐시되어 있다면 그대로 반환하고, 캐시되어 있지 않다면 페이지를 메모리에 로드한 후 해당 캐시를 반환. 즉 캐시를 반환한다는 점은 동일함
  - 이때 논리적 페이지 주소 (페이지 주소) → 물리적 주소로 변환하는 과정이 있다고 했음.
  - 해당 내용의 경우 앞 부분에서 다뤘었음. 고정 크기 페이지라면 offset = page_id \* page_size로 구할 수 있지만, 가변 및 재배치 가능 구조라면 룩업 테이블을 사용한다. 룩업 테이블은 페이지 ID를 키로 실제 오프셋에 바로 접근할 수 있게 해주는 맵 구조임.
  - 이 룩업 테이블이 페이지 캐시 내부 메모리 영역에 위치함
- 페이지가 사용중인지 여부를 나타내기 위해 ‘참조’ 상태로 표현. 페이지 사용이 끝나면 해당 버퍼에 대하여 ‘참조 해제’ 해야 함. 그러면 이 버퍼를 재사용하거나 제거할 수 있게 됨. 하지만 페이지가 계속 사용되어야 하는 경우가 있음. 이 경우 페이지를 고정(pinnning)할 수 있음. 이러면 제거 대상에서 제외됨.
  - 가령 B-트리의 루트 노드 페이지는 계속 접근되기 때문에 pinning에 대한 필요성이 있음

페이지가 수정되면 (예를 들어, 셀이 추가되면) 더티 상태로 표시됩니다. 페이지에 설정된 더티 플래그는 해당 내용이 디스크와 동기화되지 않았으며 내구성을 위해 플러시되어야 함을 나타냅니다.

- 변경사항은 페이지 캐시에 반영됨. 그리고 변경사항이 반영되면 더티 플래그가 활성화됨. 이는 해당 변경사항을 디스크로 플러시해야 한다는 의미.

### Cache Eviction

캐시를 채워두는 것은 좋습니다. 영구 저장소(디스크)에 접근하지 않고도 더 많은 읽기 요청을 처리할 수 있으며, 동일 페이지에 대한 더 많은 쓰기 작업도 함께 버퍼링될 수 있기 때문입니다. 그러나 페이지 캐시는 용량에 제한이 있으며, 조만간 새로운 내용을 수용하기 위해 오래된 페이지들은 제거(evict)되어야 합니다. 만약 페이지 내용이 디스크와 동기화되어 있고(즉, 이미 플러시되었거나 수정된 적이 없다면), 페이지가 고정(pin)되거나 참조되고 있지 않다면 즉시 제거될 수 있습니다. 더티 페이지는 제거되기 전에 반드시 플러시되어야 합니다. 참조되고 있는 페이지는 다른 스레드가 사용 중인 동안 제거되어서는 안 됩니다.

- 일단 캐시를 많이 쓴다는 건 좋음. 읽기 관점에서는 디스크 엑세스를 줄이기 때문에 좋고, 쓰기 관점에서도 더 많이 버퍼링할 수 있기 때문에 마찬가지로 디스크 엑세스를 줄일 수 있어서 좋음.
- 하지만 RAM은 디스크에 비해서 비싸고 더 적은 용량을 가지는 편임. 따라서 디스크의 모든 데이터를 메모리로 올릴 수 없음. 따라서 캐시가 가득 찬 경우 오래된 페이지를 제거(evict)해야 함.
- 만약 클린한 페이지이거나(즉 변경사항이 반영되지 않아서, 디스크 페이지 상태와 동기화된 경우) 고정된 페이지(자주 접근되어야 하는)가 아닌 경우라면 바로 evict할 수 있음.
- 하지만 더티 페이지인 경우에는 플러시 후에 evict되어야 함.
- 또 참조 상태인 페이지라면 사용 중인 것이므로 당연히 evict되면 안됨

모든 페이지를 제거할 때마다 플러시를 발생시키는 것은 성능에 좋지 않을 수 있기 때문에, 일부 데이터베이스는 별도의 백그라운드 프로세스를 사용하여, 제거될 가능성이 높은 더티 페이지를 순환하면서 디스크 버전을 갱신합니다. 예를 들어, PostgreSQL에는 이러한 역할을 수행하는 백그라운드 플러시 라이터가 있습니다.

- 페이지가 제거될 때마다 플러시하게 되면 디스크 IO 빈도가 늘어나므로 성능 저하의 우려가 있음
- 그렇다면 어떻게? → 백그라운드 플러시 프로세스. 즉 비동기적으로 플러시하는 별도의 프로세스 도입. 페이지가 실제로 제거되기 전에 미리 더티 페이지를 순회해서 플러시하는 방식임. 이러면 위에서 봤듯 바로 evict가 가능하므로 굿

또한 주의해야 할 중요한 속성은 내구성입니다. 만약 데이터베이스가 크래시된다면, 플러시되지 않은 모든 데이터는 손실됩니다. 모든 변경사항이 지속되도록 보장하려면, 플러시는 체크포인트 프로세스에 의해 조율되어야 합니다. 체크포인트 프로세스는 Write-Ahead Log(WAL)과 페이지 캐시를 동시에 제어하며, 두 시스템이 연동되도록 보장합니다. 캐시된 페이지에 적용된 연산과 관련된 로그 레코드 중, 해당 페이지가 디스크에 플러시된 경우에만 WAL에서 삭제될 수 있습니다. 더티 페이지는 이 프로세스가 완료되기 전까지 제거될 수 없습니다.

- 한편 ACID 중 D인 내구성을 보장해야 함. 이는 일단 트랜잭션이 성공적으로 커밋되었다면 무슨 일이 있어도 해당 변경사항은 영구적으로 반영된다는 것임.
- 내구성을 보장하려면 ‘체크포인트 프로세스’ 에 의해서 플러시를 수행시켜야 함
- 체크포인트 프로세스는 WAL과 페이지 캐시를 동시에 제어
- WAL은 앞에서 말했듯 각각의 변경사항이 발생하기전 이러한 내용이 먼저 기록되는 로그 파일이라고 했었음. 왜? 변경사항은 바로 플러시되지 않고 페이지 캐시에 반영되고 페이지 캐시는 휘발성 메모리에 존재하므로 장애 시 해당 변경사항이 유실될 수 있기 때문임.
- 만약 장애가 발생하면 이를 복구(리커버리)해야 하는데, 이때 WAL을 사용하여 undo할지 redo할지 결정한다 했었음.
- 따라서 이 WAL은 페이지 캐시가 플러시되어 디스크에 영속화되기 전까지 남아있어야 함. 그렇지 않으면 장애 발생 시 복구가 불가능할 수 있음. 따라서 플러시가 발생하지 않은 더티 페이지는 체크포인트 프로세스가 돌기 전까지 제거(evict)될 수 없음

이는 다음과 같은 여러 목표들 사이에서 항상 트레이드오프가 존재한다는 것을 의미합니다:

- 디스크 접근 횟수를 줄이기 위해 플러시를 지연시키는 것
- 빠른 제거(eviction)를 가능하게 하기 위해 페이지를 미리 플러시하는 것
- 제거 및 플러시 작업을 최적의 순서로 수행하는 것
- 캐시 크기를 메모리 제약 내에 유지하는 것
- 아직 영속 저장소에 기록되지 않은 데이터를 유실하지 않는 것.

우리는 앞의 세 가지 특성을 개선하면서, 나머지 두 가지 제약을 충족하는 여러 가지 기법들을 살펴볼 것입니다.

- 개선해야 하는 점
  - 플러시를 늦추면 더 많이 버퍼링할 수 있으므로 IO를 줄일 수 있음. 하지만 더티 페이지는 플러시되기 전까지 제거될 수 없으므로 이로 인한 딜레이 발생 가능
  - 반대로 자주자주 플러시를 하면 이러한 제거 딜레이를 없앨 수 있음
  - 한편 어떤 페이지를 어떤 순서로 제거하고 플러시할 지 역시 중요함
- 제약조건
  - 캐시가 커지면 오히려 좋지 않을 수도 있음. 초기에는 좋지만, 일정 수준 이상부터는 히트율이 낮아짐. 접근 빈도가 낮은 페이지들이 들어오기 때문임. 또, 캐시 관리 비용 역시 감수해야 함. 적절한 캐시 사이즈를 결정해야 하는 니즈가 있음
  - 또 플러시가 늦으면 장애 발생 시 손실이 더 자주 발생함.

### Locking Pages in Cache

읽기 또는 쓰기 작업마다 디스크 I/O를 수행해야 하는 것은 비실용적입니다. 후속 읽기 작업이 동일한 페이지를 요청할 수 있고, 후속 쓰기 작업이 동일한 페이지를 수정할 수 있기 때문입니다. B-트리가 상단으로 갈수록 '좁아지기' 때문에, (루트에 더 가까운) 상위 레벨 노드는 대부분의 읽기 작업에서 더 많이 접근됩니다. 분할(split) 및 병합(merge) 또한 결국 상위 레벨 노드로 전파됩니다. 이는 항상 트리의 일부가 캐싱으로부터 상당한 이점을 얻을 수 있음을 의미합니다.

- 이후 작업 대상이 같은 페이지일 수 있으므로 매 읽기/쓰기마다 디스크 IO를 한다면 손해임 (로컬리티)
- B-Tree 탐색은 위에서 아래로 발생. 그런데 윗쪽 노드들 개수는 적기 때문에, 윗쪽 노드들일수록 자주 읽기 작업에 의해서 접근됨
- 또 쓰기 역시 분할 및 병합이 상위 노드로 연쇄적으로 전파될 수 있기 때문에 윗쪽 노드에 대한 접근이 잦기도 함
- 이렇듯 윗쪽과 아랫쪽 노드(페이지)의 접근 빈도가 다르기 때문에, 자주 접근되는 페이지를 캐싱해둔다면 효율적으로 작업할 수 있음

가까운 시일 내에 사용될 가능성이 높은 페이지는 '잠글' 수 있습니다. 캐시에서 페이지를 잠그는 것을 '피닝(pinning)'이라고 합니다. 피닝된 페이지는 더 오랫동안 메모리에 유지되며, 이는 디스크 접근 횟수를 줄이고 성능을 향상시키는 데 도움이 됩니다 [GRAEFE11].

- pinning → 자주 사용될 것 같은 페이지를 잠구어 evict로부터 막음

각 B-트리 하위 레벨 노드는 상위 레벨 노드보다 기하급수적으로 더 많은 노드를 가지며, 상위 레벨 노드는 트리 전체에서 아주 작은 부분을 차지하기 때문에, 트리의 이 부분은 메모리에 영구적으로 상주할 수 있으며, 다른 부분은 요청 시에 페이징될 수 있습니다. 이는 쿼리를 수행하기 위해, 우리는 'B-트리 탐색 복잡도'에서 논의된 바와 같이 'h'번의 디스크 접근(여기서 'h'는 트리의 높이)을 할 필요가 없지만, 페이지가 캐시되지 않은 하위 레벨에 대해서만 디스크를 접근하면 된다는 것을 의미합니다.

- 위에서 말했듯 상위 레벨 노드가 더 적으며 자주 접근되기 때문에 이를 pinning할 수 있음.
- 일반적으로 우리는 리프까지 내려가기 위해 트리의 높이인 h만큼 디스크 엑세스가 필요했음. 하지만 캐싱을 고려하면, 캐싱되지 않은 하위 레벨 노드에 대해서만 디스크 엑세스를 수행하므로 실제로는 더 적은 횟수로 가능함

서브트리에 대해 수행되는 연산은 서로 모순되는 구조적 변경을 초래할 수 있습니다. 예를 들어, 여러 삭제 연산으로 인해 병합이 발생한 후 쓰기 연산으로 인해 분할이 발생하거나 그 반대의 경우가 있을 수 있습니다. 마찬가지로 서로 다른 서브트리에서 발생하는 구조적 변경 (시간적으로 가깝고 트리의 다른 부분에서 발생하여 상위 레벨로 전파되는 구조적 변경)의 경우에도 그렇습니다. 이러한 연산들은 변경 사항을 메모리에만 적용하여 함께 버퍼링할 수 있으며, 이는 여러 번의 쓰기 대신 한 번의 쓰기만 수행하여 디스크 쓰기 횟수를 줄이고 연산 비용을 분산시키는 효과를 가져옵니다.

- 서로 모순되는 구조적 변경 → 삭제 / 병합 / 삭제 병합이 연속적으로 발생할 수 있음. 가령 특정 노드에서 삭제로 인해 병합이 발생한 후, 다시 삽입으로 인해 분할되어야 하는 경우.
- 혹은 서로 다른 서브트리의 변경이 부모로 전파되는 경우에, 이 변경이 충돌하는 경우. 가령 한 쪽에서는 분할이 필요하고 한 쪽에서는 병합이 필요하여 부모로 전파되는 경우.
- 이 경우 버퍼링을 하면 메모리 버퍼 상에서 이러한 변경사항을 한 번에 모아서 처리할 수 있으므로, 미리 이 변경사항을 종합하여 구조적인 변경을 막을 수 있음.

### Prefetching and Immediate Eviction

페이지 캐시는 또한 스토리지 엔진이 프리패칭과 퇴출에 대해 세밀하게 제어할 수 있도록 합니다. 페이지는 접근되기 전에 미리 로드되도록 지시할 수 있습니다. 예를 들어, 범위 스캔에서 리프 노드를 순회할 때, 다음 리프 노드들을 미리 로드할 수 있습니다. 마찬가지로, 유지보수 프로세스가 페이지를 로드하는 경우, 해당 페이지는 프로세스가 완료된 직후 즉시 퇴출될 수 있습니다. 왜냐하면 해당 페이지가 진행 중인 쿼리에 유용할 가능성이 낮기 때문입니다. 일부 데이터베이스, 예를 들어 PostgreSQL은 대규모 순차 스캔을 위해 원형 버퍼(다른 말로 FIFO 페이지 교체 정책)를 사용합니다.

- 페이지 캐시는 프리페칭, 즉 페이지를 미리 불러오는 동작이나 eviction에 대하여 세밀한 제어를 가능케 함
- 프리페칭 → 앞에서 커널 페이지 캐시에서 봤던 것처럼 실제로 필요해지기 전에 미리 로드하는 방법이 가능. 다만 미리 로드할 대상을 결정하는 것이 조금 더 쉬움. 가령 범위 스캔에서 리프 노드를 순회하기 위해서는 다음 리프 노드를 미리 로드해두는 게 매우 효과적일 수 있음.
- 한메인터넌스 프로세스 → GC 같은 것들의 경우 일반적인 유저의 유즈케이스와는 다름. 즉 페이지를 로드한 이후 다시 쓸 가능성이 없음. 정확하게는 지금 유저가 실행하고 있는 쿼리에서 필요로 하는 것과 겹칠 가능성은 낮음. 그러면 이런 프로세스에 의해 로드된 페이지 캐시는 즉시 퇴출하는 것이 더 좋음.
- PostgreSQL의 경우 대규모 순차 스캔을 위해서 원형 버퍼 사용 → 오래된 데이터를 덮어쓰는 FIFO 방식. 대규모 순차 스캔의 경우 뒤의 것을 읽으면 다시 접근할 가능성이 낮음. 즉 예전 데이터가 다시 필요할 가능성은 매우 낮음. 이 경우 FIFO가 효과적으로 작동함.

### Page Replacement

캐시 용량에 도달하면, 새로운 페이지를 로드하기 위해 오래된 페이지들을 제거해야 합니다. 하지만, 곧 다시 접근될 가능성이 가장 낮은 페이지들을 제거하지 않으면, 계속 메모리에 보관할 수 있었음에도 불구하고 나중에 여러 번 다시 로드하게 될 수 있습니다. 이를 최적화하기 위해 다음 페이지 접근의 가능성을 예측할 방법을 찾아야 합니다.

- 캐시 퇴출에 대해서는 이미 논의했었음. 이때 단순히 오래된 순서대로 제거하는 게 아니라, 가까운 시일 내에 다시 안 쓰일 것 같은 페이지를 제거해야 함. 그렇지 않으면 디스크에서 다시 불러와야 하므로 비효율적

이를 위해, 페이지는 eviction policy(퇴출 정책, 때로는 page-replacement policy라고도 불림)에 따라 제거되어야 한다고 할 수 있습니다. 이 정책은 가까운 시일 내에 다시 접근될 가능성이 가장 낮은 페이지를 찾아 제거하려고 시도합니다. 페이지가 캐시에서 제거되면, 그 자리에 새로운 페이지를 로드할 수 있습니다.

- 이를 위해서 퇴출 정책 혹은 페이지 교체 정책에 따라서 페이지를 제거해야 함. 목표는 앞으로 당분간 안 쓸 페이지를 찾가서 제거하는 것.

페이지 캐시 구현이 성능을 잘 내기 위해서는 효율적인 페이지 교체 알고리즘이 필요합니다. 이상적인 페이지 교체 전략은, 마치 마법의 수정구슬처럼 앞으로 어떤 순서로 페이지에 접근할지를 예측하여, 가장 오랫동안 접근되지 않을 페이지만 제거하는 것입니다. 하지만 요청은 특정한 패턴이나 분포를 따르지 않는 경우가 많기 때문에, 이러한 동작을 정확히 예측하는 것은 복잡할 수 있습니다. 그렇지만 적절한 페이지 교체 전략을 사용하면 제거 횟수를 줄이는 데 도움이 될 수 있습니다.

- 제일 좋은 것은 미래를 정확하게 예측하는 것이지만 (우리는 미래를 모르기 때문에) 현실적으로 불가능함
- 이러한 패턴을 예측하는 건 어렵지만 적절한 전략을 쓰면 효과적으로 처리할 수 있음

더 큰 캐시를 사용하면 단순히 제거 횟수를 줄일 수 있을 것처럼 보이지만, 실제로는 그렇지 않은 경우도 있습니다. 이 딜레마를 보여주는 사례 중 하나가 Bélády의 역설(Bélády’s anomaly)입니다. 이 역설은, 비효율적인 페이지 교체 알고리즘을 사용할 경우, 캐시 페이지 수를 늘리면 오히려 제거 횟수가 증가할 수 있음을 보여줍니다. 곧 필요할 수 있는 페이지가 제거되었다가 다시 로드되면, 페이지들이 캐시 공간을 두고 경쟁하게 됩니다. 그렇기 때문에, 상황을 악화시키지 않고 개선할 수 있도록, 우리가 사용하는 알고리즘을 신중하게 선택해야 합니다.

- 그러면 그냥 캐시를 키우면 되지 않나요? → 아님. 벨라디의 역설. 페이지 수를 늘리면 오히려 페이지 폴트(페이지 제거)가 증가하는 현상.
- 따라서 캐시 크기를 키우는 방법이 아니라 알고리즘을 잘 골라야 함

### FIFO and LRU

가장 단순한 페이지 교체 전략은 FIFO(First-In, First-Out)입니다. FIFO는 삽입된 순서대로 페이지 ID를 큐에 저장하고, 새로운 페이지가 삽입될 때는 큐의 끝에 추가합니다. 캐시가 가득 차면, 큐의 앞쪽에서 가장 오래전에 삽입된 페이지를 제거합니다. 이 방식은 **페이지에 다시 접근하는 상황**을 전혀 고려하지 않고 단지 삽입 시점만을 기준으로 삼기 때문에, 대부분의 실제 시스템에서는 비효율적인 것으로 드러납니다. 예를 들어, 트리 구조에서 루트나 상위 수준 페이지는 가장 먼저 삽입되지만, 구조적으로 **곧 다시 접근될 가능성이 높음에도 불구하고** 이 알고리즘에 따르면 **가장 먼저 제거 대상이 됩니다**.

- FIFO는 가장 단순한 방법임. 즉 시간 순서에 따라 모든 걸 결정함. 페이지가 디스크에서 메모리로 페이지인 될 때, 해당 페이지 ID는 큐의 꼬리에 삽입됨. 큐의 헤드는 가장 오래 전에 페이지인된 페이지임. 만약 큐가 꽉 찼다면 헤드부터 퇴출됨.
- FIFO의 가장 큰 단점은 언제 들어왔는지만 생각한다는 것임. 즉 얼마나 자주 사용되었는지나 앞으로 얼마나 자주 사용될 것인지 등 후속 접근에 대한 정보를 전혀 고려하지 않음. 그래서 가장 자주 사용된 페이지가 오래되었다는 이유만으로 퇴출될 수 있음. 이로 인한 성능 저하 발생 가능
- B-Tree 예시로 보자. 여기서 루트 노드 및 상위 레벨 노드는 가장 먼저 삽입되며 하위 레벨 노드에 비해 빈번히 사용됨. 하지만 먼저 삽입되었다는 이유로 계속 퇴출됨.

FIFO의 자연스러운 확장 형태가 LRU(Least Recently Used)입니다 [TANENBAUM14]. LRU 역시 삽입 순서대로 대기열을 유지하지만, 페이지가 **다시 접근될 경우 해당 페이지를 큐의 끝으로 이동**시켜 마치 처음 삽입된 것처럼 취급합니다. 그러나, 매 접근마다 참조를 갱신하고 큐 내부 노드를 재정렬하는 작업은 **동시성 환경에서 비용이 매우 클 수 있습니다**.

- LRU는 페이지 제거 후보를 삽입 순서대로 큐에 유지하지만, 반복 접근 될 때마다 처음 페이지인된 것처럼 그 페이지를 꼬리로 다시 이동시킴.
- 하지만 이 과정에서 오버헤드 발생함. 큐 내부에서 위치를 변경해야 한다는 것인데, 보통 이를 구현하기 위해서 이중 연결 리스트와 해시 맵을 사용함. 가령 가장 최근에 사용된 페이지를 테일로 이동시킨다고 해보자. 그러면 그 페이지가 어디있는지 찾아야 하는데, 연결 리스트는 O(N)이므로 느림. 대신 해시 맵을 사용하여 해당 페이지 ID를 키로, 그리고 그 노드의 위치를 값으로 하여 O(1)로 접근함. 그리고 포인터를 갱신해주면 됨.
- 하지만 이는 동시성 환경에서 문제가 됨. 이러한 작업이 동시에 일어난다고 해보자. 리스트를 안전하게 조작하려면 포인터를 한 번에 수정해야 하는데, 이를 위해서 락을 잡아야 함. 하지만 이 리스트는 수정도 아니고 조회만 하더라도 재배열, 즉 변경이 발생하므로 매번 락이 잡힘. 그래서 동시성 환경에서 비용이 크다고 하는 것

이 외에도 다양한 LRU 기반 캐시 제거 전략이 존재합니다. 예를 들어 **2Q(Two-Queue LRU)**는 두 개의 큐를 유지하는 방식으로, 페이지가 처음 접근될 때는 첫 번째 큐에 넣고, 그 이후 다시 접근되면 **두 번째 '핫 큐'로 이동**시켜 **최근 접근과 자주 접근을 구분**할 수 있게 합니다 [JONSON94]. **LRU-K**는 페이지가 **마지막으로 접근된 시점 중 최근 K개의 기록을 추적**하여, 이 정보를 바탕으로 **페이지별로 예상 접근 시점**을 추정합니다 [ONEIL93].

- 2Q → 큐를 2개 유지시킴. 그래서 초기 접근 시 1번 큐, 재접근 시 2번째 핫 큐로 이동하여 ‘최근 접근’ 및 ‘자주 접근’을 구별. 즉 LRU가 한 번만 접근된 콜드 페이지를 길게 살려두는 문제를 해결함. 이렇게 하면 최소 2번 접근해야 버퍼에 들어오는데, 왜 1번만 접근은 무시하냐? 그건 범위 스캔이나 순차 읽기 같은 케이스는 1번만 접근하고 다시 접근 안하는 경우이기 때문. 이걸 필터링 하기 위해서 최소 2번을 요구하는 것
- LRU-K → 각 페이지마다 마지막으로 접근한 시간에 대하여 최근 K개를 기억하고, 이를 기반으로 한 콜드 페이지부터 퇴출. 구체적으로는 backward K-distance (= 현재 시점에서 페이지 p가 마지막 K 번째 나타난 위치까지의 거리) 를 따진다. 없으면 inf. 그러면 가장 큰 dist를 가진 페이지가 후보가 된다.
  - 보통 K=2를 따진다. 하지만 다른 값도 가능하다. 이렇게 빈도 K를 조정할 수 있다는 것이 특징.
  - 페이지마다 K개의 타임스탬프를 가지므로, N개의 페이지에 대해 K \* N만큼의 공간을 차지한다.
  - 가령 K=2라고 하자. 그러면 각 페이지에 대하여 최소 2번 이상 접근된 것들을 따지되, 2번째 접근된 시간을 따져서 가장 옛날에 접근된 것을 퇴출시킨다는 것. 2Q는 그냥 횟수만 비교했었음

### CLOCK

어떤 상황에서는 정밀성보다 효율성이 더 중요할 수 있습니다. CLOCK 알고리즘의 변형들은 LRU에 대한 간결하고, 캐시 친화적이며, 동시성 환경에 적합한 대안으로 자주 사용됩니다. 예를 들어, Linux는 CLOCK 알고리즘의 한 변형을 사용합니다.

- 정밀성과 효율성 → 실제로 캐시 적중률을 높이는 것보다 높은 성능이나 단순환 구현이 중요할 수 있음
- CLOCK은 LRU와 비슷한 성능을 내면서도 구조는 단순함.
  - 간결함 → 연결 리스트 없이 배열이나 원형 버퍼 사용
  - 캐시 친화적 → 메모리 로컬리티에 유리한 방식임.
  - 동시성 환경에 적합 → LRU처럼 포인터 재배열로 인한 락 없음.
- Linux OS의 페이지 교체를 위해 CLOCK 알고리즘 계열을 사용함

CLOCK-sweep은 페이지에 대한 참조와 연관된 접근 비트를 원형 버퍼(circular buffer)에 유지합니다. 일부 변형은 빈도를 고려하기 위해 비트 대신 카운터를 사용하기도 합니다. 페이지가 접근될 때마다 해당 페이지의 접근 비트는 1로 설정됩니다. 이 알고리즘은 원형 버퍼를 따라 이동하며 접근 비트를 확인하는 방식으로 작동합니다:

- 만약 접근 비트가 1이고 페이지가 참조되지 않았다면, 비트는 0으로 설정되고 다음 페이지가 검사됩니다.
- 만약 접근 비트가 이미 0이라면, 해당 페이지는 제거 후보가 되어 제거될 예정으로 지정됩니다.
- 만약 페이지가 현재 참조 중이라면, 접근 비트는 변경되지 않습니다. 접근된 페이지의 접근 비트는 0이 될 수 없으므로 제거될 수 없다고 가정합니다. 이는 참조된 페이지가 교체될 가능성을 낮춥니다.

그림 5-2는 접근 비트를 가진 원형 버퍼를 보여줍니다.

- CLOCK 알고리즘은 페이지 테이블 엔트리마다 ‘접근 비트’를 사용. 즉 접근 여부를 나타냄
- 원형 버퍼는 리스트 대신 사용. 포인터가 한 칸씩 돌면서 비트를 검사
- 기본형은 1비트만 사용하지만, 변형 알고리즘에서는 카운터를 써서 빈도를 나타내기도 함. LRU-K 같이…
- 핸드라고 부르는 CLOCK 포인터는 원형 버퍼를 따라 스캔(스윕)하면서 해당 비트를 보고 교체 여부를 판단함.
  - 만약 접근 비트 1이고 페이지 미참조 → 비트 0 설정 및 다음 페이지 검사
    - 즉 최근에 접근되었지만 현재 참조되지 않고 있는 페이지는 일단 교체하지는 않음.
    - 대신 비트를 0으로 설정해서 다음 스윕 때 교체 가능성을 만듬.
    - 즉 ‘유예 기회’, second chance를 준다. 만약 한 바퀴 도는 동안 접근되어서 다시 1로 되었다면 살아남는 것. 그렇지 않는다면 죽음
  - 만약 접근 비트 0이면 → 교체 대상
  - 현재 참조 중 → 접근 비트 변경 X.
    - 지금 쓰고 있는 중이라면 ‘참조 상태’이므로 교체되면 안됨

원형 버퍼를 사용하는 장점 중 하나는, CLOCK 핸드 포인터와 접근 비트를 모두 압축된 형태로 저장할 수 있고, 몇 개의 명령만으로 원자적으로 조작할 수 있다는 점입니다. 이는 포인터 역참조와 LRU에서 필요한 리스트 유지 작업을 제거하므로, CLOCK 알고리즘이 높은 수준의 동시성이 요구되는 환경에서 더 효율적이게 만듭니다. 그러나 접근 비트는 단지 항목이 접근된 순서를 나타낼 뿐이며, 얼마나 자주 접근되었는지를 반영하지는 않기 때문에, 접근 빈도가 중요한 요소인 워크로드에는 적합하지 않을 수 있습니다.

- 핸드 포인터 및 접근 비트를 압축 형태로 저장 → clock 알고리즘은 원형 버퍼, 즉 배열 및 각 배열의 1비트짜리 접근 비트를 사용함. 그리고 핸드 포인터는 그냥 배열의 인덱스임. 이게 압축됐다고 하는 이유는, LRU처럼 연결 리스트를 통해 이전 다음 노드 포인터를 덕지덕지 들고있지 않아도 되었기 때문임
- 몇 개의 명령만으로 원자적 조작 → 핸드 포인터의 접근 비트를 확인하는 것은 CPU가 하나의 명령으로 처리 가능. 접근 비트가 1일 때 0으로 바꾸고, 핸드를 다음 칸으로 이동하는 것 역시 원자적 명령으로 처리 가능. 하지만 LRU에서는 해당 노드의 이전 그리고 다음 포인터를 수정하고 맨 뒤 노드 포인터도 바꿔줘야 하는 등 여러 단계가 필요했음. 이를 위해서 락이 사용되었던 것
- 따라서 CLOCK을 쓰면 LRU와 다르게 동시성 환경에서 효율적인 것
- 하지만 접근 비트는 접근 여부만 따지기 때문에 접근 빈도를 고려해야 하는 경우에는 적합 X

### LFU

상황을 개선하기 위해, 페이지가 캐시에 들어온 시점(page-in events) 대신 페이지 참조(사용) 횟수(page reference events)를 추적하기 시작할 수 있습니다. 이를 가능하게 하는 한 가지 접근 방식은 가장 적게 사용된(Least-Frequently Used, LFU) 페이지를 추적하는 것입니다.

- → LRU와 CLOCK은 최근 사용 시점을 기준으로 제거를 결정했음. 하지만 빈도 수를 고려하지 않으므로 자주 사용됐더라도 제거될 수 있다는 문제가 있음. 이걸 개선하기 위해 빈도를 추적하는 알고리즘이 필요. 이게 LFU. LFU는 빈도 수가 높을수록 중요한 페이지로 간주하여 더 오래 유지함.

빈도 기반 페이지 제거 정책인 TinyLFU [EINZIGER17]는 정확히 이러한 방식으로 작동합니다: 페이지가 캐시에 들어온 시점의 최근성(recency)을 기반으로 페이지를 제거하는 대신, 사용 빈도에 따라 페이지의 순서를 정합니다. 이 정책은 인기 있는 Java 라이브러리인 Caffeine에 구현되어 있습니다.

TinyLFU는 전체 접근 기록을 보존하는 것이 실용적인 목적에서 너무 많은 비용을 초래할 수 있기 때문에, 간결한 캐시 접근 기록을 유지하기 위해 빈도 히스토그램 [CORMODE11]을 사용합니다.

요소들은 세 가지 큐 중 하나에 속할 수 있습니다:

- 새로 추가된 요소들을 유지하며 LRU 정책을 사용하는 Admission 큐.
- 가장 제거될 가능성이 높은 요소들을 보관하는 Probation 큐.
- 큐에 더 오래 남아 있을 요소들을 보관하는 Protected 큐.

→ 어드미션 큐는 새로 추가된 큐를 일시적으로 보관함. 얘는 LRU를 쓰는데, 처음 들어온 애는 실제로 빈도를 바로 파악하기 어렵기 떄문에 일단은 최신성 기준으로 초기 단계를 지켜보는 식. probation 큐는 second chance 느낌으로, 일단 기회를 한번 더 받는 페이지들이 있는 큐. protected 큐가 이제 진짜 자주 쓰는 페이지를 보관하는 큐

매번 어떤 요소를 제거할지 선택하는 대신, 이 접근 방식은 어떤 요소를 유지하도록 승격시킬지를 선택합니다. (승격의 결과로) 제거될 요소보다 빈도가 더 높은 요소들만이 Probation 큐로 이동할 수 있습니다. 이후 접근 시, 요소들은 Probation 큐에서 Protected 큐로 이동할 수 있습니다. Protected 큐가 가득 찬 경우, 해당 큐의 요소 중 하나가 다시 Probation 큐로 되돌려져야 할 수도 있습니다. 더 자주 접근되는 요소들은 유지될 가능성이 높고, 덜 사용되는 요소들은 제거될 가능성이 더 높습니다.

- 기존에는 어떤 페이지를 제거할 지 항상 선택하는 방식이었음. 하지만 TinyLFU는 어떤 페이지를 더 오래 유지하도록 승격시킬 것인지가 목적임. 즉 제거될 페이지를 고르기보다는 가치 있는 페이지를 고르자 라는 것.
- 일단 admission으로 받고, 여기서는 LRU로 처리하되 빈도 수를 보다가 일정 수준이 넘으면 probation으로 올림. 그리고 probation 큐에서 추가적으로 접근되어 빈도 수가 더 높아지는 경우 protected로 승급하여 보호됨.
- 하지만 protected 큐 역시 제한이 있기 떄문에 최대치에 도달하면 다시 내보내야 함. 여기서 덜 중요한 애들은 다시 강등되어 probation으로 내려감.

최적의 캐시 제거를 위해 사용할 수 있는 다른 많은 알고리즘들이 존재합니다. 페이지 교체 전략의 선택은 지연 시간(latency)과 수행되는 I/O 작업의 수에 중대한 영향을 미치므로, 반드시 신중히 고려해야 합니다.

- 더 많은 페이지 교체 알고리즘이 존재함. 어떤 전략을 선택하냐에 따라 레이턴시, IO 작업 수 둘에 영향을 줌. 가령 LRU은 적중률을 좋지만 스캔에 약함. CLOCK은 효율은 높지만 빈도 고려가 없음. 그래서 신중하게 선택해야 함. → 워크로드 특성을 고려하여 선택해야 한다
