# 고급 백엔드 스터디 11주차

B-트리의 변형들은 몇 가지 공통점을 가지고 있습니다: 트리 구조, 분할과 병합을 통한 균형 유지, 그리고 조회 및 삭제 알고리즘입니다. 동시성, 디스크 페이지 표현, 형제 노드 간 링크, 유지 관리 과정과 같은 다른 세부 사항은 구현마다 달라질 수 있습니다.

- 6장에서는 비트리 변형을 다룰 것. 하지만 비트리로서 가져야하는 자격 조건들이 있고, 여기에 대해서 어떠한 양상으로서 다르게 나타나는지 알 필요 있음 → 트리 구조, 분할 병합을 통한 밸런싱, 조회 및 삭제 알고리즘
- 하지만 동시성 제어, 저장포맷, 형제 링크, 메인터넌스 등은 구현마다 달라질 수 있음 → 이런 부분에서는 하드웨어 같은 물리적 차이나 (초점을 두는) 워크로드 특성에 따라서 다른 방식을 택할 수 있다

이 장에서는 효율적인 B-트리를 구현하거나 이를 활용하는 구조에 적용할 수 있는 여러 가지 기법을 살펴봅니다:

Copy-on-write B-트리는 기본적인 B-트리와 같은 구조이지만, 노드가 불변이며 제자리에서 수정되지 않습니다. 대신 페이지를 복사하여 수정한 뒤 새로운 위치에 기록합니다.

- B-Tree와 동일하지만 in-place update를 허용하지 않음. 즉 노드 불변성을 보장. 다른 비트리는 분할을 방지하기 위해 노드를 꽉 채우지 않고 약간의 공간 오버헤드를 감수한다는 내용을 다뤘었음. 그때 이걸 언급했었음
- 불변이기 때문에 동시성 문제로부터 어느 정도 자유로움. 읽을 때 락이 필요없다. 충돌이 발생하면 페이지 복사 후 수정하는 방식. 읽기 상황에서 좋지만 수정 시 복사가 필요하기에 공간 오버헤드 및 복사 과정에서 추가 비용 발생.

Lazy B-트리는 동일 노드에 대한 연속적인 쓰기로부터 발생하는 I/O 요청 수를 줄이기 위해 노드 갱신을 버퍼링합니다. 다음 장에서는 버퍼링을 한 단계 더 발전시켜 완전히 불변인 B-트리를 구현하는 두 구성요소 LSM 트리(“Two-component LSM Tree”)도 다룹니다.

- 쓰기 버퍼에서 동일 노드에 대한 쓰기를 배칭하여 플러시 시 한 번에 대량으로 순차 IO를 수행하는 방식. 7장에서 등장하는 LSM 트리를 쓰면 버퍼링을 극한까지 활용.

FD-트리는 LSM 트리(“LSM Trees”)와 유사하면서도 다른 방식으로 버퍼링을 적용합니다. FD-트리는 작은 B-트리에 갱신을 버퍼링하며, 이 트리가 가득 차면 그 내용을 불변 런(run)에 기록합니다. 갱신은 상위 수준에서 하위 수준으로 계단식으로 전파되며, 불변 런들 사이를 이동합니다.

- 작은 가변 B트리 (헤드) + 여러 개의 불변 런(테일)이라는 구조를 가짐
- 헤드가 꽉 차면 불변 런 배열에 덤프함
- 런 사이이 병합을 연쇄적으로 수행하여 정렬된 상태를 유지함.
- 쓰기 증폭 감소에 좋음

Bw-트리는 B-트리 노드를 여러 작은 부분으로 분리하여 추가 전용(append-only) 방식으로 기록합니다. 이는 서로 다른 노드에 대한 갱신을 묶어 배치함으로써 작은 쓰기 비용을 줄여 줍니다.

- 노드를 델타 체인으로 분할하여 append-only 로그에 기록함
- 델타를 메모리 매핑 테이블로 가리키고 + CAS로 원자적 연산을 통해 교체함으로서 락 없이 동시성 구현

Cache-oblivious B-트리는 디스크 상 데이터 구조를 메모리 내 구조를 구축하는 방식과 매우 유사하게 다룰 수 있도록 해 줍니다.

- 흠…

## Copy-on-Write

일부 데이터베이스는 복잡한 래칭 메커니즘을 구축하는 대신, 동시 작업 상황에서 데이터 무결성을 보장하기 위해 copy-on-write 기법을 사용합니다. 이 경우 페이지가 수정되려 할 때마다 해당 페이지의 내용을 복사하고, 원본 대신 복사된 페이지를 수정하며, 병렬적인 트리 계층 구조를 생성합니다. 이전 트리 버전은 작성자와 동시에 실행되는 읽기 작업에 여전히 접근 가능하지만, 수정된 페이지에 접근하는 작성자는 선행 쓰기 작업이 완료될 때까지 대기해야 합니다. 새로운 페이지 계층이 생성된 후 최상위 페이지에 대한 포인터가 원자적으로 갱신됩니다. 그림 6-1에서는 변경되지 않은 페이지를 재사용하면서 기존 트리와 병렬로 새 트리가 생성되는 모습을 볼 수 있습니다.

- 앞에서 래칭에 대해 알아봤었음. 읽기 / 쓰기 시 물리 레벨로 작동하는 락. 분할 병합 같은 상황에서 동시에 읽기 쓰기 요청이 들어오는 경우 등에서 무결성을 보장하기 위함이었음. 래치 크래빙 같은 방식을 알아봤었다
- 이러한 방식 대신 copy-on-write 방식을 선택할 수 있음. 이러면 복잡한 래칭에 대해서 고민할 필요가 없다. 어떻게? MVCC에서 했던 것과 비슷함.
  - 페이지가 수정 시 해당 페이지를 복사한다.
  - 그 복사본을 수정한다.
- 이러면 수정 중에 읽기를 시도하더라도, 원본 페이지에 접근해서 읽으면 되기 때문에 수정 중 상태를 읽어갈 걱정을 하지 않아도 됨. 즉 항상 일관된 상태를 읽음
- 단 수정하려는 페이지가 있다면 여기에 대하여 원본 페이지를 읽는 것만 가능하고, 또 수정하려는 요청이 있다면 기다려야 함. (쓰기-쓰기 충돌 방지)
- 수정을 통해 새 트리를 완성했다면 루트 포인터를 원자적으로 교체함. CAS 연산 등으로 중간 불일치 없이 변경. 하지만 기존 트리에 대하여 변경된 트리를 구성할 때 전부 복사를 뜬다면 매우 비효율적일 것임. 드라이브의 절반을 항상 남겨놔야 할 테니까. 당연히 수정이 발생하는 페이지만 복사해서 쓰고, 변경되지 않는 페이지는 자식 포인터를 통해 원본 페이지를 참조.

이 접근 방식의 명백한 단점은 더 많은 공간이 필요하고(비록 이전 버전은 그 페이지를 사용하는 동시 작업이 끝나면 즉시 회수될 수 있으므로 짧은 기간 동안만 유지되지만) 전체 페이지 내용을 복사해야 하므로 더 많은 프로세서 시간이 요구된다는 점입니다. 그러나 B-트리는 대체로 깊이가 얕기 때문에, 이러한 단점보다 이 방식의 단순성과 장점이 더 큰 경우가 많습니다.

- 단점도 있음. 당연히 복사하는 데 드는 공간 오버헤드 + 연산하는 데 쓰이는 CPU 오버헤드임. 여기서 공간 오버헤드의 경우 작업이 끝나면 기존 버전은 회수되므로 짧은 시간동안만 감수하면 됨
- 하지만 문제는 복사하는 데 걸리는 시간임. 하지만 이것도 괜찮음.
- B트리는 팬아웃이 큰 편이므로 높이를 낮게 유지할 수 있음. 즉 얕은 깊이를 가짐. 이게 왜…?
  - 특정 리프 페이지에 있는 데이터를 수정해야 한다고 하자. 그러면 일단 리프 페이지를 복사해서 수정사항을 적용할 것임. 그리고 새로운 루트 페이지를 만들고, 기존 수정하지 않는 페이지는 원본 페이지를 가리키게 하고, … 이걸 연쇄적으로 적용할 것임. 그러면 당연히 새로운 버전의 트리를 만들기 위해서는 루트 페이지부터 수정된 페이지까지에 있는 포인터들을 모두 바꿔줘야 함. 그러면 그 사이 페이지들도 복사해야 함.
  - 높이가 높다면 경로가 기므로 복사해야 하는 페이지도 많을 것. 하지만 보통 B-Tree는 3-4 깊이면 충분히 커버 가능. 하나의 페이지가 4KB라고 하면 기껏해야 12KB에 불과함. 즉 많이 복사하는게 아니기 때문에 그렇게 손해가 아니라는 것. → 쓰기 증폭을 최소화할 수 있다

이 방식의 최대 장점은 읽기 작업이 동기화를 요구하지 않는다는 것입니다. 기록된 페이지는 불변이므로 추가 래칭 없이도 접근할 수 있습니다. 쓰기는 복사된 페이지에서 이루어지므로, 읽기 작업이 쓰기 작업을 차단하지 않습니다. 어떤 연산도 완료되지 않은 상태의 페이지를 관찰하지 않으며, 모든 페이지 수정이 끝난 뒤에만 최상위 포인터가 전환되므로 시스템 크래시가 페이지를 손상된 상태로 남겨둘 수 없습니다.

- 장점을 보자. 페이지는 항상 불변이기 때문에 (쓰기가 원자적이라면) 읽는 입장에서는 내가 읽는 내용이 지금 시점에서는 바뀔 거라는 의심 없이 읽을 수 있음. 즉 래칭을 하지 않고 마음대로 읽을 수 있음.
- 왜? 쓰기는 복사된 페이지에서만 수행되기 때문임. 그래서 쓰기가 완료될 때까지 대상 리소스에 락이 걸려서 대기해야 하는 상황이 없음. 마치 MVCC 에서 그랬던 것처럼… 당연히 쓰기 연산의 경우 원자적으로 수행되며, 쓰기가 완전히 끝난 이후에만 새로운 트리 버전의 루트 페이지를 서로 스왑하기 때문에, 중간 상태의 커밋되지 않은 상태를 볼 수가 없음. → “모든 페이지 수정이 끝난 뒤에만 최상위 포인터가 전환되므로 시스템 크래시가 페이지를 손상된 상태로 남겨둘 수 없습니다”

### Implementing Copy-on-Write: LMDB

Copy-on-write를 사용하는 스토리지 엔진 가운데 하나는 Lightning Memory-Mapped Database(LMDB)로, OpenLDAP 프로젝트에서 사용하는 키-값 저장소입니다. 설계와 아키텍처 덕분에 LMDB는 페이지 캐시, 선행 기록 로그, 체크포인팅, 또는 컴팩션을 필요로 하지 않습니다.

- LMDB → 키-값 DB. copy-on-write를 사용. B+ 트리 구조 기반이며, 전체 파일을 메모리 위에 올려놓고 사용함(Memory-Mapped).
- copy-on-write를 쓰면 변경사항을 항상 새로운 위치에 쓰기 때문에 WAL을 두지 않아도 됨. 즉 기존 내용이 그대로 남아있으므로 로그에 먼저 쓰지 않아도 됨.
  - 왜? in-place update 방식에서는 중간에 크래시가 발생하면 해당 페이지가 손상됨. 그래서 wal을 통해서 페이지를 수정하기 전에 변경사항을 로그에 기록하고 실제로 수정을 수행함. 크래시 발생하면 wal을 replay하거나(redo) 롤백(undo)하여 일관된 상태를 유지.
  - 하지만 copy-on-write는 변경사항을 항상 복사된 페이지에 씀. 그래서 원본 페이지가 그대로 살아있음. 따라서 일관된 상태를 유지할 수 있음
  - 여기서 궁금증 → undo는 그렇다 쳐도 redo는 안되지 않나? cow는 기존 페이지만 기록하고 있으니까… 그런데 이게 위에서 말한 CAS 연산을 통한 루트 포인터랑 관련이 있었던 거였음. 우리가 redo를 통해 wal을 replay해야 하는 상황은 커밋 찍었는데 디스크에 변경사항이 반영 안된 경우임. 하지만 LMDB는 변경사항을 새로운 사본 트리에 기록함. 이는 항상 새로운 루트 페이지를 가짐. 그리고 변경이 끝나면 이미 변경사항은 디스크에 확정된 상황. << 즉 in-place와 다르게 이미 디스크에 변경사항이 확정된 상태에서 루트를 스왑하고 커밋하는 것. 앞에서 얘기했으니 스킵

LMDB는 단일 계층 데이터 저장소로 구현되어 있어, 읽기와 쓰기 연산이 중간의 애플리케이션 수준 캐싱 없이 메모리 매핑을 통해 직접 처리됩니다. 이는 또한 페이지를 추가로 실체화(materialize)할 필요가 없으며, 데이터를 중간 버퍼로 복사하지 않고도 메모리 매핑에서 직접 읽기를 제공할 수 있음을 의미합니다. 업데이트하는 동안에는 루트부터 대상 리프까지의 경로에 있는 모든 분기 노드가 복사되어 잠재적으로 수정됩니다: 업데이트가 전파되는 노드는 변경되고, 나머지 노드는 그대로 유지됩니다.

- “LMDB는 단일 계층 데이터 저장소로 구현되어 있어, 읽기와 쓰기 연산이 중간의 애플리케이션 수준 캐싱 없이 메모리 매핑을 통해 직접 처리됩니다.”
  - “단일 계층 데이터 저장소로 구현되어 있어” → 보통 여러 계층을 씀. 디스크에서 바로 메모리로 올라가는 게 아니라 항상 페이지 캐시를 통해서 접근하게 되어있었음. 이것도 앞에서 다뤘었다 (항상 페이지 캐시에 먼저 접근하고, 페이지 캐시는 히트했다면 바로 주고 아니라면 캐싱한 후 주기)
  - “애플리케이션 수준 캐싱 없이” → 앞에서 배웠던 페이지 캐시(혹은 버퍼 풀) 말하는 것.
  - “메모리 매핑을 통해 직접 처리” → 이건 운영체제 관련 내용인데… 앞에서 O_DIRECT 이야기 하면서 다뤘었음.
    - “I/O를 수행할 때 시스템 호출을 피하기 위해 메모리 매핑(memory mapping)을 사용할 수 있지만, 그렇게 하면 캐싱에 대한 제어를 잃게 됩니다.”
    - 메모리 매핑 (mmap) → 파일을 디스크에서 읽는 대신(보통 read()를 통해서 디스크에서 읽은 후 메모리 버퍼로 가져오는 식임), 파일 내용의 ‘전체’ 혹은 ‘일부’를 애플리케이션의 가상 주소 공간에 직접 매핑함. 매핑이 완료되면 read(), write() 호출하는 대신 메모리에 접근하듯이 포인터 연산으로 접근. 만약 첫 접근으로 인해 아직 로드되지 않은 부분이라면 페이지 폴트 발생. 이후 커널이 자동으로 데이터를 가져옴. 이는 여전히 DMA 거치므로 CPU 이슈 없음
      - 쓰는 입장에서는 메모리에서 읽고 쓰듯이 접근하지만, 내부적으로는 OS가 폴트 처리를 해줌
    - 하지만 이 방식을 쓰면 페이지 폴트에 의해서만 데이터가 올라가고 내려감. 그래서 애플리케이션이 페이지 캐시 제어하기 어려워짐. 이때 커널은 페이지 교체 알고리즘에 따라 페이지를 제어. 가령 LRU, FIFO, … 같은 것들
- “이는 또한 페이지를 추가로 실체화(materialize)할 필요가 없으며, 데이터를 중간 버퍼로 복사하지 않고도 메모리 매핑에서 직접 읽기를 제공할 수 있음을 의미합니다.”
  - materialize → 메모리 상에서 사용 가능한 형태의 데이터 복사본 객체를 만드는 것. 즉 read()를 써서 구조체(c)/객체(java)로 만들어버리는 것. 해당 데이터 크기 만큼의 공간을 확보하고, 디스크로부터 실제 데이터를 복사해온다.
  - “그 어떤 추가적인 materialize가 필요없다("This also means that pages require no additional materialization…”)
    - 원래 우리는 디스크로부터 데이터를 읽을 때 항상 페이지 버퍼를 거침. 대신 커널 페이지 버퍼를 이중으로 쓰지 않기 위해서 O_DIRECT 플래그를 썼었음. 대신 이는 애플리케이션 레이어 단에서의 구현임. 애플리케이션에서 구현하려면? 당연히 ‘페이지’ 라는 대상을 객체로 만들어야 함. c의 구조체로 나타내든… java 클래스로 나타내든…
    - 하지만 디비 페이지 버퍼 대신 커널 페이지 버퍼를 쓰게 되면? 이러한 페이지의 ‘객체 구현’이 필요없어짐. 그렇다면 페이지를 read()해서 메모리 상의 객체 표현으로 나타낼 필요가 없음. 그래서 materialize가 필요없다고 하는 것.
  - “데이터를 중간 버퍼로 복사하지 않고도 메모리 매핑에서 직접 읽기를 제공할 수 있음을 의미” → 즉 DB 페이지 캐시로 malloc(), read()를 통해 복사하는 과정 없이도 가상 메모리의 페이지 테이블 상에서 값을 바로 읽어올 수 있다는 것.
- “업데이트하는 동안에는 루트부터 대상 리프까지의 경로에 있는 모든 분기 노드가 복사되어 잠재적으로 수정됩니다: 업데이트가 전파되는 노드는 변경되고, 나머지 노드는 그대로 유지됩니다.”
  - COW에 대한 내용…

LMDB는 루트 노드의 버전을 두 개만 유지합니다. 하나는 최신 버전이고, 다른 하나는 새로운 변경 사항이 커밋될 예정인 버전입니다. 모든 쓰기 작업이 루트 노드를 통해 진행되므로 이렇게 둘만으로 충분합니다. 새 루트가 만들어지면 이전 루트는 새로운 읽기 및 쓰기에 더 이상 사용되지 않습니다. 이전 루트에 연결된 읽기 작업이 모두 끝나면 그 페이지들은 회수되어 재사용될 수 있습니다. LMDB는 append-only 설계이므로 형제 노드 포인터(sibling pointer)를 두지 않으며, 순차 스캔 시에는 부모 노드로 거슬러 올라가야 합니다.

- LMDB는 COW를 사용하여 MVCC를 구현함. 사실 앞 내용 공부했다면 둘이 꽤 연관성이 깊다는 걸 인지했을 거임. COW 설명하면서 MVCC 얘기를 하기도 했고… 수정한다면 기존 건 냅두고 복사해서 새로운 버전을 수정한다는 아이디어가 비슷함.
- “LMDB는 루트 노드의 버전을 두 개만 유지합니다. 하나는 최신 버전이고, 다른 하나는 새로운 변경 사항이 커밋될 예정인 버전입니다.”
  - → “MVCC는 커밋된 버전과 커밋되지 않은 버전을 구분하며, 이는 각각 커밋된 트랜잭션과 커밋되지 않은 트랜잭션의 값 버전에 대응합니다. 값의 마지막으로 커밋된 버전이 현재 버전으로 간주됩니다. 일반적으로 이 경우 트랜잭션 관리자의 목표는 동시에 최대 하나의 미커밋 값을 유지하는 것입니다.” 전 주차 내용.
  - 어쨌든 LMDB는 커밋된 가장 최신 값 버전에 해당하는 비트리 루트 노드와 현재 수정 중인 미커밋 버전에 해당하는 비트리 루트 노드를 유지함.
- “모든 쓰기 작업이 루트 노드를 통해 진행되므로 두 버전이면 충분합니다.”
  - 당연히 B트리니까… 루트를 통해서 탐색하고 갱신을 수행함. 그렇다면 기존 값에 대한 트리로 접근할 수 있는 루트 노드와 수정이 진행중인 트리에 접근할 수 있는 루트 2개로 충분하다는 거임.
- “새 루트가 만들어지면 이전 루트는 새로운 읽기 및 쓰기에 더 이상 사용되지 않습니다.” → 쓰기를 모두 완료하고 커밋되어 루트 포인터가 스왑되면, 당연히 이 버전의 루트가 읽기 및 쓰기의 원본으로 사용됨.
- “이전 루트에 연결된 읽기 작업이 모두 끝나면 그 페이지들은 회수되어 재사용될 수 있습니다.” → 이렇게 루트 스왑이 발생하더라도 스왑 이전의 트랜잭션은 해당 스냅샷에 접근할 수 있어야 함(COW로 MVCC를 구현한다고 했었음). 즉 스왑이 발생하기 전에 읽었으므로 자신의 트랜잭션 중간에 실제로 해당 데이터가 변경되어 커밋되고 루트 포인터까지 스왑되었다 하더라도 이러한 사실을 전혀 모르는 것처럼 (앞에서 이걸 고립성, isolation이라 했었음) 처리할 수 있어야 함. 그러려면? 루트 스왑이 발생한다고 해서 구버전 페이지를 전부 invalidate하지 않고, 이 페이지를 참조하고 있는 트랜잭션이 끝났는지 확인한 후에야 회수해야 함.
  - 강의에서 freelist + reader table 관련 및 페이지 할당 판정 로직 관련하여 설명할 내용들…
  - freelist는 그냥 트랜잭션 커밋 시점에 이 트랜잭션 ID가 해제하는 페이지 리스트임
  - 실제로는 해당 페이지를 읽고 있는 다른 트랜잭션 들이 있을 수 있기 때문에 readers table을 유지한다는 것. 만약 읽기가 발생하면 현재 트랜잭션 ID를 기록함. 이는 현재 읽기를 수행중인 트랜잭션 ID를 알 수 있습니다.
  - 이제 쓰기 트랜잭션에서 새로운 페이지가 필요해진다고 하자. 이때 freelist에서 오래된 키부터 (더 이상 읽지 않는 페이지가 있을 가능성이 높으므로) 탐색하고, 키에 해당하는 txid가 readers table의 최소 txid, 즉 가장 오래된 트랜잭션보다 작다면 freelist의 키 트랜잭션의 페이지 ID를 아무도 참조하고 있지 않을 것이므로 사용할 수 있는 것이라고 판정함
- “LMDB는 append-only 설계이므로 형제 노드 포인터(sibling pointer)를 두지 않으며, 순차 스캔 시에는 부모 노드로 거슬러 올라가야 합니다.”
  - 일단 append-only여서 형제 포인터가 없다는 내용 → 형제 포인터라는 건 오른쪽에 새로운 리프 노드가 생기면, 그 왼쪽 리프 노드의 형제 포인터를 이 노드로 업데이트해줘야 한다는 의미임. 그런데 COW는 페이지의 불변성을 보장함. 따라서 이미 기록된 페이지를 수정할 수 없으므로 COW를 쓰면 형제 포인터를 두는 게 불가능함. 따라서 형제 포인터를 통해서 범위 탐색 등에서 리프 페이지끼리 왔다갔다 하는 게 불가능하고, 부모로 올라가서 이동하는 식으로 탐색해야 함
  - 근데 이해가 안되는 부분… “Because of LMDB’s append-only design, it does not use sibling pointers and has to ascend back to the parent node during
    sequential scans.”
  - append-only가 아니라 COW 아닌지? append-only와 COW는 다르다. 이는 우리가 배웠던 챕터 2의 3가지 중요 개념 중 불변성 관련해서 등장했었음.
    > **불변성**
    >
    > 이는 스토리지 구조가 파일의 일부를 읽고, 이를 업데이트한 뒤 **같은 위치에 덮어쓸 수 있는지 여부**를 정의합니다. **불변 구조(immutable structure)**는 append-only 구조입니다. 즉, 일단 쓰여진 파일 내용은 수정되지 않으며, 변경 사항은 파일의 끝에 **추가적으로 기록됩니다**. 불변성을 구현하는 방법에는 여러 가지가 있으며, 그중 하나는 **복사 후 쓰기(copy-on-write)**입니다(“Copy-on-Write” 참고). 이 방식은 레코드의 업데이트된 버전을 포함한 페이지를 **원래 위치가 아닌 새로운 위치에 기록**하는 방식입니다. 일반적으로 **LSM 트리와 B-트리의 차이점은 불변 구조와 제자리 업데이트 구조의 차이**로 설명되지만, B-트리에서 영감을 받은 구조 중에서도 **불변성을 가진 예(Bw-Tree 등)**도 존재합니다.
  - 즉 불변 구조는 append-only, 변경사항을 파일 끝에 추가하는 방식을 취하는 구조임. 한편 COW는 append-only와 명확하게 구현되고 있으며 불변성을 구현하는 방식으로 설명되고 있음. 앞에서 freelist 등을 통한 설명에 대해 알아봤었음. 여기서도 페이지가 재사용 가능하게 되면 회수해서 사용한다고 했었음. 만약 이게 append-only이려면 모든 변경사항이 반영된 후에 (즉 reader table이 모두 비워진 다음에) freelist로 페이지를 할당할 수 있게 되어야 하는데, LMDB는 그렇게 하지 않고 현재 페이지 요청에 대하여 가장 오래된 트랜잭션이 잡고 있는 페이지가 현재 읽어질 가능성이 있는지(freelist key txid < min_reader txid) 검사하기만 함.
- 그래도 궁금해서 찾아봤는데, 마침 LMDB 제작자인 하워드 추가 해커뉴스에 직접 코멘트한 부분이 있어서 갖고왔다.

> hyc_symas (Howard Chu, LMDB 설계자) — 2015-02-01
>
> LMDB는 **COW 트리**이지만 **append-only 트리**는 결코 아닙니다. append-only는 앞선 블로그 글에서 이미 지적된 여러 이유 때문에 형편없습니다.
>
> LMDB에서 **쓰기 증폭(write amplification)** 은 트리의 높이에 의해 상한이 정해지므로 `O(log N)`(N=레코드 수)입니다. 반면 모든 WAL 기반 설계의 쓰기 증폭은 `O(N)`(N=레코드 크기)입니다. (역주: 이 부분은 위에서 언급했던 부분이기도 함)
>
> 페이지 크기가 4 KB라면 LMDB에서 쓰기 증폭이 64 KB까지 커지려면 트리 깊이가 16이어야 합니다. 레코드 크기 100 B, 페이지당 40 레코드라고 하면 40¹⁶, 즉 4.29×10²⁵ 개의 레코드가 필요합니다. 당신이 제시한 64 KB 수치는 터무니없이 과대평가된 것입니다.
>
> 다양한 DB 엔진의 쓰기 증폭을 더 자세히 조사한 자료가 여기 있습니다 → [http://symas.com/mdb/ondisk/](http://symas.com/mdb/ondisk/)
>
> 이 자료를 보면 레코드 크기가 2 KB를 넘어갈 때부터 LMDB가 WAL 기반 엔진보다 더 낮은 쓰기 증폭을 보입니다. (LDAP 엔트리는 대체로 2 KB 이상입니다.)
>
> **가장 중요한 점**—실시간 서비스에서 LMDB의 **쓰기 지연(latency)** 은 매우 좁게(예측 가능하게) 묶여 있으며, stop-the-world식 컴팩션 일시 중지가 전혀 없습니다. 이것은 설계 요구 사항이었습니다(2011 LMDB 설계 논문 3.2절 참조). 예를 들어 LMDB와 HyperLevelDB의 쓰기 지연 비교: [http://symas.com/mdb/hyperdex/#100M](http://symas.com/mdb/hyperdex/#100M)
>
> 우리는 LMDB I/O 지연을 신뢰할 수 있게 계산·예측할 수 있으며, 예측값이 실측과 완전히 일치합니다—평균 33 ms, 평균 시크 16 ms인 디스크에서요. 다른 DB 엔진으로는 이런 예측이 불가능합니다.

> slashdev — 2015-02-01
>
> 용어를 헷갈렸네요. 제 기억으로 LMDB는 append-only 트리와 비슷하지만, **사용하지 않게 된 페이지를 freelist(역시 LMDB 트리?)에 넣어둔 뒤 파일 끝에 도달하면 다시 재사용**하는 걸로 압니다. 제 수치는 100 B 레코드 기준으로 터무니없이 틀렸네요. …

> hyc_symas — 2015-02-01
>
> “파일 끝에 도달하면 재사용한다”
>
> 그 부분이 틀렸습니다. **페이지는 안전해지는 즉시 재사용**되지, 파일 끝에 도달할 때까지 기다리지 않습니다.

[https://news.ycombinator.com/item?id=8979517](https://news.ycombinator.com/item?id=8979517)

궁금하면 읽어볼 것. 다시 다음 문단으로 이어서…

---

이 설계에서는 복사된 노드에 오래된 데이터를 그대로 두는 것은 실용적이지 않습니다: MVCC에 사용되어 진행 중인 읽기 트랜잭션을 만족시킬 수 있는 사본이 이미 존재하기 때문입니다. 데이터베이스 구조가 본질적으로 다중 버전을 제공하므로, 독자들은 작성자와 전혀 충돌하지 않아 어떠한 락도 없이 실행될 수 있습니다.

- 앞 내용에 따르면 루트는 2개가 존재하면 이미 커밋된 데이터를 읽을 수 있는 루트 노드와, 수정이 진행중인 루트 노드.
- 여기서 수정 중인 버전은 기존 값을 마음대로 덮어써도 됨. 왜? 기존 값은 이전 루트 노드에서 잘 유지되고 있기 때문임. 읽기 트랜잭션은 커밋된 데이터가 존재하는 버전에서 데이터를 읽기 때문에 현재 수정 중인 동작으로 인해서 충돌이 발생할 걱정을 안해도 됨 (R-W 충돌). 따라서 락도 필요 없음.
- 왜 이 문단이 있는지 솔직히 잘 모르겠음…

## Abstracting Node Updates

디스크의 페이지를 업데이트하려면, 어떤 방식이든 우선 그 페이지의 메모리 상 표현을 업데이트해야 합니다. 그러나 메모리에서 노드를 표현하는 방법에는 몇 가지가 있습니다: 노드의 캐시된 버전에 직접 접근하거나, 래퍼 객체를 통해 접근하거나, 또는 구현 언어에 고유한 메모리 내 표현을 생성하는 방법입니다.

- 디스크 페이지를 수정하려면 먼저 메모리 상 노드를 수정해야 함. → “우리는 디스크로부터 데이터를 읽을 때 항상 DB 페이지 버퍼를 거침.” 이라고 했었음.
- 그러려면 이 노드를 참조하기 위해서 메모리에서 어떤 방식으로든 ‘표현’해야 함. 세 가지 방법이 있음
  - 캐시 페이지 직접 접근
  - 래퍼 객체 경유
  - 언어별로 고유한 인메모리 표현 방식 사용

관리되지 않는 메모리 모델을 사용하는 언어에서는 B-Tree 노드에 저장된 원시 바이너리 데이터를 재해석하여 네이티브 포인터로 조작할 수 있습니다. 이 경우 노드는 포인터 뒤의 원시 바이너리 데이터를 구조체로 정의하고, 런타임 캐스트를 사용해 접근합니다. 대부분 이러한 포인터는 페이지 캐시가 관리하는 메모리 영역을 가리키거나 메모리 매핑을 사용합니다.

- unmanaged memory model을 사용하는 언어 → C, C++ 같이 메모리 할당 해제를 직접 신경써줘야 하는 언어. mallo, free 같이 명시적으로 할당/해제. 반대로 managed 언어로는 Java가 있음. 언어 단에서 메모리를 관리해줌. 그냥 new로 객체를 힙에 객체를 생성하는 방식. 그리고 직접 해제 못하니까, GC가 돌면서 알아서 참조되지 않을 때 해제한다고 배웠었음
- 디스크에서 페이지를 읽어온다고 하자. 그러면 이 페이지는 그냥 순수한 바이트 덩어리일 것임. 하지만 우리가 저장할 때 특정한 규칙에 따라 저장했다면 그에 따라서 이 바이트 덩어리를 해석할 수 있음. 책에서 말하는 원시 바이너리 데이터가 이 바이트 덩어리임.
- 가령 우리는 3장 파일 포맷 챕터에서 슬롯 페이지라는 개념을 배웠었음. 페이지 헤더와 셀 오프셋 배열, (빈 공간과) 뒤에서부터 채워지는 셀 데이터 배열 이렇게 셋으로 구성되어 있었음. 그리고 책에서 이 슬롯 페이지의 대표적인 예시로 PostgreSQL을 소개하고있음. 마침 PostgreSQL도 C이므로 이를 기반으로 설명해보면 좋을 것.
- 그 전에 나머지 부분만 보고 가자.
  - “런타임 캐스트” → 그냥 바이트 덩어리를 가리키는 포인터를, 특정 구조체에 따라서 해석할 수 있도록 런타임에 캐스팅하겠다는 뜻. 즉 ‘의미없는 바이트 덩어리’를 ‘페이지 포맷에 따른 구조체’로 해석하겠다 라는 의미. 아래에서 더 설명.
  - “대부분 이러한 포인터는 페이지 캐시가 관리하는 메모리 영역을 가리키거나 메모리 매핑을 사용합니다.” → 즉 DB 페이지 버퍼로도 가져올 수 있지만 메모리 매핑, 즉 OS 페이지 버퍼로 가져올 수도 있다는 뜻. 어쨌든 둘 다 바이트 덩어리인 것은 동일함.
- PostgreSQL에서는 슬롯 페이지를 구현하기 위해 페이지 포맷을 아래와 같이 정의하고 있음
  - 페이지 헤더(PageHeaderData): 페이지의 전반적인 관리 정보를 담음
  - 오프셋 배열(ItemIdData[]): 책의 '셀 오프셋 배열'에 해당하며, 실제 데이터의 위치를 가리킴
  - 실제 데이터(Tuples): 책의 '셀 데이터'에 해당하며, 행(Row) 데이터가 저장
  - 빈 공간(Free Space): `ItemIdData` 배열의 끝과 `Tuple` 데이터의 시작 사이의 공간

```java
+----------------------------------------------------------------------------------------------------+
|  페이지 헤더 (PageHeaderData)  |  오프셋 배열 (ItemIdData[]) ->  | ...빈 공간... |  <- 실제 데이터 (Tuples)  |
+----------------------------------------------------------------------------------------------------+
^                             ^                              ^                                       ^
페이지 시작                  pd_lower                       pd_upper                              페이지 끝
```

```c
// 페이지의 메타데이터를 담는 헤더 구조체
typedef struct PageHeaderData {
    uint16_t pd_lower;      // 오프셋 배열의 끝 위치
    uint16_t pd_upper;      // 실제 데이터(튜플)의 시작 위치
    // 페이지 헤더는 고정 길이이므로 오프셋 배열 시작 위치 알 수 있음
    // 실제 데이터의 경우 맨 끝에서 왼쪽으로 증가하므로 튜플의 시작 위치 관리
} PageHeaderData;

// 각 데이터의 위치와 길이를 담는 오프셋 배열의 항목 구조체
typedef struct ItemIdData {
    uint16_t lp_off;        // 데이터의 시작 위치 (오프셋)
    uint16_t lp_len;        // 데이터의 길이 -> 즉 (시작 위치, 시작 위치 + 길이)만큼 읽으면 됨
} ItemIdData;

// 8KB 페이지가 메모리에 로드된 상태라고 가정합니다.
unsigned char* page_buffer = get_page_from_disk(page_id);

// --- 1단계: 헤더 정보 읽기 ---
// page_buffer의 시작 부분을 PageHeaderData 구조체로 '재해석'합니다. (런타임 캐스팅...!)
PageHeaderData* header = (PageHeaderData*)page_buffer;

// 헤더 정보를 통해 오프셋 배열이 어디까지인지 확인합니다.
printf("오프셋 배열 끝 위치: %u\n", header->pd_lower);

// --- 2단계: 오프셋 정보 읽기 ---
// 헤더 바로 다음 위치를 ItemIdData 배열로 '재해석'합니다.
// 첫 번째 데이터의 정보를 가져와 보겠습니다.
ItemIdData* item_array = (ItemIdData*)(page_buffer + sizeof(PageHeaderData));
ItemIdData* first_item_id = &item_array[0]; // 배열의 첫 번째 항목

// 첫 번째 데이터의 위치(오프셋)와 길이를 읽습니다.
uint16_t tuple_offset = first_item_id->lp_off;
uint16_t tuple_length = first_item_id->lp_len;
printf("첫 번째 데이터 위치: %u, 길이: %u\n", tuple_offset, tuple_length);

// --- 3단계: 실제 데이터(튜플) 접근 ---
// 2단계에서 얻은 오프셋을 이용해 실제 데이터의 시작 주소를 계산합니다.
unsigned char* tuple = page_buffer + tuple_offset;

// 이제 'tuple' 포인터를 사용해 'tuple_length' 만큼 데이터를 읽거나 쓸 수 있습니다.
printf("첫 번째 데이터 내용: ");
for (int i = 0; i < tuple_length; i++) {
    printf("%02x ", tuple[i]); // 16진수로 내용 출력
}
printf("\n");
```

```c
/* C 코드의 포인터 변수들이 page_buffer라는 하나의 메모리 덩어리를 어떻게 포인터 구조체로 해석하는지에 대한 예시

// [ page_buffer (8KB 크기의 메모리 공간) ]
+======================================================================================+
| [ PageHeaderData ] [ ItemIdData[] ] [           ... 빈 공간 ...        ] [   Tuple   ] |
+--------------------------------------------------------------------------------------+
^                    ^                                                  ^              ^
주소: 0            주소: 24 (Header 크기)                              주소: N      주소: 8191

*/

/* 1. 헤더 접근 코드 */
PageHeaderData* header = (PageHeaderData*)page_buffer;

/*
   header --> +------------------+
              | [ PageHeaderData ] ...
              +------------------+
              (메모리 버퍼의 시작 부분을 '헤더'라는 구조체로 해석)
*/

/* 2. 오프셋 배열 접근 코드 */
ItemIdData* item_array = (ItemIdData*)(page_buffer + sizeof(PageHeaderData));

/*
              +------------------+
   item_array --> | [ ItemIdData[] ] ...
              +------------------+
              (헤더 바로 다음 부분을 '오프셋 배열'이라는 구조체으로 해석)
*/

/* 3. 실제 데이터(튜플) 접근 코드 */
// 먼저 2번에서 구한 item_array에서 오프셋 값(N)을 읽습니다.
uint16_t tuple_offset = item_array[0].lp_off; // 예: N

// 그 오프셋을 이용해 실제 데이터의 주소를 계산합니다.
unsigned char* tuple = page_buffer + tuple_offset;

/*
                                                  +-----------+
   tuple -----------------------------------------> | [ Tuple ] |
                                                  +-----------+
                                                  (오프셋 N 위치를 '실제 데이터'라는 구조체으로 해석)
*/
```

또는 B-Tree 노드를 언어에 고유한 객체나 구조체로 실체화(materialize)할 수도 있습니다. 이렇게 생성된 구조체는 삽입, 갱신, 삭제 작업에 사용할 수 있습니다. 플러시 시점에는 변경 사항이 메모리의 페이지에 적용된 뒤, 이어서 디스크에 반영됩니다. 이 방법은 기본 원시 페이지에 대한 변경과 중간 객체에 대한 접근을 분리하므로 동시 접근을 단순화한다는 장점이 있지만, 동일한 페이지의 두 가지 버전(원시 바이너리와 언어 고유 형식)을 메모리에 동시에 보관해야 하므로 메모리 오버헤드가 커집니다.

- 앞에서는 우리가 페이지 포맷에 따라서 바이트 덩어리를 PageHeaderData 라는 구조체로 해석하기 위해 런타임 캐스팅이라는 걸 하고, 이제 우리가 원하는 데이터 위치에 접근할 수 있게 되었음.
- 따라서 `ItemIdData[]` 의 시작 위치 정보를 `PageHeaderData` 로부터 알아와서, ‘두 번째 셀’에 접근하겠다 라고 하면 `ItemIddata[1]` 과 같이 접근하여 이를 알아내고, 다시 실제 셀이 있는 곳으로 접근할 수 있게 됨. 이게 위에서 봤던 오프셋 N임. N부터 읽으면 실제 데이터가 있는 것.
- 하지만 이건 여전히 ‘작은 바이트 덩어리’임. 이걸 한번 더 해석하고 싶을 수 있음. 가령 객체로 다룬다면 좋을 것임. 하지만 자바 객체 정보는 힙에 존재한다고 했었음. 자바는 managed 언어이기 때문. (1번이 unmanged 언어에서 쓰는 방식이었다는 걸 기억하자…!). managed 언어는 커널 페이지 버퍼에 직접 접근할 수 없기에 데이터를 힙으로 복사해와서 써야 함.
- 이렇게 하면 자바 언어 단에서의 변경사항들을 객체, 즉 힙에 먼저 적용하고, 이를 플러시하여 메모리 페이지에 적용한 뒤 디스크로 반영되게 할 수 있음. 이러면 커널 페이지 캐시에 변경사항을 적용하지 않고 언어가 관리하는 영역에서 변경사항을 적용하므로, 변경 중간 상태의 커널 페이지 캐시를 공유해야 하는 경우에서 일관성이 깨지거나 / 이를 지키기 위해서 동시성을 희생해야 하는 상황을 막을 수 있음. 즉 동시 접근이 최적화됨. 하지만 같은 데이터를 커널 페이지 버퍼와 JVM 힙 영역에 이중으로 두므로 메모리 오버헤드가 발생한다는 단점.

세 번째 접근법은, B-Tree에서 변경이 이루어지는 즉시 해당 버퍼에 변경 사항을 반영하는 래퍼 객체(wrapper object)를 통해, 노드의 기반이 되는 버퍼(buffer backing the node)에 접근하는 방식입니다. 이 방법은 주로 관리형 메모리 모델(managed memory model)을 사용하는 언어에서 쓰입니다. 래퍼 객체는 변경 사항을 백킹 버퍼(backing buffers)에 적용합니다.

- 1번과 2번 둘 다 장단이 있음. 이걸 혼합해서 쓰는 방식. unmanaged 처럼 포인터를 통해 로우한 데이터, 즉 바이트 덩어리에 위험하게 바로 접근하는 걸 막고는 싶지만, 그렇다고 managed 처럼 데이터를 복사해와서 이중으로 두는 건 막고 싶다. 그러면 복사는 안하고 직접 커널 페이지에 접근하되 안전하게 객체로 접근하고 싶다는 것. 이걸 가능케 하는 것이 래퍼 객체.
- 가령 자바의 경우 IO가 느리다는 게 항상 단점이었음. 2번에서 설명한 것만 보더라도 알 수 있을 것. 그래서 Java NIO를 통해서 이런 느린 IO를 개선했음. NIO의 핵심이 뭐냐? 앞에서 말했듯 JVM 힙이 아니라 네이티브 메모리에 직접 매핑하겠다라는 것임. 그리고 이를 ByteBuffer 인터페이스를 통해 안전하게 접근하겠다 라는 뜻.
- 여기서 구현체로는 DirectByteBuffer와 MappedByteBuffer가 있는데 전자는 그냥 JVM 바깥 네이티브 메모리이고 후자는 뭔가 익숙한 워딩이 등장한다… 바로 메모리 매핑, 즉 mmap을 통해서 가져온 커널 페이지 버퍼를 말하는 것. 만약 Java로 구현되었으며 메모리 매핑을 구현한다면 해당 바이트 버퍼를 사용할 것. Apache Cassandra가 자바 기반에 mmap을 사용하는 것으로 알려져 있음.
- 아무튼 이렇게 하면 힙 영역에서의 페이지 표현을 쌓아놨다가 메모리 단의 커널 페이지 버퍼를 통해 디스크로 플러시되는 게 아니라, 우리가 수정하는 것이 바로 버퍼이므로 바로 플러시가 가능함.

디스크에 있는 페이지, 그 페이지의 캐시된 버전, 그리고 메모리 내 표현을 별도로 관리하면 서로 다른 수명 주기를 가질 수 있습니다. 예를 들어, 삽입·갱신·삭제 작업을 버퍼링해 두었다가, 읽기 시에 메모리에서 이루어진 변경을 원본 디스크 버전과 조정할 수 있습니다.

- 뭐든 간에 2번, 3번같이 디스크 페이지 / 메모리 페이지 (커널 페이지 버퍼) / 언어 별 인메모리 표현(힙의 객체)를 분리해서 나타내면 우리가 메모리 단 변경사항을 버퍼링했다가 디스크에 플러시하는 식으로 쓰기를 최적화할 수 있다는 말. 뒤의 Lazy B-Tree에서 이어지는 내용임.

## Lazy B-Trees

일부 알고리즘(이 책의 범위에서 이를 lazy B-Tree라 부릅니다)은 B-Tree 업데이트 비용을 줄이고, 갱신을 버퍼링하여 지연 전파하기 위해 더 가볍고 동시성과 업데이트에 친화적인 메모리 내 구조를 사용합니다.

- 앞에서 말했듯 온디스크 기반 B-Tree를 바로 수정하지 않고 버퍼링했다가 플러시하면 IO 횟수를 줄일 수 있음. IO 횟수가 줄어들면 당연히 동시성도 좋아짐. 심지어 분할/병합 등 큰 변경도 막을 수 있음 (분할 → 병합을 통해 다시 원래 구조로 돌아간다던가 하는)
- 이렇게 바로 쓰기를 수행하지 않고 변경을 모아둔 뒤 일괄 적용하는 걸 Lazy B-Tree라고 부르자. 세 가지가 있다. WiredTIger와 LA-Tree 그리고 FD-Tree. 세부적으로 어떻게 구현되는지 알아볼 것.

### WiredTiger

버퍼링을 활용하여 lazy B-트리를 구현하는 방법을 살펴봅시다. 이를 위해 페이지 인(디스크에서 페이지 버퍼로 로드되어)되자마자 B-트리 노드를 메모리에서 실체화(materialize)하고 플러시할 준비가 될 때까지 이 구조에 업데이트를 저장할 수 있습니다.

- 디스크에서 페이지를 읽어와서 페이지 버퍼에 캐싱함. 그리고 이를 실체화- 즉 B-Tree 노드 타입으로 인스턴스화하여 변경사항을 반영하고 커밋될 때 플러시할 수 있음.

이와 유사한 접근은 현재 MongoDB의 기본 스토리지 엔진인 WiredTiger에서도 사용됩니다. WiredTiger의 로우 스토어 B-트리 구현은 메모리 내 페이지와 온디스크 페이지에 서로 다른 형식을 사용합니다. 메모리 내 페이지가 영속화(persist)되기 전에 조정(reconciliation) 과정을 거쳐야 합니다.

- WiredTiger에서 해당 방식을 사용함. 이때 원본 페이지와 우리가 인메모리 표현, 즉 실체화한 노드 타입은 전혀 다른 자료구조로 나타날 수 있음. 왜? 온디스크와 인메모리의 물리적 특성이 다르기 때문. 가령 온디스크에서는 저장 시 공간 최적화 니즈가 있는 반면 인메모리에서는 높은 동시성 중요하고 포인터 기반 연산들이 주가 되기 때문
- 한편 조정이라는 과정이 필요함. 아래에서 설명

![image.png](%E1%84%80%E1%85%A9%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%87%E1%85%A2%E1%86%A8%E1%84%8B%E1%85%A6%E1%86%AB%E1%84%83%E1%85%B3%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%2011%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20233998e1b709807b88efd5c9f74aac6a/image.png)

그림 6-2는 WiredTiger 페이지와 그것들이 B-트리에서 어떻게 구성되는지를 개략적으로 보여 줍니다. 클린 페이지는 온디스크 페이지 이미지로부터 처음 구성된 인덱스만으로 이루어집니다. 업데이트는 먼저 업데이트 버퍼에 저장됩니다.

읽기 작업 중에는 업데이트 버퍼에 접근하여 그 내용을 원본 온디스크 페이지 내용과 병합해 최신 데이터를 반환합니다. 해당 페이지가 플러시될 때 업데이트 버퍼 내용은 페이지 내용과 조정되어 디스크에 지속되며 원본 페이지를 덮어씁니다. 조정된 페이지 크기가 최대 크기를 초과하면 여러 페이지로 분할됩니다. 업데이트 버퍼은 검색 트리와 유사한 복잡도를 가지면서 더 나은 동시성 특성을 지닌 스킵리스트로 구현됩니다.

- 먼저 B-Tree 기반 페이지 포맷을 볼 수 있음. 익숙한 구조임. 일단 디스크에서 페이지를 읽어오면, 일관성 깨지는 일 막기 위해서 읽기 전용으로 ‘클린 페이지’를 만듬. 얘는 데이터 검색 등에 쓰임. 플러시 전까지는 변경되지 않음.
- 그리고 변경사항을 반영하기 위한 업데이트 버퍼를 둠. 사진을 보면 알겠지만 페이지마다 업데이트 버퍼가 있음. 이 버퍼는 스킵리스트라는 자료구조로 구현되는데, O(log N)으로 빠른 탐색이 가능하면서도 동시성 쓰기에서 락이 최소화되어 쓰기에 유리하다는 장점 있음. 그냥 빠르고 동시성이 좋은 자료구조임.
- 이제 쓰기를 하면, 업데이트 버퍼에만 쌓임. 만약 업데이트 버퍼가 커지면? 플러시를 수행함. 이때 조정(reconciliation)을 수행. 클린 페이지의 원본 내용에 업데이트 버퍼에 쌓인 변경사항을 하나씩 적용하는 것 (마치 이벤트 소싱처럼…). 그 다음 디스크 내용을 쓴 다음 업데이트 버퍼를 비운다.
- 이때 분할 역시 이 플러시 시점에 발생함. 즉 플러시 시점에 클린 페이지와 업데이트 버퍼를 병합하는 조정 과정을 거치고 나서 이게 디스크 페이지 크기를 넘어가면 분할이 발생하는 식. 이렇게 하면 병합 비용을 최소화할 수 있음. 가령 중간 과정에서 다시 분할됐다가 병합되는 시나리오…
- 읽기에서도 조정이 수행됨. 이것도 마찬가지로 현재까지 업데이트 버퍼에 쌓인 변경사항을 원본에 적용하는 방식임. 그러면 항상 최신 내용을 보여줄 수 있음.

![image.png](%E1%84%80%E1%85%A9%E1%84%80%E1%85%B3%E1%86%B8%20%E1%84%87%E1%85%A2%E1%86%A8%E1%84%8B%E1%85%A6%E1%86%AB%E1%84%83%E1%85%B3%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%2011%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20233998e1b709807b88efd5c9f74aac6a/image%201.png)

그림 6-3은 WiredTiger에서 클린 페이지와 더티 페이지 모두 메모리 내 버전을 가지며 디스크의 기본 이미지를 참조함을 보여 줍니다. 더티 페이지는 이에 더해 업데이트 버퍼를 가지고 있습니다.

- 당연히 읽기를 위해서 페이지 버퍼로 디스크 페이지 내용을 가져와서 캐싱함.
  - MVCC와 다른 점은 각각의 트랜잭션마다 스냅샷을 두었다면 이거는 클린 페이지 / 업데이트 버퍼를 분리하는 식으로 간다는 것.
- 그리고 페이지마다 업데이트 버퍼를 두는 식. 전역 버퍼는 없나요? 그러면 페이지 오버플로 판정할 방법이 없음. 페이지마다 둬야지 업데이트 버퍼가 다 차면 클린 페이지와 머지해서 분할 여부 검사할 수 있음.

여기서의 주요 이점은 페이지 업데이트와 구조적 변경(분할 및 병합)이 백그라운드 스레드에 의해 수행되므로 읽기 및 쓰기 프로세스가 이들이 완료되기를 기다릴 필요가 없다는 점입니다.

- 이때 더티 페이지를 플러시하는 작업이 백그라운드에서 비동기적으로 동작. PostgreSQL과 비슷
- “모든 페이지를 제거할 때마다 플러시를 발생시키는 것은 성능에 좋지 않을 수 있기 때문에, 일부 데이터베이스는 별도의 백그라운드 프로세스를 사용하여, 제거될 가능성이 높은 더티 페이지를 순환하면서 디스크 버전을 갱신합니다. 예를 들어, PostgreSQL에는 이러한 역할을 수행하는 백그라운드 플러시 라이터가 있습니다.”
- 이를 통해서 변경 작업을 수행하는 경우 업데이트 버퍼에 쌓기만 하면 되고 분할/병합 등 무거운 작업은 백그라운드에서 돌기 때문에 굿. 쓰기가 이렇게 가볍기 때문에, 읽기 작업 시에도 락으로 인한 레이스 컨디션을 최소화하여 빠르게 읽을 수 있음
